{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torchtext\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler    \n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix, classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 100\n",
    "BATCH_SIZE = 256\n",
    "LEARNING_RATE = 0.001\n",
    "NODES = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "calculating mean/median\n",
      "mean:  3395.3801836343455\n",
      "median:  1400.0\n",
      "39644\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[None]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"../uci_data/OnlineNewsPopularity.csv\")\n",
    "print(\"calculating mean/median\")\n",
    "mean =  df[\" shares\"].mean()\n",
    "median = df[\" shares\"].median()\n",
    "print(\"mean: \", mean)\n",
    "print(\"median: \", median)\n",
    "[print(len(df))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc[df[' shares'] < median, ' shares'] = 0\n",
    "df.loc[df[' shares'] >= median, ' shares'] = 1\n",
    "df = df.iloc[:, 2:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "59"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()\n",
    "len(df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x1be098ac108>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.countplot(x = ' shares', data=df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.iloc[:, 0:-1]\n",
    "y = df.iloc[:, -1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>n_tokens_title</th>\n",
       "      <th>n_tokens_content</th>\n",
       "      <th>n_unique_tokens</th>\n",
       "      <th>n_non_stop_words</th>\n",
       "      <th>n_non_stop_unique_tokens</th>\n",
       "      <th>num_hrefs</th>\n",
       "      <th>num_self_hrefs</th>\n",
       "      <th>num_imgs</th>\n",
       "      <th>num_videos</th>\n",
       "      <th>average_token_length</th>\n",
       "      <th>...</th>\n",
       "      <th>avg_positive_polarity</th>\n",
       "      <th>min_positive_polarity</th>\n",
       "      <th>max_positive_polarity</th>\n",
       "      <th>avg_negative_polarity</th>\n",
       "      <th>min_negative_polarity</th>\n",
       "      <th>max_negative_polarity</th>\n",
       "      <th>title_subjectivity</th>\n",
       "      <th>title_sentiment_polarity</th>\n",
       "      <th>abs_title_subjectivity</th>\n",
       "      <th>abs_title_sentiment_polarity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>12.0</td>\n",
       "      <td>219.0</td>\n",
       "      <td>0.663594</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.815385</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.680365</td>\n",
       "      <td>...</td>\n",
       "      <td>0.378636</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.70</td>\n",
       "      <td>-0.350000</td>\n",
       "      <td>-0.600</td>\n",
       "      <td>-0.200000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>-0.187500</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.187500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>9.0</td>\n",
       "      <td>255.0</td>\n",
       "      <td>0.604743</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.791946</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.913725</td>\n",
       "      <td>...</td>\n",
       "      <td>0.286915</td>\n",
       "      <td>0.033333</td>\n",
       "      <td>0.70</td>\n",
       "      <td>-0.118750</td>\n",
       "      <td>-0.125</td>\n",
       "      <td>-0.100000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>9.0</td>\n",
       "      <td>211.0</td>\n",
       "      <td>0.575130</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.663866</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.393365</td>\n",
       "      <td>...</td>\n",
       "      <td>0.495833</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>1.00</td>\n",
       "      <td>-0.466667</td>\n",
       "      <td>-0.800</td>\n",
       "      <td>-0.133333</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9.0</td>\n",
       "      <td>531.0</td>\n",
       "      <td>0.503788</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.665635</td>\n",
       "      <td>9.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.404896</td>\n",
       "      <td>...</td>\n",
       "      <td>0.385965</td>\n",
       "      <td>0.136364</td>\n",
       "      <td>0.80</td>\n",
       "      <td>-0.369697</td>\n",
       "      <td>-0.600</td>\n",
       "      <td>-0.166667</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>13.0</td>\n",
       "      <td>1072.0</td>\n",
       "      <td>0.415646</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.540890</td>\n",
       "      <td>19.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.682836</td>\n",
       "      <td>...</td>\n",
       "      <td>0.411127</td>\n",
       "      <td>0.033333</td>\n",
       "      <td>1.00</td>\n",
       "      <td>-0.220192</td>\n",
       "      <td>-0.500</td>\n",
       "      <td>-0.050000</td>\n",
       "      <td>0.454545</td>\n",
       "      <td>0.136364</td>\n",
       "      <td>0.045455</td>\n",
       "      <td>0.136364</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39639</th>\n",
       "      <td>11.0</td>\n",
       "      <td>346.0</td>\n",
       "      <td>0.529052</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.684783</td>\n",
       "      <td>9.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.523121</td>\n",
       "      <td>...</td>\n",
       "      <td>0.333791</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.75</td>\n",
       "      <td>-0.260000</td>\n",
       "      <td>-0.500</td>\n",
       "      <td>-0.125000</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39640</th>\n",
       "      <td>12.0</td>\n",
       "      <td>328.0</td>\n",
       "      <td>0.696296</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.885057</td>\n",
       "      <td>9.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>48.0</td>\n",
       "      <td>4.405488</td>\n",
       "      <td>...</td>\n",
       "      <td>0.374825</td>\n",
       "      <td>0.136364</td>\n",
       "      <td>0.70</td>\n",
       "      <td>-0.211111</td>\n",
       "      <td>-0.400</td>\n",
       "      <td>-0.100000</td>\n",
       "      <td>0.300000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39641</th>\n",
       "      <td>10.0</td>\n",
       "      <td>442.0</td>\n",
       "      <td>0.516355</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.644128</td>\n",
       "      <td>24.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5.076923</td>\n",
       "      <td>...</td>\n",
       "      <td>0.307273</td>\n",
       "      <td>0.136364</td>\n",
       "      <td>0.50</td>\n",
       "      <td>-0.356439</td>\n",
       "      <td>-0.800</td>\n",
       "      <td>-0.166667</td>\n",
       "      <td>0.454545</td>\n",
       "      <td>0.136364</td>\n",
       "      <td>0.045455</td>\n",
       "      <td>0.136364</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39642</th>\n",
       "      <td>6.0</td>\n",
       "      <td>682.0</td>\n",
       "      <td>0.539493</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.692661</td>\n",
       "      <td>10.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.975073</td>\n",
       "      <td>...</td>\n",
       "      <td>0.236851</td>\n",
       "      <td>0.062500</td>\n",
       "      <td>0.50</td>\n",
       "      <td>-0.205246</td>\n",
       "      <td>-0.500</td>\n",
       "      <td>-0.012500</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39643</th>\n",
       "      <td>10.0</td>\n",
       "      <td>157.0</td>\n",
       "      <td>0.701987</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.846154</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>4.471338</td>\n",
       "      <td>...</td>\n",
       "      <td>0.247338</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.50</td>\n",
       "      <td>-0.200000</td>\n",
       "      <td>-0.200</td>\n",
       "      <td>-0.200000</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.250000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>39644 rows Ã— 58 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        n_tokens_title   n_tokens_content   n_unique_tokens  \\\n",
       "0                 12.0              219.0          0.663594   \n",
       "1                  9.0              255.0          0.604743   \n",
       "2                  9.0              211.0          0.575130   \n",
       "3                  9.0              531.0          0.503788   \n",
       "4                 13.0             1072.0          0.415646   \n",
       "...                ...                ...               ...   \n",
       "39639             11.0              346.0          0.529052   \n",
       "39640             12.0              328.0          0.696296   \n",
       "39641             10.0              442.0          0.516355   \n",
       "39642              6.0              682.0          0.539493   \n",
       "39643             10.0              157.0          0.701987   \n",
       "\n",
       "        n_non_stop_words   n_non_stop_unique_tokens   num_hrefs  \\\n",
       "0                    1.0                   0.815385         4.0   \n",
       "1                    1.0                   0.791946         3.0   \n",
       "2                    1.0                   0.663866         3.0   \n",
       "3                    1.0                   0.665635         9.0   \n",
       "4                    1.0                   0.540890        19.0   \n",
       "...                  ...                        ...         ...   \n",
       "39639                1.0                   0.684783         9.0   \n",
       "39640                1.0                   0.885057         9.0   \n",
       "39641                1.0                   0.644128        24.0   \n",
       "39642                1.0                   0.692661        10.0   \n",
       "39643                1.0                   0.846154         1.0   \n",
       "\n",
       "        num_self_hrefs   num_imgs   num_videos   average_token_length  ...  \\\n",
       "0                  2.0        1.0          0.0               4.680365  ...   \n",
       "1                  1.0        1.0          0.0               4.913725  ...   \n",
       "2                  1.0        1.0          0.0               4.393365  ...   \n",
       "3                  0.0        1.0          0.0               4.404896  ...   \n",
       "4                 19.0       20.0          0.0               4.682836  ...   \n",
       "...                ...        ...          ...                    ...  ...   \n",
       "39639              7.0        1.0          1.0               4.523121  ...   \n",
       "39640              7.0        3.0         48.0               4.405488  ...   \n",
       "39641              1.0       12.0          1.0               5.076923  ...   \n",
       "39642              1.0        1.0          0.0               4.975073  ...   \n",
       "39643              1.0        0.0          2.0               4.471338  ...   \n",
       "\n",
       "        avg_positive_polarity   min_positive_polarity   max_positive_polarity  \\\n",
       "0                    0.378636                0.100000                    0.70   \n",
       "1                    0.286915                0.033333                    0.70   \n",
       "2                    0.495833                0.100000                    1.00   \n",
       "3                    0.385965                0.136364                    0.80   \n",
       "4                    0.411127                0.033333                    1.00   \n",
       "...                       ...                     ...                     ...   \n",
       "39639                0.333791                0.100000                    0.75   \n",
       "39640                0.374825                0.136364                    0.70   \n",
       "39641                0.307273                0.136364                    0.50   \n",
       "39642                0.236851                0.062500                    0.50   \n",
       "39643                0.247338                0.100000                    0.50   \n",
       "\n",
       "        avg_negative_polarity   min_negative_polarity   max_negative_polarity  \\\n",
       "0                   -0.350000                  -0.600               -0.200000   \n",
       "1                   -0.118750                  -0.125               -0.100000   \n",
       "2                   -0.466667                  -0.800               -0.133333   \n",
       "3                   -0.369697                  -0.600               -0.166667   \n",
       "4                   -0.220192                  -0.500               -0.050000   \n",
       "...                       ...                     ...                     ...   \n",
       "39639               -0.260000                  -0.500               -0.125000   \n",
       "39640               -0.211111                  -0.400               -0.100000   \n",
       "39641               -0.356439                  -0.800               -0.166667   \n",
       "39642               -0.205246                  -0.500               -0.012500   \n",
       "39643               -0.200000                  -0.200               -0.200000   \n",
       "\n",
       "        title_subjectivity   title_sentiment_polarity  \\\n",
       "0                 0.500000                  -0.187500   \n",
       "1                 0.000000                   0.000000   \n",
       "2                 0.000000                   0.000000   \n",
       "3                 0.000000                   0.000000   \n",
       "4                 0.454545                   0.136364   \n",
       "...                    ...                        ...   \n",
       "39639             0.100000                   0.000000   \n",
       "39640             0.300000                   1.000000   \n",
       "39641             0.454545                   0.136364   \n",
       "39642             0.000000                   0.000000   \n",
       "39643             0.333333                   0.250000   \n",
       "\n",
       "        abs_title_subjectivity   abs_title_sentiment_polarity  \n",
       "0                     0.000000                       0.187500  \n",
       "1                     0.500000                       0.000000  \n",
       "2                     0.500000                       0.000000  \n",
       "3                     0.500000                       0.000000  \n",
       "4                     0.045455                       0.136364  \n",
       "...                        ...                            ...  \n",
       "39639                 0.400000                       0.000000  \n",
       "39640                 0.200000                       1.000000  \n",
       "39641                 0.045455                       0.136364  \n",
       "39642                 0.500000                       0.000000  \n",
       "39643                 0.166667                       0.250000  \n",
       "\n",
       "[39644 rows x 58 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        0\n",
       "1        0\n",
       "2        1\n",
       "3        0\n",
       "4        0\n",
       "        ..\n",
       "39639    1\n",
       "39640    1\n",
       "39641    1\n",
       "39642    0\n",
       "39643    0\n",
       "Name:  shares, Length: 39644, dtype: int64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=69)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.fit_transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "## train data\n",
    "class trainData(Dataset):\n",
    "    \n",
    "    def __init__(self, X_data, y_data):\n",
    "        self.X_data = X_data\n",
    "        self.y_data = y_data\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        return self.X_data[index], self.y_data[index]\n",
    "        \n",
    "    def __len__ (self):\n",
    "        return len(self.X_data)\n",
    "\n",
    "\n",
    "train_data = trainData(torch.FloatTensor(X_train), \n",
    "                       torch.FloatTensor(y_train))\n",
    "## test data    \n",
    "class testData(Dataset):\n",
    "    \n",
    "    def __init__(self, X_data):\n",
    "        self.X_data = X_data\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        return self.X_data[index]\n",
    "        \n",
    "    def __len__ (self):\n",
    "        return len(self.X_data)\n",
    "    \n",
    "\n",
    "test_data = testData(torch.FloatTensor(X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(dataset=train_data, batch_size=BATCH_SIZE, shuffle=True)\n",
    "test_loader = DataLoader(dataset=test_data, batch_size=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "class binaryClassification(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(binaryClassification, self).__init__()\n",
    "        # Number of input features is 58.\n",
    "        self.layer_1 = nn.Linear(58, NODES) \n",
    "        self.layer_2 = nn.Linear(NODES, NODES)\n",
    "        self.layer_3 = nn.Linear(NODES, NODES)\n",
    "        self.layer_4= nn.Linear(NODES, NODES)\n",
    "        self.layer_out = nn.Linear(NODES, 1) \n",
    "        \n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(p=0.1)\n",
    "        self.batchnorm1 = nn.BatchNorm1d(NODES)\n",
    "        self.batchnorm2 = nn.BatchNorm1d(NODES)\n",
    "        self.batchnorm3 = nn.BatchNorm1d(NODES)\n",
    "        self.batchnorm4 = nn.BatchNorm1d(NODES)\n",
    "        \n",
    "    def forward(self, inputs):\n",
    "        x = self.relu(self.layer_1(inputs))\n",
    "        x = self.batchnorm1(x)\n",
    "        x = self.relu(self.layer_2(x))\n",
    "        x = self.batchnorm2(x)\n",
    "        x = self.relu(self.layer_3(x))\n",
    "        x = self.batchnorm3(x)\n",
    "        x = self.relu(self.layer_4(x))\n",
    "        x = self.batchnorm4(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.layer_out(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "binaryClassification(\n",
      "  (layer_1): Linear(in_features=58, out_features=1000, bias=True)\n",
      "  (layer_2): Linear(in_features=1000, out_features=1000, bias=True)\n",
      "  (layer_3): Linear(in_features=1000, out_features=1000, bias=True)\n",
      "  (layer_4): Linear(in_features=1000, out_features=1000, bias=True)\n",
      "  (layer_out): Linear(in_features=1000, out_features=1, bias=True)\n",
      "  (relu): ReLU()\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      "  (batchnorm1): BatchNorm1d(1000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (batchnorm2): BatchNorm1d(1000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (batchnorm3): BatchNorm1d(1000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (batchnorm4): BatchNorm1d(1000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "model = binaryClassification()\n",
    "model.to(device)\n",
    "print(model)\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "def binary_acc(y_pred, y_test):\n",
    "    y_pred_tag = torch.round(torch.sigmoid(y_pred))\n",
    "\n",
    "    correct_results_sum = (y_pred_tag == y_test).sum().float()\n",
    "    acc = correct_results_sum/y_test.shape[0]\n",
    "    acc = torch.round(acc * 100)\n",
    "    \n",
    "    return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 001: | Loss: 0.66039 | Acc: 63.279\n",
      "Epoch 002: | Loss: 0.61167 | Acc: 66.606\n",
      "Epoch 003: | Loss: 0.60290 | Acc: 67.587\n",
      "Epoch 004: | Loss: 0.59184 | Acc: 68.067\n",
      "Epoch 005: | Loss: 0.58322 | Acc: 68.913\n",
      "Epoch 006: | Loss: 0.57233 | Acc: 69.635\n",
      "Epoch 007: | Loss: 0.55725 | Acc: 70.962\n",
      "Epoch 008: | Loss: 0.54488 | Acc: 71.625\n",
      "Epoch 009: | Loss: 0.52243 | Acc: 73.356\n",
      "Epoch 010: | Loss: 0.50960 | Acc: 74.135\n",
      "Epoch 011: | Loss: 0.48391 | Acc: 75.981\n",
      "Epoch 012: | Loss: 0.45588 | Acc: 77.769\n",
      "Epoch 013: | Loss: 0.42766 | Acc: 79.279\n",
      "Epoch 014: | Loss: 0.39070 | Acc: 81.548\n",
      "Epoch 015: | Loss: 0.35392 | Acc: 83.558\n",
      "Epoch 016: | Loss: 0.30813 | Acc: 86.106\n",
      "Epoch 017: | Loss: 0.27973 | Acc: 87.606\n",
      "Epoch 018: | Loss: 0.25027 | Acc: 89.058\n",
      "Epoch 019: | Loss: 0.21046 | Acc: 91.000\n",
      "Epoch 020: | Loss: 0.19261 | Acc: 91.904\n",
      "Epoch 021: | Loss: 0.16147 | Acc: 93.337\n",
      "Epoch 022: | Loss: 0.14403 | Acc: 94.144\n",
      "Epoch 023: | Loss: 0.12276 | Acc: 94.885\n",
      "Epoch 024: | Loss: 0.11840 | Acc: 95.212\n",
      "Epoch 025: | Loss: 0.10100 | Acc: 95.856\n",
      "Epoch 026: | Loss: 0.09544 | Acc: 96.231\n",
      "Epoch 027: | Loss: 0.08783 | Acc: 96.385\n",
      "Epoch 028: | Loss: 0.09066 | Acc: 96.202\n",
      "Epoch 029: | Loss: 0.07740 | Acc: 96.798\n",
      "Epoch 030: | Loss: 0.06258 | Acc: 97.394\n",
      "Epoch 031: | Loss: 0.06646 | Acc: 97.365\n",
      "Epoch 032: | Loss: 0.06432 | Acc: 97.250\n",
      "Epoch 033: | Loss: 0.07351 | Acc: 96.923\n",
      "Epoch 034: | Loss: 0.06468 | Acc: 97.269\n",
      "Epoch 035: | Loss: 0.05587 | Acc: 97.625\n",
      "Epoch 036: | Loss: 0.05126 | Acc: 97.798\n",
      "Epoch 037: | Loss: 0.04988 | Acc: 97.913\n",
      "Epoch 038: | Loss: 0.05695 | Acc: 97.740\n",
      "Epoch 039: | Loss: 0.04998 | Acc: 97.798\n",
      "Epoch 040: | Loss: 0.04820 | Acc: 97.952\n",
      "Epoch 041: | Loss: 0.04593 | Acc: 98.000\n",
      "Epoch 042: | Loss: 0.05891 | Acc: 97.712\n",
      "Epoch 043: | Loss: 0.05130 | Acc: 97.885\n",
      "Epoch 044: | Loss: 0.04399 | Acc: 98.240\n",
      "Epoch 045: | Loss: 0.03871 | Acc: 98.375\n",
      "Epoch 046: | Loss: 0.03870 | Acc: 98.394\n",
      "Epoch 047: | Loss: 0.04378 | Acc: 98.356\n",
      "Epoch 048: | Loss: 0.04137 | Acc: 98.346\n",
      "Epoch 049: | Loss: 0.03805 | Acc: 98.413\n",
      "Epoch 050: | Loss: 0.04107 | Acc: 98.308\n",
      "Epoch 051: | Loss: 0.04672 | Acc: 98.163\n",
      "Epoch 052: | Loss: 0.04314 | Acc: 98.365\n",
      "Epoch 053: | Loss: 0.03789 | Acc: 98.471\n",
      "Epoch 054: | Loss: 0.03304 | Acc: 98.808\n",
      "Epoch 055: | Loss: 0.02700 | Acc: 98.952\n",
      "Epoch 056: | Loss: 0.03220 | Acc: 98.817\n",
      "Epoch 057: | Loss: 0.03600 | Acc: 98.567\n",
      "Epoch 058: | Loss: 0.03631 | Acc: 98.702\n",
      "Epoch 059: | Loss: 0.04268 | Acc: 98.356\n",
      "Epoch 060: | Loss: 0.03698 | Acc: 98.625\n",
      "Epoch 061: | Loss: 0.02671 | Acc: 99.000\n",
      "Epoch 062: | Loss: 0.03359 | Acc: 98.721\n",
      "Epoch 063: | Loss: 0.03572 | Acc: 98.606\n",
      "Epoch 064: | Loss: 0.03102 | Acc: 98.827\n",
      "Epoch 065: | Loss: 0.03060 | Acc: 98.913\n",
      "Epoch 066: | Loss: 0.03248 | Acc: 98.702\n",
      "Epoch 067: | Loss: 0.03501 | Acc: 98.740\n",
      "Epoch 068: | Loss: 0.03040 | Acc: 98.865\n",
      "Epoch 069: | Loss: 0.03364 | Acc: 98.663\n",
      "Epoch 070: | Loss: 0.03044 | Acc: 98.942\n",
      "Epoch 071: | Loss: 0.02646 | Acc: 99.067\n",
      "Epoch 072: | Loss: 0.02784 | Acc: 98.952\n",
      "Epoch 073: | Loss: 0.02897 | Acc: 98.981\n",
      "Epoch 074: | Loss: 0.03196 | Acc: 98.904\n",
      "Epoch 075: | Loss: 0.02609 | Acc: 99.125\n",
      "Epoch 076: | Loss: 0.02193 | Acc: 99.192\n",
      "Epoch 077: | Loss: 0.02080 | Acc: 99.250\n",
      "Epoch 078: | Loss: 0.02884 | Acc: 99.010\n",
      "Epoch 079: | Loss: 0.02493 | Acc: 99.144\n",
      "Epoch 080: | Loss: 0.02885 | Acc: 99.048\n",
      "Epoch 081: | Loss: 0.02772 | Acc: 98.913\n",
      "Epoch 082: | Loss: 0.02393 | Acc: 99.231\n",
      "Epoch 083: | Loss: 0.02227 | Acc: 99.144\n",
      "Epoch 084: | Loss: 0.02424 | Acc: 99.212\n",
      "Epoch 085: | Loss: 0.02750 | Acc: 99.019\n",
      "Epoch 086: | Loss: 0.02573 | Acc: 99.154\n",
      "Epoch 087: | Loss: 0.01999 | Acc: 99.375\n",
      "Epoch 088: | Loss: 0.02029 | Acc: 99.365\n",
      "Epoch 089: | Loss: 0.02204 | Acc: 99.298\n",
      "Epoch 090: | Loss: 0.02454 | Acc: 99.173\n",
      "Epoch 091: | Loss: 0.02333 | Acc: 99.240\n",
      "Epoch 092: | Loss: 0.01763 | Acc: 99.413\n",
      "Epoch 093: | Loss: 0.01653 | Acc: 99.548\n",
      "Epoch 094: | Loss: 0.01981 | Acc: 99.327\n",
      "Epoch 095: | Loss: 0.02016 | Acc: 99.375\n",
      "Epoch 096: | Loss: 0.01906 | Acc: 99.423\n",
      "Epoch 097: | Loss: 0.02288 | Acc: 99.202\n",
      "Epoch 098: | Loss: 0.02834 | Acc: 99.048\n",
      "Epoch 099: | Loss: 0.02507 | Acc: 99.154\n",
      "Epoch 100: | Loss: 0.01597 | Acc: 99.481\n"
     ]
    }
   ],
   "source": [
    "model.train()\n",
    "for e in range(1, EPOCHS+1):\n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0\n",
    "    for X_batch, y_batch in train_loader:\n",
    "        X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        y_pred = model(X_batch)\n",
    "        \n",
    "        loss = criterion(y_pred, y_batch.unsqueeze(1))\n",
    "        acc = binary_acc(y_pred, y_batch.unsqueeze(1))\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        epoch_loss += loss.item()\n",
    "        epoch_acc += acc.item()\n",
    "        \n",
    "\n",
    "    print(f'Epoch {e+0:03}: | Loss: {epoch_loss/len(train_loader):.5f} | Acc: {epoch_acc/len(train_loader):.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_list = []\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    for X_batch in test_loader:\n",
    "        X_batch = X_batch.to(device)\n",
    "        y_test_pred = model(X_batch)\n",
    "        y_test_pred = torch.sigmoid(y_test_pred)\n",
    "        y_pred_tag = torch.round(y_test_pred)\n",
    "        y_pred_list.append(y_pred_tag.cpu().numpy())\n",
    "\n",
    "y_pred_list = [a.squeeze().tolist() for a in y_pred_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[3468, 2539],\n",
       "       [2594, 4482]], dtype=int64)"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "confusion_matrix(y_test, y_pred_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.57      0.58      0.57      6007\n",
      "           1       0.64      0.63      0.64      7076\n",
      "\n",
      "    accuracy                           0.61     13083\n",
      "   macro avg       0.61      0.61      0.61     13083\n",
      "weighted avg       0.61      0.61      0.61     13083\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_test, y_pred_list))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
