{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from textblob import TextBlob\n",
    "from datetime import datetime\n",
    "\n",
    "import spacy\n",
    "import torch\n",
    "import torchtext\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader, WeightedRandomSampler\n",
    "import torch.nn.functional as F\n",
    "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler    \n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "from collections import Counter\n",
    "import re\n",
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 200\n",
    "BATCH_SIZE = 256\n",
    "LEARNING_RATE = 0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in CSV\n",
    "df = pd.read_csv(\"../data/Organic_extended_finalv3.csv\", sep=\"|\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find max retweets\n",
    "max_list = list()\n",
    "for index, row in df.iterrows():\n",
    "    num_list = list()\n",
    "    num_list = {row[\"1\"], row[\"2\"],row[\"3\"], row[\"4\"],row[\"5\"], row[\"6\"]}\n",
    "    max_list.append(max(num_list))\n",
    "df[\"max_retweets\"] = max_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "calculating mean/median\n",
      "mean:  149.06297636191212\n",
      "median:  50.0\n",
      "Number of entries:  28471\n",
      "std:  620.5472819951422\n"
     ]
    }
   ],
   "source": [
    "# Find mean/median and size\n",
    "print(\"calculating mean/median\")\n",
    "mean =  df[\"max_retweets\"].mean()\n",
    "median = df[\"max_retweets\"].median()\n",
    "print(\"mean: \", mean)\n",
    "print(\"median: \", median)\n",
    "print(\"Number of entries: \", len(df))\n",
    "df['max_retweets'].min()\n",
    "std = df.loc[:,\"max_retweets\"].std()\n",
    "print(\"std: \", std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert date strings to datetime objeccts\n",
    "date_time = list()\n",
    "for index, row in df.iterrows():\n",
    "    if(row[\"created_time\"].lower().islower()):\n",
    "        # date time w/ letter (Jun, Mon, etc)\n",
    "        date_time_obj = datetime.strptime(row[\"created_time\"], '%a %b %d %H:%M:%S +0000 %Y')\n",
    "        date_time.append(date_time_obj)\n",
    "    else:\n",
    "        # date time w/ not letters (Jun, Mon, etc)\n",
    "        date_time_obj = datetime.strptime(row[\"created_time\"], '%Y-%m-%d %H:%M:%S+00:00')\n",
    "        date_time.append(date_time_obj)\n",
    "df[\"created_datetime\"] = date_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add data for what day of week the article was published\n",
    "df['is_mon'] = 0\n",
    "df['is_tue'] = 0\n",
    "df['is_wed'] = 0\n",
    "df['is_thu'] = 0\n",
    "df['is_fri'] = 0\n",
    "df['is_sat'] = 0\n",
    "df['is_sun'] = 0\n",
    "df['is_weekend'] = 0\n",
    "\n",
    "for index, row in df.iterrows():\n",
    "    day = row[\"created_datetime\"].weekday()\n",
    "    if day is 0:\n",
    "        df.at[index,'is_sun'] = 1\n",
    "        df.at[index,'is_weekend'] = 1\n",
    "    elif day is 1:\n",
    "        df.at[index,'is_mon'] = 1\n",
    "    elif day is 2:\n",
    "        df.at[index,'is_tue'] = 1\n",
    "    elif day is 3:\n",
    "        df.at[index,'is_wed'] = 1\n",
    "    elif day is 4:\n",
    "        df.at[index,'is_thu'] = 1\n",
    "    elif day is 5:\n",
    "        df.at[index,'is_fri'] = 1\n",
    "    elif day is 6:\n",
    "        df.at[index,'is_sat'] = 1\n",
    "        df.at[index,'is_weekend'] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Subjectivity and polarity\n",
    "for index, row in df.iterrows():\n",
    "    title_score = TextBlob(row[\"title\"]).sentiment\n",
    "    content_score = TextBlob(row[\"content\"]).sentiment\n",
    "    df.at[index,'title_polarity'] = title_score[0]\n",
    "    df.at[index,'title_subjectivity'] = title_score[1]\n",
    "    df.at[index,'content_polarity'] = content_score[0]\n",
    "    df.at[index,'content_subjectivity'] = content_score[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# source\n",
    "accounts = [\"CNN\",\"The Wall Street Journal\",\"The Washington Post\",\"NBC News\",\n",
    "            \"The Associated Press\",\"ABC News\",\"Los Angeles Times\",\"The New York Times\",\"NPR\",\"TIME\",\"U.S. News\",\"USA TODAY\",\n",
    "            \"Fox News\",\"Reuters\",\"HuffPost\"]\n",
    "for i in accounts:\n",
    "    df[i] = 0\n",
    "\n",
    "\n",
    "for index, row in df.iterrows():\n",
    "    df.at[index,row[\"screen_name\"]] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove punctuation from content\n",
    "for index, row in df.iterrows():\n",
    "    temp_str = row[\"content\"].translate(str.maketrans('','',string.punctuation))\n",
    "    #temp_str = ' '.join(temp_str.split()[:500])\n",
    "    #df.at[index,\"content\"] = ' '.join(temp_str.split()[:200])\n",
    "    df.at[index,\"content\"] = temp_str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# combine title and text, delete other columns\n",
    "#df[\"full_text\"] = df[\"title\"] + ' ' + df[\"content\"]\n",
    "df[\"full_text\"] =  df[\"content\"]\n",
    "df = df[[\"full_text\", \"max_retweets\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1:  10 2:  100 3:  1000\n"
     ]
    }
   ],
   "source": [
    "# 0 -> 0-0.25 quantile\n",
    "# 1 -> 0.26-0.50 quantile\n",
    "# 2 -> < 0.51-0.75 quantile\n",
    "# 3 -> >= 0.76-1.00 quantile\n",
    "\n",
    "\n",
    "#quan_dict=df.max_retweets.quantile([0.25, 0.5, 0.75])\n",
    "#one_quar = quan_dict[0.25]\n",
    "#two_quar = quan_dict[0.5]\n",
    "#three_quar = quan_dict[0.75]\n",
    "\n",
    "one_quar = 10\n",
    "two_quar = 100\n",
    "three_quar = 1000\n",
    "\n",
    "print(\"1: \", one_quar, \"2: \", two_quar, \"3: \", three_quar)\n",
    "\n",
    "df.loc[df['max_retweets'] <= one_quar, 'shares'] = 0\n",
    "df.loc[((df['max_retweets'] > one_quar) & (df['max_retweets'] <= two_quar)), 'shares'] = 1\n",
    "df.loc[((df['max_retweets'] > two_quar) & (df['max_retweets'] <= three_quar)), 'shares'] = 2\n",
    "df.loc[df['max_retweets'] > three_quar, 'shares'] = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>tweet_id</th>\n",
       "      <th>created_time</th>\n",
       "      <th>count</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>...</th>\n",
       "      <th>Los Angeles Times</th>\n",
       "      <th>The New York Times</th>\n",
       "      <th>NPR</th>\n",
       "      <th>TIME</th>\n",
       "      <th>U.S. News</th>\n",
       "      <th>USA TODAY</th>\n",
       "      <th>Fox News</th>\n",
       "      <th>Reuters</th>\n",
       "      <th>HuffPost</th>\n",
       "      <th>full_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1272217655630458881</td>\n",
       "      <td>2020-06-14 17:21:40+00:00</td>\n",
       "      <td>17</td>\n",
       "      <td>454</td>\n",
       "      <td>463.0</td>\n",
       "      <td>462.0</td>\n",
       "      <td>464.0</td>\n",
       "      <td>464.0</td>\n",
       "      <td>466.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>‘All Black Lives Matter’ painted on Hollywood ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1272216897237516289</td>\n",
       "      <td>2020-06-14 17:18:39+00:00</td>\n",
       "      <td>17</td>\n",
       "      <td>163</td>\n",
       "      <td>163.0</td>\n",
       "      <td>163.0</td>\n",
       "      <td>163.0</td>\n",
       "      <td>162.0</td>\n",
       "      <td>162.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Millions in lawsuit settlements are another hi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>1272220034065186817</td>\n",
       "      <td>2020-06-14 17:31:07+00:00</td>\n",
       "      <td>17</td>\n",
       "      <td>910</td>\n",
       "      <td>927.0</td>\n",
       "      <td>929.0</td>\n",
       "      <td>933.0</td>\n",
       "      <td>934.0</td>\n",
       "      <td>936.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Woman becomes first observant Sikh to graduate...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>1272219784743202816</td>\n",
       "      <td>2020-06-14 17:30:08+00:00</td>\n",
       "      <td>17</td>\n",
       "      <td>2352</td>\n",
       "      <td>2377.0</td>\n",
       "      <td>2381.0</td>\n",
       "      <td>2378.0</td>\n",
       "      <td>2376.0</td>\n",
       "      <td>2373.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>As Social Distancing Wanes, Cuomo Warns of Ano...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>1272220746014572545</td>\n",
       "      <td>2020-06-14 17:33:57+00:00</td>\n",
       "      <td>17</td>\n",
       "      <td>241</td>\n",
       "      <td>267.0</td>\n",
       "      <td>267.0</td>\n",
       "      <td>267.0</td>\n",
       "      <td>267.0</td>\n",
       "      <td>267.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>They lost loved ones to police violence. Georg...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 49 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0             tweet_id               created_time  count     1  \\\n",
       "0           0  1272217655630458881  2020-06-14 17:21:40+00:00     17   454   \n",
       "1           1  1272216897237516289  2020-06-14 17:18:39+00:00     17   163   \n",
       "2           2  1272220034065186817  2020-06-14 17:31:07+00:00     17   910   \n",
       "3           3  1272219784743202816  2020-06-14 17:30:08+00:00     17  2352   \n",
       "4           4  1272220746014572545  2020-06-14 17:33:57+00:00     17   241   \n",
       "\n",
       "        2       3       4       5       6  ...  Los Angeles Times  \\\n",
       "0   463.0   462.0   464.0   464.0   466.0  ...                  1   \n",
       "1   163.0   163.0   163.0   162.0   162.0  ...                  0   \n",
       "2   927.0   929.0   933.0   934.0   936.0  ...                  0   \n",
       "3  2377.0  2381.0  2378.0  2376.0  2373.0  ...                  0   \n",
       "4   267.0   267.0   267.0   267.0   267.0  ...                  1   \n",
       "\n",
       "  The New York Times NPR  TIME U.S. News USA TODAY Fox News  Reuters  \\\n",
       "0                  0   0     0         0         0        0        0   \n",
       "1                  0   0     0         0         0        0        0   \n",
       "2                  0   0     0         0         0        0        0   \n",
       "3                  1   0     0         0         0        0        0   \n",
       "4                  0   0     0         0         0        0        0   \n",
       "\n",
       "   HuffPost                                          full_text  \n",
       "0         0  ‘All Black Lives Matter’ painted on Hollywood ...  \n",
       "1         0  Millions in lawsuit settlements are another hi...  \n",
       "2         0  Woman becomes first observant Sikh to graduate...  \n",
       "3         0  As Social Distancing Wanes, Cuomo Warns of Ano...  \n",
       "4         0  They lost loved ones to police violence. Georg...  \n",
       "\n",
       "[5 rows x 49 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#pd.set_option('display.max_colwidth', -1)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df['content'] = df['content'].fillna('')\n",
    "df['content_length'] = df['full_text'].apply(lambda x: len(x.split()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "738.4095395314531"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# mean content length\n",
    "np.mean(df['content_length'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train len: 22669 test len:  5802\n"
     ]
    }
   ],
   "source": [
    "# Split Train and Test dfs\n",
    "mask = np.random.rand(len(df)) < 0.8\n",
    "train = df[mask]\n",
    "test = df[~mask]\n",
    "print(\"train len:\", len(train), \"test len: \", len(test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tokenization\n",
    "tok = spacy.load('en')\n",
    "def tokenize(text):\n",
    "    text = re.sub(r\"[^\\x00-\\x7F]+\", \" \", text)\n",
    "    regex = re.compile('[' + re.escape(string.punctuation) + '0-9\\\\r\\\\t\\\\n]')\n",
    "    nopunct = regex.sub(\" \", text.lower())\n",
    "    return [token.text for token in tok.tokenizer(nopunct)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#count number of occurences of each word\n",
    "counts = Counter()\n",
    "for index, row in train.iterrows():\n",
    "    counts.update(tokenize(row['full_text']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_words before: 91230\n",
      "num_words after: 69318\n"
     ]
    }
   ],
   "source": [
    "#deleting infrequnet words\n",
    "print(\"num_words before:\", len(counts.keys()))\n",
    "for word in list(counts):\n",
    "    if counts[word] < 2:\n",
    "        del counts[word]\n",
    "print(\"num_words after:\",len(counts.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating vocab\n",
    "vocab2index = {\"\":0, \"UNK\":1}\n",
    "words = [\"\",\"UNK\"]\n",
    "for word in counts:\n",
    "    vocab2index[word] = len(words)\n",
    "    words.append(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_sentence(text, vocab2index, N=450):\n",
    "    tokenized = tokenize(text)\n",
    "    encoded = np.zeros(N,dtype=int)\n",
    "    enc1 = np.array([vocab2index.get(word,vocab2index[\"UNK\"]) for word in tokenized])\n",
    "    length = min(N, len(enc1))\n",
    "    encoded[:length] = enc1[:length]\n",
    "    return encoded, length"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GloVe Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_glove_vectors(glove_file=\"../data/glove.6B.100d.txt\"):\n",
    "    \"\"\"Load the glove word vectors\"\"\"\n",
    "    word_vectors = {}\n",
    "    with open(glove_file, encoding=\"utf8\") as f:\n",
    "        for line in f:\n",
    "            split = line.split()\n",
    "            word_vectors[split[0]] = np.array([float(x) for x in split[1:]])\n",
    "    return word_vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_emb_matrix(pretrained, word_counts, emb_size = 100):\n",
    "    \"\"\" Creates embedding matrix from word vectors\"\"\"\n",
    "    vocab_size = len(word_counts) + 2\n",
    "    vocab_to_idx = {}\n",
    "    vocab = [\"\", \"UNK\"]\n",
    "    W = np.zeros((vocab_size, emb_size), dtype=\"float32\")\n",
    "    W[0] = np.zeros(emb_size, dtype='float32') # adding a vector for padding\n",
    "    W[1] = np.random.uniform(-0.25, 0.25, emb_size) # adding a vector for unknown words \n",
    "    vocab_to_idx[\"UNK\"] = 1\n",
    "    i = 2\n",
    "    for word in word_counts:\n",
    "        if word in word_vecs:\n",
    "            W[i] = word_vecs[word]\n",
    "        else:\n",
    "            W[i] = np.random.uniform(-0.25,0.25, emb_size)\n",
    "        vocab_to_idx[word] = i\n",
    "        vocab.append(word)\n",
    "        i += 1   \n",
    "    return W, np.array(vocab), vocab_to_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_vecs = load_glove_vectors()\n",
    "pretrained_weights, vocab, vocab2index = get_emb_matrix(word_vecs, counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E:\\Programs\\lib\\site-packages\\ipykernel_launcher.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n",
      "E:\\Programs\\lib\\site-packages\\ipykernel_launcher.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "train['encoded'] = train['full_text'].apply(lambda x: np.array(encode_sentence(x,vocab2index)))\n",
    "test['encoded'] = test['full_text'].apply(lambda x: np.array(encode_sentence(x,vocab2index)))\n",
    "\n",
    "#df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()\n",
    "len(df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x147674f1788>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.countplot(x = 'shares', data=df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>follower_count</th>\n",
       "      <th>title_len</th>\n",
       "      <th>content_len</th>\n",
       "      <th>is_mon</th>\n",
       "      <th>is_tue</th>\n",
       "      <th>is_wed</th>\n",
       "      <th>is_thu</th>\n",
       "      <th>is_fri</th>\n",
       "      <th>is_sat</th>\n",
       "      <th>...</th>\n",
       "      <th>The New York Times</th>\n",
       "      <th>NPR</th>\n",
       "      <th>TIME</th>\n",
       "      <th>U.S. News</th>\n",
       "      <th>USA TODAY</th>\n",
       "      <th>Fox News</th>\n",
       "      <th>Reuters</th>\n",
       "      <th>HuffPost</th>\n",
       "      <th>content_length</th>\n",
       "      <th>encoded</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>8</td>\n",
       "      <td>3634146.0</td>\n",
       "      <td>16</td>\n",
       "      <td>2826</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2837</td>\n",
       "      <td>[[151, 1506, 1507, 748, 67, 30, 435, 2, 34, 35...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>14</td>\n",
       "      <td>14315833.0</td>\n",
       "      <td>8</td>\n",
       "      <td>1062</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1069</td>\n",
       "      <td>[[2, 147, 3811, 86, 249, 1465, 1078, 1082, 105...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>28</td>\n",
       "      <td>7668571.0</td>\n",
       "      <td>15</td>\n",
       "      <td>999</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1014</td>\n",
       "      <td>[[5043, 2, 3956, 458, 1879, 13, 2, 1763, 2552,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>35</td>\n",
       "      <td>17862906.0</td>\n",
       "      <td>7</td>\n",
       "      <td>55</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>62</td>\n",
       "      <td>[[865, 24254, 62, 7120, 4307, 67, 13386, 7120,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>37</td>\n",
       "      <td>4102107.0</td>\n",
       "      <td>12</td>\n",
       "      <td>1619</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1626</td>\n",
       "      <td>[[86, 303, 1156, 2, 3891, 2, 192, 764, 845, 21...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 33 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    Unnamed: 0  follower_count  title_len  content_len  is_mon  is_tue  \\\n",
       "8            8       3634146.0         16         2826       0       0   \n",
       "14          14      14315833.0          8         1062       0       0   \n",
       "26          28       7668571.0         15          999       0       0   \n",
       "33          35      17862906.0          7           55       0       0   \n",
       "35          37       4102107.0         12         1619       0       0   \n",
       "\n",
       "    is_wed  is_thu  is_fri  is_sat  ...  The New York Times  NPR  TIME  \\\n",
       "8        0       0       0       1  ...                   0    0     0   \n",
       "14       0       0       0       1  ...                   0    0     0   \n",
       "26       0       0       0       1  ...                   0    0     0   \n",
       "33       0       0       0       1  ...                   0    0     0   \n",
       "35       0       0       0       1  ...                   0    0     0   \n",
       "\n",
       "    U.S. News  USA TODAY  Fox News  Reuters  HuffPost  content_length  \\\n",
       "8           0          0         0        0         0            2837   \n",
       "14          0          0         0        0         0            1069   \n",
       "26          0          0         0        0         0            1014   \n",
       "33          0          0         0        0         0              62   \n",
       "35          0          1         0        0         0            1626   \n",
       "\n",
       "                                              encoded  \n",
       "8   [[151, 1506, 1507, 748, 67, 30, 435, 2, 34, 35...  \n",
       "14  [[2, 147, 3811, 86, 249, 1465, 1078, 1082, 105...  \n",
       "26  [[5043, 2, 3956, 458, 1879, 13, 2, 1763, 2552,...  \n",
       "33  [[865, 24254, 62, 7120, 4307, 67, 13386, 7120,...  \n",
       "35  [[86, 303, 1156, 2, 3891, 2, 192, 764, 845, 21...  \n",
       "\n",
       "[5 rows x 33 columns]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#train = train.drop(['tweet_id','created_time','count','1','2','3','4','5','6','user_id','screen_name','title','content','url','expanded_url','created_datetime','max_retweets'], axis = 1)\n",
    "#test = test.drop(['tweet_id','created_time','count','1','2','3','4','5','6','user_id','screen_name','title','content','url','expanded_url','created_datetime','max_retweets'], axis = 1)\n",
    "train = train.drop(['full_text'],axis=1)\n",
    "test = test.drop(['full_text'],axis=1)\n",
    "test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, y_train = list(train['encoded']), list(train['shares'])\n",
    "X_valid, y_valid = list(test['encoded']), list(test['shares'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=0.20, random_state=6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NewsDataset(Dataset):\n",
    "    def __init__(self, X, Y):\n",
    "            self.X = X\n",
    "            self.y = Y\n",
    "            \n",
    "    def __len__(self):\n",
    "        return len(self.y)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return torch.from_numpy(self.X[idx][0].astype(np.int32)).to(device), self.y[idx], self.X[idx][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = NewsDataset(X_train, torch.LongTensor(y_train))\n",
    "valid_ds = NewsDataset(X_valid, torch.LongTensor(y_valid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_class_distribution(obj):\n",
    "    count_dict = {\n",
    "        \"0\":0,\n",
    "        \"1\":0,\n",
    "        \"2\":0,\n",
    "        \"3\":0\n",
    "    }\n",
    "    \n",
    "    for i in obj:\n",
    "        if i == 0:\n",
    "            count_dict[\"0\"] += 1\n",
    "        elif i == 1:\n",
    "            count_dict[\"1\"] += 1\n",
    "        elif i == 2:\n",
    "            count_dict[\"2\"] += 1\n",
    "        else:\n",
    "            count_dict[\"3\"] += 1\n",
    "    return count_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_list = []\n",
    "\n",
    "for _, t, s in train_ds:\n",
    "    target_list.append(t)\n",
    "\n",
    "target_list = torch.tensor(target_list)\n",
    "target_list = target_list[torch.randperm(len(target_list))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([6.9541e-04, 6.6573e-05, 1.7331e-04, 2.2727e-03])\n"
     ]
    }
   ],
   "source": [
    "class_count = [i for i in get_class_distribution(y_train).values()]\n",
    "class_weights = 1./torch.tensor(class_count, dtype=torch.float)\n",
    "\n",
    "print(class_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_weights_all = class_weights[target_list]\n",
    "\n",
    "weighted_sampler = WeightedRandomSampler(\n",
    "    weights = class_weights_all, \n",
    "    num_samples = len(class_weights_all),\n",
    "    replacement = True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model):\n",
    "    parameters = filter(lambda p: p.requires_grad, model.parameters())\n",
    "    optimizer = torch.optim.Adam(parameters, lr=LEARNING_RATE)\n",
    "    criterion = nn.CrossEntropyLoss(weight=class_weights.to(device))\n",
    "    for i in range(EPOCHS):\n",
    "        model.train()\n",
    "        sum_loss = 0.0\n",
    "        total = 0\n",
    "        for x, y, l in train_dl:\n",
    "            x = x.long().to(device)\n",
    "            y = y.long().to(device)\n",
    "            y_pred = model(x, l)\n",
    "            optimizer.zero_grad()\n",
    "            #loss = F.cross_entropy(y_pred, y)\n",
    "            loss = criterion(y_pred, y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            sum_loss += loss.item() *y.shape[0]\n",
    "            total += y.shape[0]\n",
    "        val_loss, val_acc, val_rmse, y_pred_list = validation_metrics(model, val_dl)\n",
    "        if i%5 == 1:\n",
    "            print(\"train loss %.3f, val loss %.3f, val accuracy %.3f, and val rmse %.3f\" % (sum_loss/total, val_loss, val_acc, val_rmse))\n",
    "    print(classification_report(y_valid, y_pred_list))\n",
    "def validation_metrics (model, valid_dl):\n",
    "    #criterion = nn.CrossEntropyLoss(weight=class_weights.to(\"cpu\"))\n",
    "    zero_pred = 0\n",
    "    one_pred = 0\n",
    "    two_pred = 0\n",
    "    three_pred =0\n",
    "    zero = 0\n",
    "    one = 0\n",
    "    two = 0\n",
    "    three = 0\n",
    "    y_pred_list = list()\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    sum_loss = 0.0\n",
    "    sum_rmse = 0.0\n",
    "    for x, y, l in valid_dl:\n",
    "        x = x.long().to(device)\n",
    "        y = y.long()\n",
    "        #print(y)\n",
    "        y_hat = model(x, l).cpu()\n",
    "        loss = F.cross_entropy(y_hat, y)\n",
    "        #loss = criterion(y_hat, y)\n",
    "        pred = torch.max(y_hat, 1)[1]\n",
    "        #y_pred_list.append(pred)\n",
    "        #print(\"y\", y, \"pred: \", pred)\n",
    "        for i in y:\n",
    "            tmp = int(i.numpy())\n",
    "            if tmp is 0:\n",
    "                zero += 1\n",
    "            elif tmp is 1:\n",
    "                one += 1\n",
    "            elif tmp is 2:\n",
    "                two += 1\n",
    "            else:\n",
    "                three += 1\n",
    "        for i in pred:\n",
    "            tmp = int(i.numpy())\n",
    "            y_pred_list.append(tmp)\n",
    "            if tmp is 0:\n",
    "                zero_pred += 1\n",
    "            elif tmp is 1:\n",
    "                one_pred += 1\n",
    "            elif tmp is 2:\n",
    "                two_pred += 1\n",
    "            else:\n",
    "                three_pred += 1\n",
    "        correct += (pred == y).float().sum()\n",
    "        total += y.shape[0]\n",
    "        #print(\"correct: \", correct, \" total: \", total)\n",
    "        #print(classification_report(y, pred))\n",
    "        sum_loss += loss.item()*y.shape[0]\n",
    "        sum_rmse += np.sqrt(mean_squared_error(pred, y.unsqueeze(-1)))*y.shape[0]\n",
    "    #print(\"pred: \", zero_pred, one_pred, two_pred, three_pred)\n",
    "    #print(\"actual: \", zero, one, two, three)\n",
    "    return sum_loss/total, correct/total, sum_rmse/total, y_pred_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model_regr(model):\n",
    "    parameters = filter(lambda p: p.requires_grad, model.parameters())\n",
    "    optimizer = torch.optim.Adam(parameters, lr=LEARNING_RATE)\n",
    "    criterion = nn.CrossEntropyLoss(weight=class_weights.to(device))\n",
    "    for i in range(EPOCHS):\n",
    "        model.to(\"cuda:0\")\n",
    "        model.train()\n",
    "        sum_loss = 0.0\n",
    "        total = 0\n",
    "        for x, y, l in train_dl:\n",
    "            x = x.long().to(device)\n",
    "            y = y.float().to(device)\n",
    "            y_pred = model(x, l)\n",
    "            optimizer.zero_grad()\n",
    "            loss = F.cross_entropy(y_pred, y)\n",
    "            loss = criterion(y_pred, y.long())\n",
    "            #loss = F.mse_loss(y_pred, y.unsqueeze(-1))\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            sum_loss += loss.item() *y.shape[0]\n",
    "            total += y.shape[0]\n",
    "        val_loss = validation_metrics_regr(model.to(\"cpu\"), val_dl)\n",
    "        if i%5 == 1:\n",
    "            print(\"train mse %.3f, val rmse %.3f\" % (sum_loss/total, val_loss))\n",
    "def validation_metrics_regr (model, valid_dl):\n",
    "    #criterion = nn.CrossEntropyLoss(weight=class_weights.to(\"cpu\"))\n",
    "    model.to(\"cpu\")\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    sum_loss = 0.0\n",
    "    for x, y, l in valid_dl:\n",
    "        x = x.long()\n",
    "        y = y.float()\n",
    "        y_hat = model(x.cpu(), l)\n",
    "        loss =np.sqrt(F.mse_loss(y_hat, y.unsqueeze(-1)).item())\n",
    "        #loss = criterion(y_hat, y)\n",
    "        total += y.shape[0]\n",
    "        sum_loss += loss.item()*y.shape[0]\n",
    "    return sum_loss/total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = len(words)\n",
    "#train_dl = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True)\n",
    "train_dl = DataLoader(train_ds, batch_size=BATCH_SIZE, sampler=weighted_sampler)\n",
    "val_dl = DataLoader(valid_ds, batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTM_fixed_len(torch.nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim):\n",
    "        super().__init__()\n",
    "        self.embeddings = nn.Embedding(vocab_size, embedding_dim, padding_idx=0).to(device)\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, batch_first=True).to(device)\n",
    "        self.linear = nn.Linear(hidden_dim, 4).to(device)\n",
    "        self.dropout = nn.Dropout(0.2)\n",
    "        \n",
    "    def forward(self, x, l):\n",
    "        x = self.embeddings(x)\n",
    "        x = self.dropout(x)\n",
    "        lstm_out, (ht, ct) = self.lstm(x)\n",
    "        return self.linear(ht[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTM_variable_len(torch.nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim):\n",
    "        super().__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.dropout = nn.Dropout(0.3)\n",
    "        self.embeddings = nn.Embedding(vocab_size, embedding_dim, padding_idx=0)\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, batch_first=True)\n",
    "        self.linear = nn.Linear(hidden_dim, 4)\n",
    "        \n",
    "    def forward(self, x, s):\n",
    "        x = self.embeddings(x)\n",
    "        x = self.dropout(x)\n",
    "        x_pack = pack_padded_sequence(x, s, batch_first=True, enforce_sorted=False)\n",
    "        out_pack, (ht, ct) = self.lstm(x_pack)\n",
    "        out = self.linear(ht[-1])\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTM_regr(torch.nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim):\n",
    "        super().__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.dropout = nn.Dropout(0.2)\n",
    "        self.embeddings = nn.Embedding(vocab_size, embedding_dim, padding_idx=0).to(device)\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, batch_first=True).to(device)\n",
    "        self.linear = nn.Linear(hidden_dim,1).to(device)\n",
    "        \n",
    "    def forward(self, x, s):\n",
    "        x = self.embeddings(x)\n",
    "        x = self.dropout(x)\n",
    "        out_pack, (ht, ct) = self.lstm(x)\n",
    "        out = self.linear(ht[-1])\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTM_glove_vecs(torch.nn.Module) :\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, glove_weights) :\n",
    "        super().__init__()\n",
    "        self.embeddings = nn.Embedding(vocab_size, embedding_dim, padding_idx=0)\n",
    "        self.embeddings.weight.data.copy_(torch.from_numpy(glove_weights))\n",
    "        self.embeddings.weight.requires_grad = False ## freeze embeddings\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, batch_first=True)\n",
    "        self.linear = nn.Linear(hidden_dim, 4)\n",
    "        self.dropout = nn.Dropout(0.2)\n",
    "        \n",
    "    def forward(self, x, l):\n",
    "        x = self.embeddings(x)\n",
    "        x = self.dropout(x)\n",
    "        lstm_out, (ht, ct) = self.lstm(x)\n",
    "        return self.linear(ht[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model_fixed = LSTM_fixed_len(vocab_size, 50, 50).to(device)\n",
    "#model_variable = LSTM_variable_len(vocab_size, 50, 50)\n",
    "#model_reg = LSTM_regr(vocab_size, 50, 50).to(device)\n",
    "model = LSTM_glove_vecs(vocab_size, 100, 100, pretrained_weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Full title + content no punc [4 quantiles]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss 1.358, val loss 1.355, val accuracy 0.307, and val rmse 1.525\n",
      "train loss 1.202, val loss 1.284, val accuracy 0.406, and val rmse 1.317\n",
      "train loss 1.060, val loss 1.443, val accuracy 0.416, and val rmse 1.297\n",
      "train loss 0.959, val loss 1.440, val accuracy 0.430, and val rmse 1.240\n",
      "train loss 0.885, val loss 1.675, val accuracy 0.430, and val rmse 1.246\n",
      "train loss 0.812, val loss 1.774, val accuracy 0.434, and val rmse 1.238\n",
      "train loss 0.749, val loss 1.904, val accuracy 0.435, and val rmse 1.245\n",
      "train loss 0.707, val loss 1.865, val accuracy 0.454, and val rmse 1.195\n",
      "train loss 0.667, val loss 2.065, val accuracy 0.445, and val rmse 1.226\n",
      "train loss 0.639, val loss 1.890, val accuracy 0.466, and val rmse 1.148\n",
      "train loss 0.596, val loss 2.111, val accuracy 0.459, and val rmse 1.181\n",
      "train loss 0.569, val loss 2.005, val accuracy 0.471, and val rmse 1.133\n",
      "train loss 0.547, val loss 2.081, val accuracy 0.471, and val rmse 1.137\n",
      "train loss 0.518, val loss 1.968, val accuracy 0.482, and val rmse 1.095\n",
      "train loss 0.498, val loss 2.126, val accuracy 0.483, and val rmse 1.109\n",
      "train loss 0.480, val loss 2.429, val accuracy 0.463, and val rmse 1.169\n",
      "train loss 0.461, val loss 2.201, val accuracy 0.479, and val rmse 1.131\n",
      "train loss 0.455, val loss 2.565, val accuracy 0.459, and val rmse 1.198\n",
      "train loss 0.432, val loss 2.561, val accuracy 0.461, and val rmse 1.197\n",
      "train loss 0.423, val loss 2.136, val accuracy 0.494, and val rmse 1.068\n",
      "train loss 0.406, val loss 2.373, val accuracy 0.493, and val rmse 1.092\n",
      "train loss 0.396, val loss 2.301, val accuracy 0.495, and val rmse 1.081\n",
      "train loss 0.390, val loss 2.402, val accuracy 0.491, and val rmse 1.084\n",
      "train loss 0.380, val loss 2.315, val accuracy 0.494, and val rmse 1.070\n",
      "train loss 0.370, val loss 2.423, val accuracy 0.492, and val rmse 1.079\n",
      "train loss 0.369, val loss 2.347, val accuracy 0.498, and val rmse 1.054\n",
      "train loss 0.358, val loss 2.423, val accuracy 0.496, and val rmse 1.059\n",
      "train loss 0.352, val loss 2.373, val accuracy 0.499, and val rmse 1.042\n",
      "train loss 0.343, val loss 2.452, val accuracy 0.494, and val rmse 1.060\n",
      "train loss 0.338, val loss 2.569, val accuracy 0.495, and val rmse 1.064\n",
      "train loss 0.339, val loss 2.661, val accuracy 0.496, and val rmse 1.072\n",
      "train loss 0.327, val loss 2.522, val accuracy 0.497, and val rmse 1.043\n",
      "train loss 0.326, val loss 2.553, val accuracy 0.498, and val rmse 1.042\n",
      "train loss 0.321, val loss 2.508, val accuracy 0.498, and val rmse 1.040\n",
      "train loss 0.313, val loss 2.576, val accuracy 0.499, and val rmse 1.036\n",
      "train loss 0.310, val loss 2.656, val accuracy 0.497, and val rmse 1.050\n",
      "train loss 0.308, val loss 2.467, val accuracy 0.496, and val rmse 1.006\n",
      "train loss 0.305, val loss 2.682, val accuracy 0.495, and val rmse 1.031\n",
      "train loss 0.301, val loss 2.888, val accuracy 0.499, and val rmse 1.062\n",
      "train loss 0.298, val loss 2.700, val accuracy 0.502, and val rmse 1.032\n",
      "train loss 0.297, val loss 2.669, val accuracy 0.497, and val rmse 1.052\n",
      "train loss 0.291, val loss 2.713, val accuracy 0.496, and val rmse 1.032\n",
      "train loss 0.291, val loss 2.751, val accuracy 0.501, and val rmse 1.042\n",
      "train loss 0.282, val loss 2.855, val accuracy 0.492, and val rmse 1.054\n",
      "train loss 0.284, val loss 2.904, val accuracy 0.495, and val rmse 1.044\n",
      "train loss 0.279, val loss 2.883, val accuracy 0.501, and val rmse 1.036\n",
      "train loss 0.280, val loss 2.896, val accuracy 0.493, and val rmse 1.044\n",
      "train loss 0.275, val loss 2.925, val accuracy 0.494, and val rmse 1.041\n",
      "train loss 0.272, val loss 2.956, val accuracy 0.491, and val rmse 1.043\n",
      "train loss 0.271, val loss 3.017, val accuracy 0.491, and val rmse 1.053\n",
      "train loss 0.267, val loss 2.952, val accuracy 0.497, and val rmse 1.034\n",
      "train loss 0.266, val loss 2.884, val accuracy 0.499, and val rmse 1.017\n",
      "train loss 0.268, val loss 2.948, val accuracy 0.487, and val rmse 1.028\n",
      "train loss 0.265, val loss 2.952, val accuracy 0.497, and val rmse 1.019\n",
      "train loss 0.262, val loss 3.003, val accuracy 0.498, and val rmse 1.029\n",
      "train loss 0.261, val loss 3.134, val accuracy 0.498, and val rmse 1.039\n",
      "train loss 0.260, val loss 3.068, val accuracy 0.492, and val rmse 1.038\n",
      "train loss 0.258, val loss 3.149, val accuracy 0.493, and val rmse 1.033\n",
      "train loss 0.259, val loss 3.103, val accuracy 0.495, and val rmse 1.020\n",
      "train loss 0.256, val loss 3.364, val accuracy 0.492, and val rmse 1.038\n",
      "train loss 0.250, val loss 3.154, val accuracy 0.493, and val rmse 1.024\n",
      "train loss 0.250, val loss 3.340, val accuracy 0.497, and val rmse 1.028\n",
      "train loss 0.250, val loss 3.332, val accuracy 0.493, and val rmse 1.020\n",
      "train loss 0.250, val loss 3.309, val accuracy 0.495, and val rmse 1.020\n",
      "train loss 0.248, val loss 3.406, val accuracy 0.494, and val rmse 1.017\n",
      "train loss 0.248, val loss 3.405, val accuracy 0.493, and val rmse 1.035\n",
      "train loss 0.246, val loss 3.385, val accuracy 0.495, and val rmse 1.023\n",
      "train loss 0.248, val loss 3.248, val accuracy 0.494, and val rmse 1.030\n",
      "train loss 0.244, val loss 3.278, val accuracy 0.494, and val rmse 1.017\n",
      "train loss 0.243, val loss 3.372, val accuracy 0.494, and val rmse 1.017\n",
      "train loss 0.243, val loss 3.332, val accuracy 0.496, and val rmse 1.021\n",
      "train loss 0.241, val loss 3.412, val accuracy 0.494, and val rmse 1.020\n",
      "train loss 0.242, val loss 3.447, val accuracy 0.499, and val rmse 1.022\n",
      "train loss 0.239, val loss 3.399, val accuracy 0.497, and val rmse 0.990\n",
      "train loss 0.239, val loss 3.390, val accuracy 0.496, and val rmse 1.020\n",
      "train loss 0.239, val loss 3.424, val accuracy 0.499, and val rmse 1.018\n",
      "train loss 0.238, val loss 3.377, val accuracy 0.494, and val rmse 1.025\n",
      "train loss 0.235, val loss 3.437, val accuracy 0.494, and val rmse 1.017\n",
      "train loss 0.239, val loss 3.628, val accuracy 0.493, and val rmse 1.042\n",
      "train loss 0.234, val loss 3.550, val accuracy 0.494, and val rmse 1.031\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.63      0.58      0.60      1454\n",
      "         1.0       0.41      0.41      0.41      1464\n",
      "         2.0       0.41      0.41      0.41      1442\n",
      "         3.0       0.54      0.58      0.56      1432\n",
      "\n",
      "    accuracy                           0.49      5792\n",
      "   macro avg       0.50      0.49      0.50      5792\n",
      "weighted avg       0.50      0.49      0.49      5792\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train_model(model_variable.to(device))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## title + content, no punc [log10 scale]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss 1.214, val loss 1.246, val accuracy 0.368, and val rmse 1.038\n",
      "train loss 0.817, val loss 1.468, val accuracy 0.402, and val rmse 0.947\n",
      "train loss 0.657, val loss 1.782, val accuracy 0.315, and val rmse 1.104\n",
      "train loss 0.562, val loss 1.660, val accuracy 0.370, and val rmse 0.953\n",
      "train loss 0.502, val loss 1.595, val accuracy 0.424, and val rmse 0.909\n",
      "train loss 0.412, val loss 1.759, val accuracy 0.437, and val rmse 0.879\n",
      "train loss 0.404, val loss 1.910, val accuracy 0.393, and val rmse 0.934\n",
      "train loss 0.351, val loss 1.689, val accuracy 0.447, and val rmse 0.862\n",
      "train loss 0.388, val loss 1.611, val accuracy 0.494, and val rmse 0.852\n",
      "train loss 0.313, val loss 1.914, val accuracy 0.449, and val rmse 0.849\n",
      "train loss 0.288, val loss 1.618, val accuracy 0.491, and val rmse 0.852\n",
      "train loss 0.281, val loss 1.536, val accuracy 0.520, and val rmse 0.809\n",
      "train loss 0.260, val loss 1.522, val accuracy 0.529, and val rmse 0.796\n",
      "train loss 0.244, val loss 1.738, val accuracy 0.502, and val rmse 0.802\n",
      "train loss 0.233, val loss 2.005, val accuracy 0.423, and val rmse 0.895\n",
      "train loss 0.245, val loss 1.649, val accuracy 0.536, and val rmse 0.756\n",
      "train loss 0.211, val loss 2.039, val accuracy 0.442, and val rmse 0.860\n",
      "train loss 0.219, val loss 1.566, val accuracy 0.559, and val rmse 0.777\n",
      "train loss 0.197, val loss 1.602, val accuracy 0.550, and val rmse 0.748\n",
      "train loss 0.207, val loss 1.493, val accuracy 0.591, and val rmse 0.703\n",
      "train loss 0.204, val loss 1.717, val accuracy 0.537, and val rmse 0.762\n",
      "train loss 0.194, val loss 1.900, val accuracy 0.506, and val rmse 0.798\n",
      "train loss 0.183, val loss 1.518, val accuracy 0.578, and val rmse 0.734\n",
      "train loss 0.188, val loss 1.515, val accuracy 0.591, and val rmse 0.714\n",
      "train loss 0.181, val loss 1.554, val accuracy 0.584, and val rmse 0.721\n",
      "train loss 0.168, val loss 1.573, val accuracy 0.589, and val rmse 0.700\n",
      "train loss 0.172, val loss 1.567, val accuracy 0.604, and val rmse 0.700\n",
      "train loss 0.174, val loss 1.660, val accuracy 0.578, and val rmse 0.717\n",
      "train loss 0.167, val loss 1.680, val accuracy 0.572, and val rmse 0.772\n",
      "train loss 0.157, val loss 1.583, val accuracy 0.600, and val rmse 0.691\n",
      "train loss 0.164, val loss 1.537, val accuracy 0.596, and val rmse 0.704\n",
      "train loss 0.158, val loss 1.585, val accuracy 0.602, and val rmse 0.698\n",
      "train loss 0.161, val loss 1.625, val accuracy 0.587, and val rmse 0.704\n",
      "train loss 0.151, val loss 1.527, val accuracy 0.625, and val rmse 0.669\n",
      "train loss 0.150, val loss 1.508, val accuracy 0.627, and val rmse 0.666\n",
      "train loss 0.167, val loss 1.662, val accuracy 0.596, and val rmse 0.700\n",
      "train loss 0.148, val loss 1.577, val accuracy 0.615, and val rmse 0.688\n",
      "train loss 0.151, val loss 1.479, val accuracy 0.632, and val rmse 0.648\n",
      "train loss 0.154, val loss 1.536, val accuracy 0.631, and val rmse 0.653\n",
      "train loss 0.147, val loss 1.565, val accuracy 0.615, and val rmse 0.692\n",
      "train loss 0.134, val loss 1.526, val accuracy 0.641, and val rmse 0.647\n",
      "train loss 0.144, val loss 1.634, val accuracy 0.620, and val rmse 0.659\n",
      "train loss 0.147, val loss 1.541, val accuracy 0.634, and val rmse 0.661\n",
      "train loss 0.142, val loss 1.751, val accuracy 0.584, and val rmse 0.715\n",
      "train loss 0.137, val loss 1.610, val accuracy 0.637, and val rmse 0.656\n",
      "train loss 0.132, val loss 1.640, val accuracy 0.625, and val rmse 0.674\n",
      "train loss 0.130, val loss 1.624, val accuracy 0.631, and val rmse 0.661\n",
      "train loss 0.131, val loss 1.554, val accuracy 0.635, and val rmse 0.655\n",
      "train loss 0.134, val loss 1.668, val accuracy 0.630, and val rmse 0.658\n",
      "train loss 0.127, val loss 1.707, val accuracy 0.613, and val rmse 0.680\n",
      "train loss 0.134, val loss 1.609, val accuracy 0.643, and val rmse 0.658\n",
      "train loss 0.132, val loss 1.667, val accuracy 0.633, and val rmse 0.663\n",
      "train loss 0.133, val loss 1.505, val accuracy 0.631, and val rmse 0.659\n",
      "train loss 0.132, val loss 1.590, val accuracy 0.643, and val rmse 0.652\n",
      "train loss 0.141, val loss 1.571, val accuracy 0.643, and val rmse 0.657\n",
      "train loss 0.124, val loss 1.604, val accuracy 0.644, and val rmse 0.645\n",
      "train loss 0.124, val loss 1.632, val accuracy 0.630, and val rmse 0.663\n",
      "train loss 0.123, val loss 1.630, val accuracy 0.648, and val rmse 0.640\n",
      "train loss 0.126, val loss 1.636, val accuracy 0.649, and val rmse 0.646\n",
      "train loss 0.125, val loss 1.635, val accuracy 0.640, and val rmse 0.659\n",
      "train loss 0.128, val loss 1.668, val accuracy 0.634, and val rmse 0.662\n",
      "train loss 0.124, val loss 1.684, val accuracy 0.641, and val rmse 0.651\n",
      "train loss 0.116, val loss 1.713, val accuracy 0.649, and val rmse 0.642\n",
      "train loss 0.124, val loss 1.662, val accuracy 0.648, and val rmse 0.648\n",
      "train loss 0.114, val loss 1.691, val accuracy 0.661, and val rmse 0.630\n",
      "train loss 0.132, val loss 1.590, val accuracy 0.638, and val rmse 0.655\n",
      "train loss 0.121, val loss 1.689, val accuracy 0.643, and val rmse 0.658\n",
      "train loss 0.124, val loss 1.660, val accuracy 0.635, and val rmse 0.660\n",
      "train loss 0.123, val loss 1.676, val accuracy 0.644, and val rmse 0.649\n",
      "train loss 0.114, val loss 1.750, val accuracy 0.653, and val rmse 0.642\n",
      "train loss 0.122, val loss 1.703, val accuracy 0.641, and val rmse 0.652\n",
      "train loss 0.119, val loss 1.632, val accuracy 0.631, and val rmse 0.669\n",
      "train loss 0.117, val loss 1.702, val accuracy 0.640, and val rmse 0.652\n",
      "train loss 0.117, val loss 1.733, val accuracy 0.648, and val rmse 0.650\n",
      "train loss 0.124, val loss 1.705, val accuracy 0.638, and val rmse 0.661\n",
      "train loss 0.128, val loss 1.685, val accuracy 0.642, and val rmse 0.659\n",
      "train loss 0.123, val loss 1.738, val accuracy 0.639, and val rmse 0.664\n",
      "train loss 0.121, val loss 1.706, val accuracy 0.643, and val rmse 0.657\n",
      "train loss 0.123, val loss 1.680, val accuracy 0.659, and val rmse 0.640\n",
      "train loss 0.120, val loss 1.751, val accuracy 0.646, and val rmse 0.645\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.35      0.50      0.41       375\n",
      "         1.0       0.78      0.74      0.76      3727\n",
      "         2.0       0.52      0.51      0.51      1425\n",
      "         3.0       0.12      0.15      0.13       116\n",
      "\n",
      "    accuracy                           0.66      5643\n",
      "   macro avg       0.44      0.47      0.45      5643\n",
      "weighted avg       0.67      0.66      0.66      5643\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train_model(model_variable.to(device))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## title + content no punc, GloVe Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss 1.322, val loss 1.348, val accuracy 0.387, and val rmse 1.128\n",
      "train loss 1.139, val loss 1.288, val accuracy 0.471, and val rmse 0.941\n",
      "train loss 1.018, val loss 1.261, val accuracy 0.516, and val rmse 0.894\n",
      "train loss 0.958, val loss 1.189, val accuracy 0.552, and val rmse 0.776\n",
      "train loss 0.883, val loss 1.252, val accuracy 0.534, and val rmse 0.817\n",
      "train loss 0.841, val loss 1.168, val accuracy 0.588, and val rmse 0.735\n",
      "train loss 0.859, val loss 1.228, val accuracy 0.545, and val rmse 0.777\n",
      "train loss 0.793, val loss 1.185, val accuracy 0.587, and val rmse 0.732\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA error: unspecified launch failure",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-43-b6bbceada84a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mtrain_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-28-034fe1448ff7>\u001b[0m in \u001b[0;36mtrain_model\u001b[1;34m(model)\u001b[0m\n\u001b[0;32m     15\u001b[0m             \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_pred\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m             \u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 17\u001b[1;33m             \u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     18\u001b[0m             \u001b[0msum_loss\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m             \u001b[0mtotal\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mE:\\Programs\\lib\\site-packages\\torch\\autograd\\grad_mode.py\u001b[0m in \u001b[0;36mdecorate_context\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     13\u001b[0m         \u001b[1;32mdef\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m             \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 15\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     16\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mE:\\Programs\\lib\\site-packages\\torch\\optim\\adam.py\u001b[0m in \u001b[0;36mstep\u001b[1;34m(self, closure)\u001b[0m\n\u001b[0;32m     98\u001b[0m                 \u001b[1;31m# Decay the first and second moment running average coefficient\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     99\u001b[0m                 \u001b[0mexp_avg\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmul_\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbeta1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd_\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgrad\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0malpha\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mbeta1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 100\u001b[1;33m                 \u001b[0mexp_avg_sq\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmul_\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbeta2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0maddcmul_\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgrad\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgrad\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mbeta2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    101\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0mamsgrad\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    102\u001b[0m                     \u001b[1;31m# Maintains the maximum of all 2nd moment running avg. till now\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: CUDA error: unspecified launch failure"
     ]
    }
   ],
   "source": [
    "train_model(model.to(device))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Full content + title (no punc) [4 quantiles] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss 1.388, val loss 1.388, val accuracy 0.248, and val rmse 1.443\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.25      0.08      0.12       660\n",
      "         1.0       0.27      0.14      0.19       616\n",
      "         2.0       0.23      0.55      0.33       620\n",
      "         3.0       0.27      0.23      0.25       593\n",
      "\n",
      "    accuracy                           0.25      2489\n",
      "   macro avg       0.26      0.25      0.22      2489\n",
      "weighted avg       0.26      0.25      0.22      2489\n",
      "\n",
      "train loss 1.377, val loss 1.379, val accuracy 0.278, and val rmse 1.550\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.32      0.34      0.33       660\n",
      "         1.0       0.26      0.19      0.22       616\n",
      "         2.0       0.25      0.27      0.26       620\n",
      "         3.0       0.28      0.31      0.30       593\n",
      "\n",
      "    accuracy                           0.28      2489\n",
      "   macro avg       0.28      0.28      0.27      2489\n",
      "weighted avg       0.28      0.28      0.28      2489\n",
      "\n",
      "train loss 1.365, val loss 1.371, val accuracy 0.286, and val rmse 1.605\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.31      0.47      0.37       660\n",
      "         1.0       0.26      0.19      0.22       616\n",
      "         2.0       0.24      0.14      0.18       620\n",
      "         3.0       0.29      0.33      0.31       593\n",
      "\n",
      "    accuracy                           0.29      2489\n",
      "   macro avg       0.28      0.28      0.27      2489\n",
      "weighted avg       0.28      0.29      0.27      2489\n",
      "\n",
      "train loss 1.356, val loss 1.364, val accuracy 0.294, and val rmse 1.599\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.31      0.52      0.39       660\n",
      "         1.0       0.27      0.20      0.23       616\n",
      "         2.0       0.25      0.12      0.16       620\n",
      "         3.0       0.31      0.32      0.31       593\n",
      "\n",
      "    accuracy                           0.29      2489\n",
      "   macro avg       0.28      0.29      0.27      2489\n",
      "weighted avg       0.28      0.29      0.27      2489\n",
      "\n",
      "train loss 1.345, val loss 1.358, val accuracy 0.297, and val rmse 1.576\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.31      0.52      0.39       660\n",
      "         1.0       0.26      0.22      0.24       616\n",
      "         2.0       0.25      0.12      0.16       620\n",
      "         3.0       0.32      0.31      0.32       593\n",
      "\n",
      "    accuracy                           0.30      2489\n",
      "   macro avg       0.29      0.29      0.28      2489\n",
      "weighted avg       0.29      0.30      0.28      2489\n",
      "\n",
      "train loss 1.332, val loss 1.353, val accuracy 0.307, and val rmse 1.552\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.33      0.53      0.40       660\n",
      "         1.0       0.28      0.23      0.25       616\n",
      "         2.0       0.25      0.11      0.15       620\n",
      "         3.0       0.33      0.34      0.33       593\n",
      "\n",
      "    accuracy                           0.31      2489\n",
      "   macro avg       0.29      0.30      0.29      2489\n",
      "weighted avg       0.29      0.31      0.29      2489\n",
      "\n",
      "train loss 1.317, val loss 1.348, val accuracy 0.321, and val rmse 1.537\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.33      0.53      0.41       660\n",
      "         1.0       0.29      0.23      0.26       616\n",
      "         2.0       0.26      0.12      0.17       620\n",
      "         3.0       0.35      0.38      0.37       593\n",
      "\n",
      "    accuracy                           0.32      2489\n",
      "   macro avg       0.31      0.32      0.30      2489\n",
      "weighted avg       0.31      0.32      0.30      2489\n",
      "\n",
      "train loss 1.294, val loss 1.344, val accuracy 0.338, and val rmse 1.522\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.35      0.53      0.42       660\n",
      "         1.0       0.31      0.24      0.27       616\n",
      "         2.0       0.30      0.13      0.18       620\n",
      "         3.0       0.35      0.44      0.39       593\n",
      "\n",
      "    accuracy                           0.34      2489\n",
      "   macro avg       0.33      0.34      0.32      2489\n",
      "weighted avg       0.33      0.34      0.32      2489\n",
      "\n",
      "train loss 1.267, val loss 1.335, val accuracy 0.364, and val rmse 1.474\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.39      0.59      0.47       660\n",
      "         1.0       0.33      0.20      0.25       616\n",
      "         2.0       0.29      0.13      0.17       620\n",
      "         3.0       0.37      0.52      0.43       593\n",
      "\n",
      "    accuracy                           0.36      2489\n",
      "   macro avg       0.34      0.36      0.33      2489\n",
      "weighted avg       0.35      0.36      0.33      2489\n",
      "\n",
      "train loss 1.235, val loss 1.334, val accuracy 0.374, and val rmse 1.449\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.41      0.66      0.50       660\n",
      "         1.0       0.32      0.14      0.19       616\n",
      "         2.0       0.29      0.09      0.14       620\n",
      "         3.0       0.37      0.60      0.46       593\n",
      "\n",
      "    accuracy                           0.37      2489\n",
      "   macro avg       0.35      0.37      0.32      2489\n",
      "weighted avg       0.35      0.37      0.32      2489\n",
      "\n",
      "train loss 1.198, val loss 1.364, val accuracy 0.376, and val rmse 1.471\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.40      0.68      0.51       660\n",
      "         1.0       0.33      0.07      0.12       616\n",
      "         2.0       0.29      0.06      0.11       620\n",
      "         3.0       0.37      0.68      0.48       593\n",
      "\n",
      "    accuracy                           0.38      2489\n",
      "   macro avg       0.35      0.37      0.30      2489\n",
      "weighted avg       0.35      0.38      0.30      2489\n",
      "\n",
      "train loss 1.171, val loss 1.413, val accuracy 0.384, and val rmse 1.452\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.40      0.74      0.52       660\n",
      "         1.0       0.34      0.06      0.11       616\n",
      "         2.0       0.29      0.07      0.12       620\n",
      "         3.0       0.39      0.65      0.48       593\n",
      "\n",
      "    accuracy                           0.38      2489\n",
      "   macro avg       0.35      0.38      0.31      2489\n",
      "weighted avg       0.35      0.38      0.31      2489\n",
      "\n",
      "train loss 1.127, val loss 1.537, val accuracy 0.375, and val rmse 1.507\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.38      0.78      0.51       660\n",
      "         1.0       0.33      0.03      0.06       616\n",
      "         2.0       0.37      0.03      0.06       620\n",
      "         3.0       0.37      0.64      0.47       593\n",
      "\n",
      "    accuracy                           0.37      2489\n",
      "   macro avg       0.36      0.37      0.27      2489\n",
      "weighted avg       0.36      0.37      0.28      2489\n",
      "\n",
      "train loss 1.102, val loss 1.638, val accuracy 0.376, and val rmse 1.512\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.38      0.78      0.51       660\n",
      "         1.0       0.41      0.03      0.06       616\n",
      "         2.0       0.39      0.05      0.08       620\n",
      "         3.0       0.37      0.63      0.46       593\n",
      "\n",
      "    accuracy                           0.38      2489\n",
      "   macro avg       0.39      0.37      0.28      2489\n",
      "weighted avg       0.39      0.38      0.28      2489\n",
      "\n",
      "train loss 1.071, val loss 1.711, val accuracy 0.378, and val rmse 1.509\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.39      0.77      0.52       660\n",
      "         1.0       0.50      0.03      0.05       616\n",
      "         2.0       0.37      0.02      0.05       620\n",
      "         3.0       0.36      0.67      0.47       593\n",
      "\n",
      "    accuracy                           0.38      2489\n",
      "   macro avg       0.40      0.37      0.27      2489\n",
      "weighted avg       0.40      0.38      0.27      2489\n",
      "\n",
      "train loss 1.046, val loss 1.738, val accuracy 0.381, and val rmse 1.490\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.41      0.76      0.53       660\n",
      "         1.0       0.45      0.03      0.06       616\n",
      "         2.0       0.32      0.04      0.06       620\n",
      "         3.0       0.36      0.69      0.47       593\n",
      "\n",
      "    accuracy                           0.38      2489\n",
      "   macro avg       0.38      0.38      0.28      2489\n",
      "weighted avg       0.38      0.38      0.28      2489\n",
      "\n",
      "train loss 1.017, val loss 1.860, val accuracy 0.382, and val rmse 1.498\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.38      0.82      0.52       660\n",
      "         1.0       0.53      0.03      0.05       616\n",
      "         2.0       0.41      0.05      0.08       620\n",
      "         3.0       0.38      0.62      0.47       593\n",
      "\n",
      "    accuracy                           0.38      2489\n",
      "   macro avg       0.43      0.38      0.28      2489\n",
      "weighted avg       0.43      0.38      0.28      2489\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss 0.993, val loss 1.813, val accuracy 0.390, and val rmse 1.468\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.40      0.79      0.53       660\n",
      "         1.0       0.44      0.04      0.08       616\n",
      "         2.0       0.40      0.06      0.11       620\n",
      "         3.0       0.37      0.65      0.47       593\n",
      "\n",
      "    accuracy                           0.39      2489\n",
      "   macro avg       0.40      0.39      0.30      2489\n",
      "weighted avg       0.40      0.39      0.30      2489\n",
      "\n",
      "train loss 0.981, val loss 1.785, val accuracy 0.387, and val rmse 1.461\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.45      0.69      0.55       660\n",
      "         1.0       0.39      0.02      0.04       616\n",
      "         2.0       0.32      0.06      0.10       620\n",
      "         3.0       0.34      0.77      0.47       593\n",
      "\n",
      "    accuracy                           0.39      2489\n",
      "   macro avg       0.38      0.39      0.29      2489\n",
      "weighted avg       0.38      0.39      0.29      2489\n",
      "\n",
      "train loss 0.970, val loss 1.980, val accuracy 0.382, and val rmse 1.489\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.40      0.76      0.53       660\n",
      "         1.0       0.47      0.02      0.04       616\n",
      "         2.0       0.39      0.02      0.04       620\n",
      "         3.0       0.36      0.71      0.47       593\n",
      "\n",
      "    accuracy                           0.38      2489\n",
      "   macro avg       0.40      0.38      0.27      2489\n",
      "weighted avg       0.40      0.38      0.27      2489\n",
      "\n",
      "train loss 0.949, val loss 1.875, val accuracy 0.398, and val rmse 1.421\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.38      0.85      0.53       660\n",
      "         1.0       0.44      0.06      0.10       616\n",
      "         2.0       0.35      0.12      0.18       620\n",
      "         3.0       0.45      0.53      0.49       593\n",
      "\n",
      "    accuracy                           0.40      2489\n",
      "   macro avg       0.40      0.39      0.32      2489\n",
      "weighted avg       0.40      0.40      0.33      2489\n",
      "\n",
      "train loss 0.921, val loss 1.832, val accuracy 0.399, and val rmse 1.423\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.41      0.80      0.54       660\n",
      "         1.0       0.36      0.07      0.12       616\n",
      "         2.0       0.38      0.08      0.14       620\n",
      "         3.0       0.40      0.62      0.48       593\n",
      "\n",
      "    accuracy                           0.40      2489\n",
      "   macro avg       0.39      0.39      0.32      2489\n",
      "weighted avg       0.39      0.40      0.32      2489\n",
      "\n",
      "train loss 0.894, val loss 2.007, val accuracy 0.393, and val rmse 1.449\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.40      0.80      0.54       660\n",
      "         1.0       0.45      0.04      0.07       616\n",
      "         2.0       0.35      0.08      0.12       620\n",
      "         3.0       0.38      0.64      0.48       593\n",
      "\n",
      "    accuracy                           0.39      2489\n",
      "   macro avg       0.40      0.39      0.30      2489\n",
      "weighted avg       0.40      0.39      0.30      2489\n",
      "\n",
      "train loss 0.878, val loss 2.181, val accuracy 0.383, and val rmse 1.487\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.38      0.80      0.52       660\n",
      "         1.0       0.50      0.03      0.06       616\n",
      "         2.0       0.32      0.03      0.06       620\n",
      "         3.0       0.38      0.64      0.48       593\n",
      "\n",
      "    accuracy                           0.38      2489\n",
      "   macro avg       0.40      0.38      0.28      2489\n",
      "weighted avg       0.40      0.38      0.28      2489\n",
      "\n",
      "train loss 0.866, val loss 2.209, val accuracy 0.388, and val rmse 1.461\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.38      0.84      0.52       660\n",
      "         1.0       0.45      0.04      0.07       616\n",
      "         2.0       0.33      0.07      0.11       620\n",
      "         3.0       0.41      0.59      0.48       593\n",
      "\n",
      "    accuracy                           0.39      2489\n",
      "   macro avg       0.39      0.38      0.30      2489\n",
      "weighted avg       0.39      0.39      0.30      2489\n",
      "\n",
      "train loss 0.840, val loss 2.268, val accuracy 0.389, and val rmse 1.472\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.39      0.80      0.53       660\n",
      "         1.0       0.53      0.03      0.05       616\n",
      "         2.0       0.38      0.06      0.10       620\n",
      "         3.0       0.38      0.65      0.48       593\n",
      "\n",
      "    accuracy                           0.39      2489\n",
      "   macro avg       0.42      0.38      0.29      2489\n",
      "weighted avg       0.42      0.39      0.29      2489\n",
      "\n",
      "train loss 0.831, val loss 2.312, val accuracy 0.389, and val rmse 1.471\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.40      0.80      0.53       660\n",
      "         1.0       0.48      0.03      0.06       616\n",
      "         2.0       0.33      0.04      0.08       620\n",
      "         3.0       0.38      0.66      0.48       593\n",
      "\n",
      "    accuracy                           0.39      2489\n",
      "   macro avg       0.39      0.38      0.29      2489\n",
      "weighted avg       0.39      0.39      0.29      2489\n",
      "\n",
      "train loss 0.814, val loss 2.387, val accuracy 0.383, and val rmse 1.482\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.40      0.79      0.53       660\n",
      "         1.0       0.40      0.02      0.04       616\n",
      "         2.0       0.34      0.05      0.09       620\n",
      "         3.0       0.37      0.66      0.47       593\n",
      "\n",
      "    accuracy                           0.38      2489\n",
      "   macro avg       0.38      0.38      0.28      2489\n",
      "weighted avg       0.38      0.38      0.28      2489\n",
      "\n",
      "train loss 0.794, val loss 2.434, val accuracy 0.386, and val rmse 1.476\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.38      0.82      0.52       660\n",
      "         1.0       0.42      0.03      0.05       616\n",
      "         2.0       0.33      0.05      0.08       620\n",
      "         3.0       0.39      0.63      0.48       593\n",
      "\n",
      "    accuracy                           0.39      2489\n",
      "   macro avg       0.38      0.38      0.29      2489\n",
      "weighted avg       0.38      0.39      0.29      2489\n",
      "\n",
      "train loss 0.793, val loss 2.451, val accuracy 0.387, and val rmse 1.468\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.39      0.81      0.53       660\n",
      "         1.0       0.44      0.03      0.05       616\n",
      "         2.0       0.33      0.05      0.08       620\n",
      "         3.0       0.38      0.64      0.48       593\n",
      "\n",
      "    accuracy                           0.39      2489\n",
      "   macro avg       0.38      0.38      0.29      2489\n",
      "weighted avg       0.38      0.39      0.29      2489\n",
      "\n",
      "train loss 0.771, val loss 2.554, val accuracy 0.384, and val rmse 1.476\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.39      0.82      0.52       660\n",
      "         1.0       0.47      0.03      0.05       616\n",
      "         2.0       0.33      0.04      0.07       620\n",
      "         3.0       0.38      0.64      0.48       593\n",
      "\n",
      "    accuracy                           0.38      2489\n",
      "   macro avg       0.39      0.38      0.28      2489\n",
      "weighted avg       0.39      0.38      0.28      2489\n",
      "\n",
      "train loss 0.759, val loss 2.580, val accuracy 0.386, and val rmse 1.475\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.39      0.80      0.53       660\n",
      "         1.0       0.51      0.03      0.06       616\n",
      "         2.0       0.34      0.05      0.09       620\n",
      "         3.0       0.38      0.65      0.48       593\n",
      "\n",
      "    accuracy                           0.39      2489\n",
      "   macro avg       0.41      0.38      0.29      2489\n",
      "weighted avg       0.41      0.39      0.29      2489\n",
      "\n",
      "train loss 0.743, val loss 2.603, val accuracy 0.389, and val rmse 1.461\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.40      0.80      0.53       660\n",
      "         1.0       0.46      0.03      0.05       616\n",
      "         2.0       0.34      0.06      0.10       620\n",
      "         3.0       0.38      0.65      0.48       593\n",
      "\n",
      "    accuracy                           0.39      2489\n",
      "   macro avg       0.39      0.38      0.29      2489\n",
      "weighted avg       0.39      0.39      0.29      2489\n",
      "\n",
      "train loss 0.732, val loss 2.710, val accuracy 0.384, and val rmse 1.478\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.39      0.78      0.53       660\n",
      "         1.0       0.42      0.02      0.05       616\n",
      "         2.0       0.33      0.04      0.06       620\n",
      "         3.0       0.37      0.68      0.48       593\n",
      "\n",
      "    accuracy                           0.38      2489\n",
      "   macro avg       0.38      0.38      0.28      2489\n",
      "weighted avg       0.38      0.38      0.28      2489\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss 0.719, val loss 2.778, val accuracy 0.384, and val rmse 1.484\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.38      0.82      0.52       660\n",
      "         1.0       0.41      0.03      0.05       616\n",
      "         2.0       0.33      0.03      0.06       620\n",
      "         3.0       0.39      0.63      0.48       593\n",
      "\n",
      "    accuracy                           0.38      2489\n",
      "   macro avg       0.38      0.38      0.28      2489\n",
      "weighted avg       0.38      0.38      0.28      2489\n",
      "\n",
      "train loss 0.708, val loss 2.697, val accuracy 0.389, and val rmse 1.457\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.40      0.79      0.53       660\n",
      "         1.0       0.38      0.03      0.05       616\n",
      "         2.0       0.33      0.05      0.08       620\n",
      "         3.0       0.38      0.67      0.49       593\n",
      "\n",
      "    accuracy                           0.39      2489\n",
      "   macro avg       0.37      0.38      0.29      2489\n",
      "weighted avg       0.37      0.39      0.29      2489\n",
      "\n",
      "train loss 0.697, val loss 2.885, val accuracy 0.386, and val rmse 1.481\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.38      0.81      0.52       660\n",
      "         1.0       0.50      0.03      0.05       616\n",
      "         2.0       0.36      0.04      0.07       620\n",
      "         3.0       0.39      0.65      0.49       593\n",
      "\n",
      "    accuracy                           0.39      2489\n",
      "   macro avg       0.41      0.38      0.28      2489\n",
      "weighted avg       0.41      0.39      0.28      2489\n",
      "\n",
      "train loss 0.687, val loss 2.878, val accuracy 0.385, and val rmse 1.477\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.40      0.79      0.53       660\n",
      "         1.0       0.44      0.02      0.05       616\n",
      "         2.0       0.35      0.04      0.07       620\n",
      "         3.0       0.37      0.67      0.48       593\n",
      "\n",
      "    accuracy                           0.39      2489\n",
      "   macro avg       0.39      0.38      0.28      2489\n",
      "weighted avg       0.39      0.39      0.28      2489\n",
      "\n",
      "train loss 0.685, val loss 2.912, val accuracy 0.389, and val rmse 1.475\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.39      0.80      0.52       660\n",
      "         1.0       0.46      0.03      0.05       616\n",
      "         2.0       0.36      0.05      0.08       620\n",
      "         3.0       0.39      0.67      0.49       593\n",
      "\n",
      "    accuracy                           0.39      2489\n",
      "   macro avg       0.40      0.38      0.29      2489\n",
      "weighted avg       0.40      0.39      0.29      2489\n",
      "\n",
      "train loss 0.665, val loss 3.000, val accuracy 0.384, and val rmse 1.483\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.39      0.78      0.52       660\n",
      "         1.0       0.38      0.01      0.03       616\n",
      "         2.0       0.36      0.04      0.08       620\n",
      "         3.0       0.37      0.69      0.48       593\n",
      "\n",
      "    accuracy                           0.38      2489\n",
      "   macro avg       0.37      0.38      0.28      2489\n",
      "weighted avg       0.37      0.38      0.28      2489\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train_model(model_variable.to(device))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Full content + title (kept punc, deleted non-common words) [4 quantiles]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss 1.389, val loss 1.386, val accuracy 0.266, and val rmse 1.492\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.29      0.47      0.36       662\n",
      "         1.0       0.24      0.51      0.32       622\n",
      "         2.0       0.27      0.03      0.05       621\n",
      "         3.0       0.44      0.04      0.07       611\n",
      "\n",
      "    accuracy                           0.27      2516\n",
      "   macro avg       0.31      0.26      0.20      2516\n",
      "weighted avg       0.31      0.27      0.20      2516\n",
      "\n",
      "train loss 1.373, val loss 1.377, val accuracy 0.277, and val rmse 1.556\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.29      0.38      0.33       662\n",
      "         1.0       0.25      0.29      0.27       622\n",
      "         2.0       0.26      0.15      0.19       621\n",
      "         3.0       0.31      0.27      0.29       611\n",
      "\n",
      "    accuracy                           0.28      2516\n",
      "   macro avg       0.28      0.28      0.27      2516\n",
      "weighted avg       0.28      0.28      0.27      2516\n",
      "\n",
      "train loss 1.362, val loss 1.372, val accuracy 0.275, and val rmse 1.624\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.30      0.31      0.30       662\n",
      "         1.0       0.25      0.19      0.22       622\n",
      "         2.0       0.26      0.16      0.20       621\n",
      "         3.0       0.28      0.44      0.34       611\n",
      "\n",
      "    accuracy                           0.28      2516\n",
      "   macro avg       0.27      0.28      0.26      2516\n",
      "weighted avg       0.27      0.28      0.26      2516\n",
      "\n",
      "train loss 1.350, val loss 1.366, val accuracy 0.291, and val rmse 1.587\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.31      0.37      0.34       662\n",
      "         1.0       0.27      0.22      0.24       622\n",
      "         2.0       0.26      0.15      0.19       621\n",
      "         3.0       0.29      0.42      0.35       611\n",
      "\n",
      "    accuracy                           0.29      2516\n",
      "   macro avg       0.29      0.29      0.28      2516\n",
      "weighted avg       0.29      0.29      0.28      2516\n",
      "\n",
      "train loss 1.341, val loss 1.362, val accuracy 0.293, and val rmse 1.549\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.31      0.38      0.34       662\n",
      "         1.0       0.28      0.25      0.27       622\n",
      "         2.0       0.26      0.18      0.21       621\n",
      "         3.0       0.30      0.35      0.32       611\n",
      "\n",
      "    accuracy                           0.29      2516\n",
      "   macro avg       0.29      0.29      0.29      2516\n",
      "weighted avg       0.29      0.29      0.29      2516\n",
      "\n",
      "train loss 1.328, val loss 1.358, val accuracy 0.302, and val rmse 1.513\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.33      0.37      0.35       662\n",
      "         1.0       0.29      0.25      0.27       622\n",
      "         2.0       0.26      0.21      0.23       621\n",
      "         3.0       0.31      0.38      0.34       611\n",
      "\n",
      "    accuracy                           0.30      2516\n",
      "   macro avg       0.30      0.30      0.30      2516\n",
      "weighted avg       0.30      0.30      0.30      2516\n",
      "\n",
      "train loss 1.317, val loss 1.355, val accuracy 0.307, and val rmse 1.511\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.33      0.36      0.34       662\n",
      "         1.0       0.28      0.23      0.25       622\n",
      "         2.0       0.26      0.21      0.23       621\n",
      "         3.0       0.33      0.43      0.37       611\n",
      "\n",
      "    accuracy                           0.31      2516\n",
      "   macro avg       0.30      0.31      0.30      2516\n",
      "weighted avg       0.30      0.31      0.30      2516\n",
      "\n",
      "train loss 1.298, val loss 1.356, val accuracy 0.318, and val rmse 1.517\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.35      0.37      0.36       662\n",
      "         1.0       0.28      0.21      0.24       622\n",
      "         2.0       0.27      0.17      0.21       621\n",
      "         3.0       0.33      0.51      0.40       611\n",
      "\n",
      "    accuracy                           0.32      2516\n",
      "   macro avg       0.31      0.32      0.30      2516\n",
      "weighted avg       0.31      0.32      0.30      2516\n",
      "\n",
      "train loss 1.279, val loss 1.379, val accuracy 0.329, and val rmse 1.505\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.42      0.34      0.38       662\n",
      "         1.0       0.28      0.19      0.23       622\n",
      "         2.0       0.29      0.15      0.20       621\n",
      "         3.0       0.32      0.64      0.42       611\n",
      "\n",
      "    accuracy                           0.33      2516\n",
      "   macro avg       0.33      0.33      0.31      2516\n",
      "weighted avg       0.33      0.33      0.31      2516\n",
      "\n",
      "train loss 1.250, val loss 1.390, val accuracy 0.335, and val rmse 1.484\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.45      0.32      0.38       662\n",
      "         1.0       0.26      0.14      0.18       622\n",
      "         2.0       0.29      0.20      0.24       621\n",
      "         3.0       0.33      0.68      0.44       611\n",
      "\n",
      "    accuracy                           0.34      2516\n",
      "   macro avg       0.33      0.34      0.31      2516\n",
      "weighted avg       0.33      0.34      0.31      2516\n",
      "\n",
      "train loss 1.226, val loss 1.432, val accuracy 0.331, and val rmse 1.494\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.51      0.28      0.36       662\n",
      "         1.0       0.25      0.12      0.16       622\n",
      "         2.0       0.27      0.18      0.22       621\n",
      "         3.0       0.32      0.75      0.45       611\n",
      "\n",
      "    accuracy                           0.33      2516\n",
      "   macro avg       0.34      0.33      0.30      2516\n",
      "weighted avg       0.34      0.33      0.30      2516\n",
      "\n",
      "train loss 1.195, val loss 1.483, val accuracy 0.322, and val rmse 1.522\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.54      0.22      0.31       662\n",
      "         1.0       0.28      0.10      0.15       622\n",
      "         2.0       0.26      0.21      0.23       621\n",
      "         3.0       0.31      0.77      0.44       611\n",
      "\n",
      "    accuracy                           0.32      2516\n",
      "   macro avg       0.35      0.33      0.28      2516\n",
      "weighted avg       0.35      0.32      0.28      2516\n",
      "\n",
      "train loss 1.176, val loss 1.616, val accuracy 0.306, and val rmse 1.605\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.54      0.15      0.24       662\n",
      "         1.0       0.27      0.07      0.11       622\n",
      "         2.0       0.26      0.19      0.22       621\n",
      "         3.0       0.29      0.83      0.44       611\n",
      "\n",
      "    accuracy                           0.31      2516\n",
      "   macro avg       0.34      0.31      0.25      2516\n",
      "weighted avg       0.34      0.31      0.25      2516\n",
      "\n",
      "train loss 1.143, val loss 1.688, val accuracy 0.314, and val rmse 1.615\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.57      0.17      0.26       662\n",
      "         1.0       0.36      0.06      0.10       622\n",
      "         2.0       0.27      0.21      0.24       621\n",
      "         3.0       0.29      0.84      0.43       611\n",
      "\n",
      "    accuracy                           0.31      2516\n",
      "   macro avg       0.37      0.32      0.26      2516\n",
      "weighted avg       0.38      0.31      0.26      2516\n",
      "\n",
      "train loss 1.106, val loss 1.828, val accuracy 0.294, and val rmse 1.683\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.57      0.10      0.17       662\n",
      "         1.0       0.42      0.04      0.07       622\n",
      "         2.0       0.26      0.19      0.22       621\n",
      "         3.0       0.28      0.87      0.42       611\n",
      "\n",
      "    accuracy                           0.29      2516\n",
      "   macro avg       0.38      0.30      0.22      2516\n",
      "weighted avg       0.39      0.29      0.22      2516\n",
      "\n",
      "train loss 1.086, val loss 1.759, val accuracy 0.312, and val rmse 1.615\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.59      0.17      0.26       662\n",
      "         1.0       0.37      0.05      0.10       622\n",
      "         2.0       0.27      0.20      0.23       621\n",
      "         3.0       0.29      0.84      0.43       611\n",
      "\n",
      "    accuracy                           0.31      2516\n",
      "   macro avg       0.38      0.32      0.26      2516\n",
      "weighted avg       0.38      0.31      0.25      2516\n",
      "\n",
      "train loss 1.062, val loss 1.973, val accuracy 0.293, and val rmse 1.700\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.57      0.12      0.19       662\n",
      "         1.0       0.38      0.03      0.05       622\n",
      "         2.0       0.27      0.16      0.20       621\n",
      "         3.0       0.28      0.89      0.42       611\n",
      "\n",
      "    accuracy                           0.29      2516\n",
      "   macro avg       0.37      0.30      0.22      2516\n",
      "weighted avg       0.38      0.29      0.22      2516\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss 1.042, val loss 1.792, val accuracy 0.333, and val rmse 1.565\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.62      0.25      0.35       662\n",
      "         1.0       0.33      0.04      0.07       622\n",
      "         2.0       0.28      0.20      0.24       621\n",
      "         3.0       0.30      0.86      0.45       611\n",
      "\n",
      "    accuracy                           0.33      2516\n",
      "   macro avg       0.38      0.34      0.28      2516\n",
      "weighted avg       0.39      0.33      0.28      2516\n",
      "\n",
      "train loss 1.008, val loss 1.917, val accuracy 0.327, and val rmse 1.616\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.63      0.24      0.35       662\n",
      "         1.0       0.42      0.04      0.07       622\n",
      "         2.0       0.28      0.16      0.20       621\n",
      "         3.0       0.29      0.89      0.44       611\n",
      "\n",
      "    accuracy                           0.33      2516\n",
      "   macro avg       0.40      0.33      0.26      2516\n",
      "weighted avg       0.41      0.33      0.26      2516\n",
      "\n",
      "train loss 0.989, val loss 1.989, val accuracy 0.330, and val rmse 1.609\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.65      0.24      0.36       662\n",
      "         1.0       0.47      0.04      0.07       622\n",
      "         2.0       0.29      0.17      0.21       621\n",
      "         3.0       0.29      0.89      0.44       611\n",
      "\n",
      "    accuracy                           0.33      2516\n",
      "   macro avg       0.42      0.33      0.27      2516\n",
      "weighted avg       0.43      0.33      0.27      2516\n",
      "\n",
      "train loss 0.968, val loss 2.103, val accuracy 0.323, and val rmse 1.633\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.66      0.23      0.34       662\n",
      "         1.0       0.46      0.03      0.05       622\n",
      "         2.0       0.28      0.14      0.19       621\n",
      "         3.0       0.29      0.90      0.44       611\n",
      "\n",
      "    accuracy                           0.32      2516\n",
      "   macro avg       0.42      0.33      0.26      2516\n",
      "weighted avg       0.43      0.32      0.26      2516\n",
      "\n",
      "train loss 0.940, val loss 2.095, val accuracy 0.333, and val rmse 1.609\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.69      0.28      0.39       662\n",
      "         1.0       0.47      0.02      0.05       622\n",
      "         2.0       0.28      0.14      0.19       621\n",
      "         3.0       0.29      0.90      0.44       611\n",
      "\n",
      "    accuracy                           0.33      2516\n",
      "   macro avg       0.43      0.34      0.27      2516\n",
      "weighted avg       0.44      0.33      0.27      2516\n",
      "\n",
      "train loss 0.930, val loss 2.122, val accuracy 0.344, and val rmse 1.583\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.63      0.33      0.43       662\n",
      "         1.0       0.44      0.02      0.05       622\n",
      "         2.0       0.30      0.14      0.19       621\n",
      "         3.0       0.30      0.90      0.45       611\n",
      "\n",
      "    accuracy                           0.34      2516\n",
      "   macro avg       0.42      0.35      0.28      2516\n",
      "weighted avg       0.42      0.34      0.28      2516\n",
      "\n",
      "train loss 0.906, val loss 2.258, val accuracy 0.312, and val rmse 1.648\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.69      0.18      0.29       662\n",
      "         1.0       0.34      0.02      0.05       622\n",
      "         2.0       0.27      0.17      0.21       621\n",
      "         3.0       0.28      0.89      0.43       611\n",
      "\n",
      "    accuracy                           0.31      2516\n",
      "   macro avg       0.40      0.32      0.24      2516\n",
      "weighted avg       0.40      0.31      0.24      2516\n",
      "\n",
      "train loss 0.897, val loss 2.178, val accuracy 0.345, and val rmse 1.567\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.66      0.31      0.43       662\n",
      "         1.0       0.46      0.03      0.05       622\n",
      "         2.0       0.29      0.16      0.21       621\n",
      "         3.0       0.30      0.89      0.45       611\n",
      "\n",
      "    accuracy                           0.35      2516\n",
      "   macro avg       0.43      0.35      0.28      2516\n",
      "weighted avg       0.43      0.35      0.28      2516\n",
      "\n",
      "train loss 0.874, val loss 2.238, val accuracy 0.345, and val rmse 1.562\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.66      0.31      0.43       662\n",
      "         1.0       0.49      0.03      0.06       622\n",
      "         2.0       0.28      0.16      0.21       621\n",
      "         3.0       0.30      0.88      0.45       611\n",
      "\n",
      "    accuracy                           0.34      2516\n",
      "   macro avg       0.43      0.35      0.28      2516\n",
      "weighted avg       0.44      0.34      0.29      2516\n",
      "\n",
      "train loss 0.855, val loss 2.307, val accuracy 0.344, and val rmse 1.578\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.68      0.31      0.42       662\n",
      "         1.0       0.50      0.03      0.06       622\n",
      "         2.0       0.29      0.16      0.20       621\n",
      "         3.0       0.30      0.89      0.45       611\n",
      "\n",
      "    accuracy                           0.34      2516\n",
      "   macro avg       0.44      0.35      0.28      2516\n",
      "weighted avg       0.45      0.34      0.28      2516\n",
      "\n",
      "train loss 0.841, val loss 2.427, val accuracy 0.345, and val rmse 1.584\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.67      0.31      0.43       662\n",
      "         1.0       0.53      0.03      0.06       622\n",
      "         2.0       0.30      0.14      0.19       621\n",
      "         3.0       0.29      0.90      0.44       611\n",
      "\n",
      "    accuracy                           0.34      2516\n",
      "   macro avg       0.45      0.35      0.28      2516\n",
      "weighted avg       0.45      0.34      0.28      2516\n",
      "\n",
      "train loss 0.823, val loss 2.469, val accuracy 0.344, and val rmse 1.588\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.67      0.31      0.43       662\n",
      "         1.0       0.55      0.03      0.05       622\n",
      "         2.0       0.29      0.14      0.19       621\n",
      "         3.0       0.30      0.91      0.45       611\n",
      "\n",
      "    accuracy                           0.34      2516\n",
      "   macro avg       0.45      0.35      0.28      2516\n",
      "weighted avg       0.46      0.34      0.28      2516\n",
      "\n",
      "train loss 0.819, val loss 2.522, val accuracy 0.339, and val rmse 1.597\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.69      0.29      0.40       662\n",
      "         1.0       0.50      0.03      0.05       622\n",
      "         2.0       0.29      0.15      0.20       621\n",
      "         3.0       0.29      0.91      0.44       611\n",
      "\n",
      "    accuracy                           0.34      2516\n",
      "   macro avg       0.44      0.34      0.27      2516\n",
      "weighted avg       0.45      0.34      0.27      2516\n",
      "\n",
      "train loss 0.795, val loss 2.688, val accuracy 0.324, and val rmse 1.634\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.69      0.26      0.38       662\n",
      "         1.0       0.28      0.01      0.02       622\n",
      "         2.0       0.27      0.12      0.17       621\n",
      "         3.0       0.29      0.92      0.44       611\n",
      "\n",
      "    accuracy                           0.32      2516\n",
      "   macro avg       0.38      0.33      0.25      2516\n",
      "weighted avg       0.39      0.32      0.25      2516\n",
      "\n",
      "train loss 0.783, val loss 2.595, val accuracy 0.338, and val rmse 1.599\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.67      0.30      0.41       662\n",
      "         1.0       0.50      0.03      0.05       622\n",
      "         2.0       0.27      0.12      0.17       621\n",
      "         3.0       0.29      0.91      0.44       611\n",
      "\n",
      "    accuracy                           0.34      2516\n",
      "   macro avg       0.43      0.34      0.27      2516\n",
      "weighted avg       0.44      0.34      0.27      2516\n",
      "\n",
      "train loss 0.775, val loss 2.534, val accuracy 0.356, and val rmse 1.551\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.65      0.36      0.46       662\n",
      "         1.0       0.51      0.03      0.06       622\n",
      "         2.0       0.29      0.14      0.18       621\n",
      "         3.0       0.30      0.91      0.46       611\n",
      "\n",
      "    accuracy                           0.36      2516\n",
      "   macro avg       0.44      0.36      0.29      2516\n",
      "weighted avg       0.44      0.36      0.29      2516\n",
      "\n",
      "train loss 0.757, val loss 2.600, val accuracy 0.350, and val rmse 1.565\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.64      0.34      0.45       662\n",
      "         1.0       0.52      0.03      0.05       622\n",
      "         2.0       0.29      0.14      0.19       621\n",
      "         3.0       0.30      0.90      0.45       611\n",
      "\n",
      "    accuracy                           0.35      2516\n",
      "   macro avg       0.44      0.35      0.28      2516\n",
      "weighted avg       0.44      0.35      0.29      2516\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss 0.757, val loss 2.701, val accuracy 0.348, and val rmse 1.572\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.64      0.35      0.45       662\n",
      "         1.0       0.55      0.03      0.05       622\n",
      "         2.0       0.28      0.12      0.17       621\n",
      "         3.0       0.30      0.91      0.45       611\n",
      "\n",
      "    accuracy                           0.35      2516\n",
      "   macro avg       0.44      0.35      0.28      2516\n",
      "weighted avg       0.44      0.35      0.28      2516\n",
      "\n",
      "train loss 0.733, val loss 2.782, val accuracy 0.347, and val rmse 1.580\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.64      0.36      0.46       662\n",
      "         1.0       0.55      0.03      0.05       622\n",
      "         2.0       0.27      0.09      0.14       621\n",
      "         3.0       0.30      0.92      0.45       611\n",
      "\n",
      "    accuracy                           0.35      2516\n",
      "   macro avg       0.44      0.35      0.27      2516\n",
      "weighted avg       0.44      0.35      0.28      2516\n",
      "\n",
      "train loss 0.714, val loss 2.793, val accuracy 0.346, and val rmse 1.574\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.65      0.36      0.46       662\n",
      "         1.0       0.50      0.02      0.04       622\n",
      "         2.0       0.25      0.09      0.13       621\n",
      "         3.0       0.30      0.92      0.45       611\n",
      "\n",
      "    accuracy                           0.35      2516\n",
      "   macro avg       0.42      0.35      0.27      2516\n",
      "weighted avg       0.43      0.35      0.27      2516\n",
      "\n",
      "train loss 0.714, val loss 2.846, val accuracy 0.345, and val rmse 1.572\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.63      0.35      0.45       662\n",
      "         1.0       0.50      0.02      0.03       622\n",
      "         2.0       0.26      0.11      0.15       621\n",
      "         3.0       0.30      0.92      0.45       611\n",
      "\n",
      "    accuracy                           0.35      2516\n",
      "   macro avg       0.42      0.35      0.27      2516\n",
      "weighted avg       0.43      0.35      0.27      2516\n",
      "\n",
      "train loss 0.706, val loss 2.934, val accuracy 0.339, and val rmse 1.593\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.65      0.32      0.43       662\n",
      "         1.0       0.41      0.01      0.02       622\n",
      "         2.0       0.26      0.11      0.15       621\n",
      "         3.0       0.30      0.93      0.45       611\n",
      "\n",
      "    accuracy                           0.34      2516\n",
      "   macro avg       0.40      0.34      0.26      2516\n",
      "weighted avg       0.41      0.34      0.26      2516\n",
      "\n",
      "train loss 0.696, val loss 2.994, val accuracy 0.337, and val rmse 1.598\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.65      0.32      0.43       662\n",
      "         1.0       0.53      0.01      0.03       622\n",
      "         2.0       0.25      0.10      0.14       621\n",
      "         3.0       0.29      0.93      0.45       611\n",
      "\n",
      "    accuracy                           0.34      2516\n",
      "   macro avg       0.43      0.34      0.26      2516\n",
      "weighted avg       0.44      0.34      0.26      2516\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train_model(model_variable.to(device))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Full content + title (kept punc) [4 quantiles]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "CUDA out of memory. Tried to allocate 2.00 MiB (GPU 0; 8.00 GiB total capacity; 4.50 GiB already allocated; 0 bytes free; 5.67 GiB reserved in total by PyTorch)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-37-1c3194146ba1>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mtrain_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel_variable\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-24-eaa7f02a4075>\u001b[0m in \u001b[0;36mtrain_model\u001b[1;34m(model)\u001b[0m\n\u001b[0;32m     10\u001b[0m             \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlong\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m             \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlong\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 12\u001b[1;33m             \u001b[0my_pred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0ml\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     13\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m             \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mF\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcross_entropy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_pred\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mE:\\Programs\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    548\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    549\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 550\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    551\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    552\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-19-6d6c5d19cd6a>\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, x, s)\u001b[0m\n\u001b[0;32m     11\u001b[0m         \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0membeddings\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m         \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdropout\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 13\u001b[1;33m         \u001b[0mx_pack\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpack_padded_sequence\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0ms\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_first\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0menforce_sorted\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     14\u001b[0m         \u001b[0mout_pack\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mht\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mct\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlstm\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_pack\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m         \u001b[0mout\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mht\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mE:\\Programs\\lib\\site-packages\\torch\\nn\\utils\\rnn.py\u001b[0m in \u001b[0;36mpack_padded_sequence\u001b[1;34m(input, lengths, batch_first, enforce_sorted)\u001b[0m\n\u001b[0;32m    242\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    243\u001b[0m     \u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_sizes\u001b[0m \u001b[1;33m=\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 244\u001b[1;33m         \u001b[0m_VF\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_pack_padded_sequence\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlengths\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_first\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    245\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0m_packed_sequence_init\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_sizes\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msorted_indices\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    246\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: CUDA out of memory. Tried to allocate 2.00 MiB (GPU 0; 8.00 GiB total capacity; 4.50 GiB already allocated; 0 bytes free; 5.67 GiB reserved in total by PyTorch)"
     ]
    }
   ],
   "source": [
    "train_model(model_variable.to(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
