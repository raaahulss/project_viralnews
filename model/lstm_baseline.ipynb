{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<spacy.lang.en.English at 0x13e892f0408>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from textblob import TextBlob\n",
    "from datetime import datetime\n",
    "\n",
    "import spacy\n",
    "import torch\n",
    "import torchtext\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader, WeightedRandomSampler\n",
    "import torch.nn.functional as F\n",
    "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler    \n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "\n",
    "from collections import Counter\n",
    "import re\n",
    "import string\n",
    "import tqdm\n",
    "spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
<<<<<<< HEAD
    "EPOCHS = 400\n",
    "BATCH_SIZE = 128\n",
    "LEARNING_RATE = 0.001"
=======
    "EPOCHS = 100\n",
    "BATCH_SIZE = 128\n",
    "LEARNING_RATE = 0.0001\n",
    "NODES = 1000"
>>>>>>> b4a2e592245f7bee6a5e05a9b63fd51bb9cd5b5f
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
<<<<<<< HEAD
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in CSV\n",
    "df = pd.read_csv(\"../data/Organic_extended_finalv2.csv\", sep=\"|\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find max retweets\n",
    "max_list = list()\n",
    "for index, row in df.iterrows():\n",
    "    num_list = list()\n",
    "    num_list = {row[\"1\"], row[\"2\"],row[\"3\"], row[\"4\"],row[\"5\"], row[\"6\"]}\n",
    "    max_list.append(max(num_list))\n",
    "df[\"max_retweets\"] = max_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
=======
>>>>>>> b4a2e592245f7bee6a5e05a9b63fd51bb9cd5b5f
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "calculating mean/median\n",
      "mean:  144.079085973584\n",
      "median:  49.0\n",
      "Number of entries:  12341\n",
      "std:  556.3053103134963\n"
     ]
<<<<<<< HEAD
    }
   ],
   "source": [
    "# Find mean/median and size\n",
=======
    },
    {
     "data": {
      "text/plain": [
       "[None]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"../data/OnlineNewsPopularity_extended.csv\")\n",
>>>>>>> b4a2e592245f7bee6a5e05a9b63fd51bb9cd5b5f
    "print(\"calculating mean/median\")\n",
    "mean =  df[\"max_retweets\"].mean()\n",
    "median = df[\"max_retweets\"].median()\n",
    "print(\"mean: \", mean)\n",
    "print(\"median: \", median)\n",
    "print(\"Number of entries: \", len(df))\n",
    "df['max_retweets'].min()\n",
    "std = df.loc[:,\"max_retweets\"].std()\n",
    "print(\"std: \", std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert date strings to datetime objeccts\n",
    "date_time = list()\n",
    "for index, row in df.iterrows():\n",
    "    if(row[\"created_time\"].lower().islower()):\n",
    "        # date time w/ letter (Jun, Mon, etc)\n",
    "        date_time_obj = datetime.strptime(row[\"created_time\"], '%a %b %d %H:%M:%S +0000 %Y')\n",
    "        date_time.append(date_time_obj)\n",
    "    else:\n",
    "        # date time w/ not letters (Jun, Mon, etc)\n",
    "        date_time_obj = datetime.strptime(row[\"created_time\"], '%Y-%m-%d %H:%M:%S+00:00')\n",
    "        date_time.append(date_time_obj)\n",
    "df[\"created_datetime\"] = date_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add data for what day of week the article was published\n",
    "df['is_mon'] = 0\n",
    "df['is_tue'] = 0\n",
    "df['is_wed'] = 0\n",
    "df['is_thu'] = 0\n",
    "df['is_fri'] = 0\n",
    "df['is_sat'] = 0\n",
    "df['is_sun'] = 0\n",
    "df['is_weekend'] = 0\n",
    "\n",
    "for index, row in df.iterrows():\n",
    "    day = row[\"created_datetime\"].weekday()\n",
    "    if day is 0:\n",
    "        df.at[index,'is_sun'] = 1\n",
    "        df.at[index,'is_weekend'] = 1\n",
    "    elif day is 1:\n",
    "        df.at[index,'is_mon'] = 1\n",
    "    elif day is 2:\n",
    "        df.at[index,'is_tue'] = 1\n",
    "    elif day is 3:\n",
    "        df.at[index,'is_wed'] = 1\n",
    "    elif day is 4:\n",
    "        df.at[index,'is_thu'] = 1\n",
    "    elif day is 5:\n",
    "        df.at[index,'is_fri'] = 1\n",
    "    elif day is 6:\n",
    "        df.at[index,'is_sat'] = 1\n",
    "        df.at[index,'is_weekend'] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Subjectivity and polarity\n",
    "for index, row in df.iterrows():\n",
    "    title_score = TextBlob(row[\"title\"]).sentiment\n",
    "    content_score = TextBlob(row[\"content\"]).sentiment\n",
    "    df.at[index,'title_polarity'] = title_score[0]\n",
    "    df.at[index,'title_subjectivity'] = title_score[1]\n",
    "    df.at[index,'content_polarity'] = content_score[0]\n",
    "    df.at[index,'content_subjectivity'] = content_score[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# source\n",
    "accounts = [\"CNN\",\"The Wall Street Journal\",\"The Washington Post\",\"NBC News\",\n",
    "            \"The Associated Press\",\"ABC News\",\"Los Angeles Times\",\"The New York Times\",\"NPR\",\"TIME\",\"U.S. News\",\"USA TODAY\",\n",
    "            \"Fox News\",\"Reuters\",\"HuffPost\"]\n",
    "for i in accounts:\n",
    "    df[i] = 0\n",
    "\n",
    "\n",
    "for index, row in df.iterrows():\n",
    "    df.at[index,row[\"screen_name\"]] = 1"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 10,
=======
   "execution_count": 4,
>>>>>>> b4a2e592245f7bee6a5e05a9b63fd51bb9cd5b5f
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove punctuation from content\n",
    "for index, row in df.iterrows():\n",
    "    temp_str = row[\"content\"].translate(str.maketrans('','',string.punctuation))\n",
    "    #temp_str = ' '.join(temp_str.split()[:500])\n",
    "    #df.at[index,\"content\"] = ' '.join(temp_str.split()[:200])\n",
    "    df.at[index,\"content\"] = temp_str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# combine title and text, delete other columns\n",
    "df[\"full_text\"] = df[\"title\"] + ' ' + df[\"content\"]\n",
    "df = df[[\"full_text\", \"max_retweets\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1:  24.0 2:  49.0 3:  108.0\n"
     ]
    }
   ],
   "source": [
    "# 0 -> 0-0.25 quantile\n",
    "# 1 -> 0.26-0.50 quantile\n",
    "# 2 -> < 0.51-0.75 quantile\n",
    "# 3 -> >= 0.76-1.00 quantile\n",
    "\n",
    "\n",
    "quan_dict=df.max_retweets.quantile([0.25, 0.5, 0.75])\n",
    "one_quar = quan_dict[0.25]\n",
    "two_quar = quan_dict[0.5]\n",
    "three_quar = quan_dict[0.75]\n",
    "\n",
    "#one_quar = 50\n",
    "#two_quar = 100\n",
    "#three_quar = 500\n",
    "\n",
    "print(\"1: \", one_quar, \"2: \", two_quar, \"3: \", three_quar)\n",
    "\n",
    "df.loc[df['max_retweets'] <= one_quar, 'shares'] = 0\n",
    "df.loc[((df['max_retweets'] > one_quar) & (df['max_retweets'] <= two_quar)), 'shares'] = 1\n",
    "df.loc[((df['max_retweets'] > two_quar) & (df['max_retweets'] <= three_quar)), 'shares'] = 2\n",
    "df.loc[df['max_retweets'] > three_quar, 'shares'] = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>tweet_id</th>\n",
       "      <th>created_time</th>\n",
       "      <th>count</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>...</th>\n",
       "      <th>Los Angeles Times</th>\n",
       "      <th>The New York Times</th>\n",
       "      <th>NPR</th>\n",
       "      <th>TIME</th>\n",
       "      <th>U.S. News</th>\n",
       "      <th>USA TODAY</th>\n",
       "      <th>Fox News</th>\n",
       "      <th>Reuters</th>\n",
       "      <th>HuffPost</th>\n",
       "      <th>full_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1272217655630458881</td>\n",
       "      <td>2020-06-14 17:21:40+00:00</td>\n",
       "      <td>17</td>\n",
       "      <td>454</td>\n",
       "      <td>463.0</td>\n",
       "      <td>462.0</td>\n",
       "      <td>464.0</td>\n",
       "      <td>464.0</td>\n",
       "      <td>466.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>‘All Black Lives Matter’ painted on Hollywood ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1272216897237516289</td>\n",
       "      <td>2020-06-14 17:18:39+00:00</td>\n",
       "      <td>17</td>\n",
       "      <td>163</td>\n",
       "      <td>163.0</td>\n",
       "      <td>163.0</td>\n",
       "      <td>163.0</td>\n",
       "      <td>162.0</td>\n",
       "      <td>162.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Millions in lawsuit settlements are another hi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>1272220034065186817</td>\n",
       "      <td>2020-06-14 17:31:07+00:00</td>\n",
       "      <td>17</td>\n",
       "      <td>910</td>\n",
       "      <td>927.0</td>\n",
       "      <td>929.0</td>\n",
       "      <td>933.0</td>\n",
       "      <td>934.0</td>\n",
       "      <td>936.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Woman becomes first observant Sikh to graduate...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>1272219784743202816</td>\n",
       "      <td>2020-06-14 17:30:08+00:00</td>\n",
       "      <td>17</td>\n",
       "      <td>2352</td>\n",
       "      <td>2377.0</td>\n",
       "      <td>2381.0</td>\n",
       "      <td>2378.0</td>\n",
       "      <td>2376.0</td>\n",
       "      <td>2373.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>As Social Distancing Wanes, Cuomo Warns of Ano...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>1272220746014572545</td>\n",
       "      <td>2020-06-14 17:33:57+00:00</td>\n",
       "      <td>17</td>\n",
       "      <td>241</td>\n",
       "      <td>267.0</td>\n",
       "      <td>267.0</td>\n",
       "      <td>267.0</td>\n",
       "      <td>267.0</td>\n",
       "      <td>267.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>They lost loved ones to police violence. Georg...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 49 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0             tweet_id               created_time  count     1  \\\n",
       "0           0  1272217655630458881  2020-06-14 17:21:40+00:00     17   454   \n",
       "1           1  1272216897237516289  2020-06-14 17:18:39+00:00     17   163   \n",
       "2           2  1272220034065186817  2020-06-14 17:31:07+00:00     17   910   \n",
       "3           3  1272219784743202816  2020-06-14 17:30:08+00:00     17  2352   \n",
       "4           4  1272220746014572545  2020-06-14 17:33:57+00:00     17   241   \n",
       "\n",
       "        2       3       4       5       6  ...  Los Angeles Times  \\\n",
       "0   463.0   462.0   464.0   464.0   466.0  ...                  1   \n",
       "1   163.0   163.0   163.0   162.0   162.0  ...                  0   \n",
       "2   927.0   929.0   933.0   934.0   936.0  ...                  0   \n",
       "3  2377.0  2381.0  2378.0  2376.0  2373.0  ...                  0   \n",
       "4   267.0   267.0   267.0   267.0   267.0  ...                  1   \n",
       "\n",
       "  The New York Times NPR  TIME U.S. News USA TODAY Fox News  Reuters  \\\n",
       "0                  0   0     0         0         0        0        0   \n",
       "1                  0   0     0         0         0        0        0   \n",
       "2                  0   0     0         0         0        0        0   \n",
       "3                  1   0     0         0         0        0        0   \n",
       "4                  0   0     0         0         0        0        0   \n",
       "\n",
       "   HuffPost                                          full_text  \n",
       "0         0  ‘All Black Lives Matter’ painted on Hollywood ...  \n",
       "1         0  Millions in lawsuit settlements are another hi...  \n",
       "2         0  Woman becomes first observant Sikh to graduate...  \n",
       "3         0  As Social Distancing Wanes, Cuomo Warns of Ano...  \n",
       "4         0  They lost loved ones to police violence. Georg...  \n",
       "\n",
       "[5 rows x 49 columns]"
      ]
     },
<<<<<<< HEAD
     "execution_count": 11,
=======
     "execution_count": 6,
>>>>>>> b4a2e592245f7bee6a5e05a9b63fd51bb9cd5b5f
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#pd.set_option('display.max_colwidth', -1)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df['content'] = df['content'].fillna('')\n",
    "df['content_length'] = df['full_text'].apply(lambda x: len(x.split()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
=======
   "execution_count": 7,
>>>>>>> b4a2e592245f7bee6a5e05a9b63fd51bb9cd5b5f
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "745.1554979337169"
      ]
     },
<<<<<<< HEAD
     "execution_count": 13,
=======
     "execution_count": 7,
>>>>>>> b4a2e592245f7bee6a5e05a9b63fd51bb9cd5b5f
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# mean content length\n",
    "np.mean(df['content_length'])"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train len: 9843 test len:  2498\n"
     ]
    }
   ],
   "source": [
    "# Split Train and Test dfs\n",
    "mask = np.random.rand(len(df)) < 0.8\n",
    "train = df[mask]\n",
    "test = df[~mask]\n",
    "print(\"train len:\", len(train), \"test len: \", len(test))"
   ]
  },
  {
   "cell_type": "code",
=======
>>>>>>> b4a2e592245f7bee6a5e05a9b63fd51bb9cd5b5f
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tokenization\n",
    "tok = spacy.load('en_core_web_sm')\n",
    "def tokenize(text):\n",
    "    text = re.sub(r\"[^\\x00-\\x7F]+\", \" \", text)\n",
    "    regex = re.compile('[' + re.escape(string.punctuation) + '0-9\\\\r\\\\t\\\\n]')\n",
    "    nopunct = regex.sub(\" \", text.lower())\n",
    "    return [token.text for token in tok.tokenizer(nopunct)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#count number of occurences of each word\n",
    "counts = Counter()\n",
    "for index, row in train.iterrows():\n",
    "    counts.update(tokenize(row['full_text']))"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 11,
=======
   "execution_count": 10,
>>>>>>> b4a2e592245f7bee6a5e05a9b63fd51bb9cd5b5f
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_words before: 62842\n",
      "num_words after: 47531\n"
     ]
    }
   ],
   "source": [
    "#deleting infrequnet words\n",
    "print(\"num_words before:\", len(counts.keys()))\n",
    "for word in list(counts):\n",
    "    if counts[word] < 2:\n",
    "        del counts[word]\n",
    "print(\"num_words after:\",len(counts.keys()))"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 10,
=======
   "execution_count": 11,
>>>>>>> b4a2e592245f7bee6a5e05a9b63fd51bb9cd5b5f
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating vocab\n",
    "vocab2index = {\"\":0, \"UNK\":1}\n",
    "words = [\"\",\"UNK\"]\n",
    "for word in counts:\n",
    "    vocab2index[word] = len(words)\n",
    "    words.append(word)"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 11,
=======
   "execution_count": 12,
>>>>>>> b4a2e592245f7bee6a5e05a9b63fd51bb9cd5b5f
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_sentence(text, vocab2index, N=450):\n",
    "    tokenized = tokenize(text)\n",
    "    encoded = np.zeros(N,dtype=int)\n",
    "    enc1 = np.array([vocab2index.get(word,vocab2index[\"UNK\"]) for word in tokenized])\n",
    "    length = min(N, len(enc1))\n",
    "    encoded[:length] = enc1[:length]\n",
    "    return encoded, length"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 12,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E:\\Programs\\lib\\site-packages\\ipykernel_launcher.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n",
      "E:\\Programs\\lib\\site-packages\\ipykernel_launcher.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "train['encoded'] = train['full_text'].apply(lambda x: np.array(encode_sentence(x,vocab2index)))\n",
    "test['encoded'] = test['full_text'].apply(lambda x: np.array(encode_sentence(x,vocab2index)))\n",
    "\n",
    "#df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()\n",
    "len(df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x15e27d0a848>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.countplot(x = 'shares', data=df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
=======
   "execution_count": 13,
>>>>>>> b4a2e592245f7bee6a5e05a9b63fd51bb9cd5b5f
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>follower_count</th>\n",
       "      <th>title_len</th>\n",
       "      <th>content_len</th>\n",
       "      <th>is_mon</th>\n",
       "      <th>is_tue</th>\n",
       "      <th>is_wed</th>\n",
       "      <th>is_thu</th>\n",
       "      <th>is_fri</th>\n",
       "      <th>is_sat</th>\n",
       "      <th>...</th>\n",
       "      <th>The New York Times</th>\n",
       "      <th>NPR</th>\n",
       "      <th>TIME</th>\n",
       "      <th>U.S. News</th>\n",
       "      <th>USA TODAY</th>\n",
       "      <th>Fox News</th>\n",
       "      <th>Reuters</th>\n",
       "      <th>HuffPost</th>\n",
       "      <th>content_length</th>\n",
       "      <th>encoded</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>8</td>\n",
       "      <td>3634146.0</td>\n",
       "      <td>16</td>\n",
       "      <td>2826</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2837</td>\n",
       "      <td>[[151, 1506, 1507, 748, 67, 30, 435, 2, 34, 35...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>14</td>\n",
       "      <td>14315833.0</td>\n",
       "      <td>8</td>\n",
       "      <td>1062</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1069</td>\n",
       "      <td>[[2, 147, 3811, 86, 249, 1465, 1078, 1082, 105...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>28</td>\n",
       "      <td>7668571.0</td>\n",
       "      <td>15</td>\n",
       "      <td>999</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1014</td>\n",
       "      <td>[[5043, 2, 3956, 458, 1879, 13, 2, 1763, 2552,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>35</td>\n",
       "      <td>17862906.0</td>\n",
       "      <td>7</td>\n",
       "      <td>55</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>62</td>\n",
       "      <td>[[865, 24254, 62, 7120, 4307, 67, 13386, 7120,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>37</td>\n",
       "      <td>4102107.0</td>\n",
       "      <td>12</td>\n",
       "      <td>1619</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1626</td>\n",
       "      <td>[[86, 303, 1156, 2, 3891, 2, 192, 764, 845, 21...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 33 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    Unnamed: 0  follower_count  title_len  content_len  is_mon  is_tue  \\\n",
       "8            8       3634146.0         16         2826       0       0   \n",
       "14          14      14315833.0          8         1062       0       0   \n",
       "26          28       7668571.0         15          999       0       0   \n",
       "33          35      17862906.0          7           55       0       0   \n",
       "35          37       4102107.0         12         1619       0       0   \n",
       "\n",
       "    is_wed  is_thu  is_fri  is_sat  ...  The New York Times  NPR  TIME  \\\n",
       "8        0       0       0       1  ...                   0    0     0   \n",
       "14       0       0       0       1  ...                   0    0     0   \n",
       "26       0       0       0       1  ...                   0    0     0   \n",
       "33       0       0       0       1  ...                   0    0     0   \n",
       "35       0       0       0       1  ...                   0    0     0   \n",
       "\n",
       "    U.S. News  USA TODAY  Fox News  Reuters  HuffPost  content_length  \\\n",
       "8           0          0         0        0         0            2837   \n",
       "14          0          0         0        0         0            1069   \n",
       "26          0          0         0        0         0            1014   \n",
       "33          0          0         0        0         0              62   \n",
       "35          0          1         0        0         0            1626   \n",
       "\n",
       "                                              encoded  \n",
       "8   [[151, 1506, 1507, 748, 67, 30, 435, 2, 34, 35...  \n",
       "14  [[2, 147, 3811, 86, 249, 1465, 1078, 1082, 105...  \n",
       "26  [[5043, 2, 3956, 458, 1879, 13, 2, 1763, 2552,...  \n",
       "33  [[865, 24254, 62, 7120, 4307, 67, 13386, 7120,...  \n",
       "35  [[86, 303, 1156, 2, 3891, 2, 192, 764, 845, 21...  \n",
       "\n",
       "[5 rows x 33 columns]"
      ]
     },
<<<<<<< HEAD
     "execution_count": 30,
=======
     "execution_count": 13,
>>>>>>> b4a2e592245f7bee6a5e05a9b63fd51bb9cd5b5f
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#train = train.drop(['tweet_id','created_time','count','1','2','3','4','5','6','user_id','screen_name','title','content','url','expanded_url','created_datetime','max_retweets'], axis = 1)\n",
    "#test = test.drop(['tweet_id','created_time','count','1','2','3','4','5','6','user_id','screen_name','title','content','url','expanded_url','created_datetime','max_retweets'], axis = 1)\n",
    "#train = train.drop(['full_text'],axis=1)\n",
    "test = test.drop(['full_text'],axis=1)\n",
    "test.head()"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
=======
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "66"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
>>>>>>> b4a2e592245f7bee6a5e05a9b63fd51bb9cd5b5f
   "source": [
    "X_train, y_train = list(train['encoded']), list(train['shares'])\n",
    "X_valid, y_valid = list(test['encoded']), list(test['shares'])"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
=======
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x13eb5fcc548>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
>>>>>>> b4a2e592245f7bee6a5e05a9b63fd51bb9cd5b5f
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=0.20, random_state=6)"
=======
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.iloc[:, 0:-1]\n",
    "y = df.iloc[:, -1]"
>>>>>>> b4a2e592245f7bee6a5e05a9b63fd51bb9cd5b5f
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
=======
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>timedelta</th>\n",
       "      <th>n_tokens_title</th>\n",
       "      <th>n_tokens_content</th>\n",
       "      <th>n_unique_tokens</th>\n",
       "      <th>n_non_stop_words</th>\n",
       "      <th>n_non_stop_unique_tokens</th>\n",
       "      <th>num_hrefs</th>\n",
       "      <th>num_self_hrefs</th>\n",
       "      <th>num_imgs</th>\n",
       "      <th>num_videos</th>\n",
       "      <th>...</th>\n",
       "      <th>title_subjectivity</th>\n",
       "      <th>title_sentiment_polarity</th>\n",
       "      <th>abs_title_subjectivity</th>\n",
       "      <th>abs_title_sentiment_polarity</th>\n",
       "      <th>shares</th>\n",
       "      <th>title</th>\n",
       "      <th>content</th>\n",
       "      <th>label_by_mean</th>\n",
       "      <th>label_by_median</th>\n",
       "      <th>content_length</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>731.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>219.0</td>\n",
       "      <td>0.663594</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.815385</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>-0.187500</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.187500</td>\n",
       "      <td>0</td>\n",
       "      <td>Amazons Streaming Video Library Now a Little E...</td>\n",
       "      <td>Having trouble finding something to watch on A...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>210</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>731.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>255.0</td>\n",
       "      <td>0.604743</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.791946</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>APs Twitter to Begin Displaying Sponsored Tweets</td>\n",
       "      <td>The Associated Press is the latest news organi...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>731.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>211.0</td>\n",
       "      <td>0.575130</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.663866</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1</td>\n",
       "      <td>Apples App Store Passes 40 Billion Downloads</td>\n",
       "      <td>It looks like 2012 was a pretty good year for ...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>197</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>731.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>531.0</td>\n",
       "      <td>0.503788</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.665635</td>\n",
       "      <td>9.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>This Astronaut Is Rooting for Notre Dame Tonight</td>\n",
       "      <td>When it comes to college football NASA astrona...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>478</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>731.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>1072.0</td>\n",
       "      <td>0.415646</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.540890</td>\n",
       "      <td>19.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.454545</td>\n",
       "      <td>0.136364</td>\n",
       "      <td>0.045455</td>\n",
       "      <td>0.136364</td>\n",
       "      <td>0</td>\n",
       "      <td>New UVerse Apps Simplify Sharing Photos and Vi...</td>\n",
       "      <td>LAS VEGAS — Sharing photos and videos on your ...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>289</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39639</th>\n",
       "      <td>8.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>346.0</td>\n",
       "      <td>0.529052</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.684783</td>\n",
       "      <td>9.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1</td>\n",
       "      <td>Samsung app aims to improve autistic childrens...</td>\n",
       "      <td>While some believe smartphones and tablets may...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>285</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39640</th>\n",
       "      <td>8.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>328.0</td>\n",
       "      <td>0.696296</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.885057</td>\n",
       "      <td>9.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>48.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.300000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1</td>\n",
       "      <td>Seth Rogen James Franco are planning to livetw...</td>\n",
       "      <td>LOS ANGELES — Call it their exit Interview\\n\\n...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>89</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39641</th>\n",
       "      <td>8.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>442.0</td>\n",
       "      <td>0.516355</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.644128</td>\n",
       "      <td>24.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.454545</td>\n",
       "      <td>0.136364</td>\n",
       "      <td>0.045455</td>\n",
       "      <td>0.136364</td>\n",
       "      <td>1</td>\n",
       "      <td>App developer says Merry Christmas by paying o...</td>\n",
       "      <td>Nothing says Merry Christmas like never having...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39642</th>\n",
       "      <td>8.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>682.0</td>\n",
       "      <td>0.539493</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.692661</td>\n",
       "      <td>10.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>Two dead after Ukraine rocked by series of blasts</td>\n",
       "      <td>Ukrainians were on high alert on Saturday afte...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>823</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39643</th>\n",
       "      <td>8.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>157.0</td>\n",
       "      <td>0.701987</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.846154</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0</td>\n",
       "      <td>8 YouTube channels to watch in 2015</td>\n",
       "      <td>We collectively watch more than 6 billion hour...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>86</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>39644 rows × 65 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       timedelta  n_tokens_title  n_tokens_content  n_unique_tokens  \\\n",
       "0          731.0            12.0             219.0         0.663594   \n",
       "1          731.0             9.0             255.0         0.604743   \n",
       "2          731.0             9.0             211.0         0.575130   \n",
       "3          731.0             9.0             531.0         0.503788   \n",
       "4          731.0            13.0            1072.0         0.415646   \n",
       "...          ...             ...               ...              ...   \n",
       "39639        8.0            11.0             346.0         0.529052   \n",
       "39640        8.0            12.0             328.0         0.696296   \n",
       "39641        8.0            10.0             442.0         0.516355   \n",
       "39642        8.0             6.0             682.0         0.539493   \n",
       "39643        8.0            10.0             157.0         0.701987   \n",
       "\n",
       "       n_non_stop_words  n_non_stop_unique_tokens  num_hrefs  num_self_hrefs  \\\n",
       "0                   1.0                  0.815385        4.0             2.0   \n",
       "1                   1.0                  0.791946        3.0             1.0   \n",
       "2                   1.0                  0.663866        3.0             1.0   \n",
       "3                   1.0                  0.665635        9.0             0.0   \n",
       "4                   1.0                  0.540890       19.0            19.0   \n",
       "...                 ...                       ...        ...             ...   \n",
       "39639               1.0                  0.684783        9.0             7.0   \n",
       "39640               1.0                  0.885057        9.0             7.0   \n",
       "39641               1.0                  0.644128       24.0             1.0   \n",
       "39642               1.0                  0.692661       10.0             1.0   \n",
       "39643               1.0                  0.846154        1.0             1.0   \n",
       "\n",
       "       num_imgs  num_videos  ...  title_subjectivity  \\\n",
       "0           1.0         0.0  ...            0.500000   \n",
       "1           1.0         0.0  ...            0.000000   \n",
       "2           1.0         0.0  ...            0.000000   \n",
       "3           1.0         0.0  ...            0.000000   \n",
       "4          20.0         0.0  ...            0.454545   \n",
       "...         ...         ...  ...                 ...   \n",
       "39639       1.0         1.0  ...            0.100000   \n",
       "39640       3.0        48.0  ...            0.300000   \n",
       "39641      12.0         1.0  ...            0.454545   \n",
       "39642       1.0         0.0  ...            0.000000   \n",
       "39643       0.0         2.0  ...            0.333333   \n",
       "\n",
       "       title_sentiment_polarity  abs_title_subjectivity  \\\n",
       "0                     -0.187500                0.000000   \n",
       "1                      0.000000                0.500000   \n",
       "2                      0.000000                0.500000   \n",
       "3                      0.000000                0.500000   \n",
       "4                      0.136364                0.045455   \n",
       "...                         ...                     ...   \n",
       "39639                  0.000000                0.400000   \n",
       "39640                  1.000000                0.200000   \n",
       "39641                  0.136364                0.045455   \n",
       "39642                  0.000000                0.500000   \n",
       "39643                  0.250000                0.166667   \n",
       "\n",
       "       abs_title_sentiment_polarity  shares  \\\n",
       "0                          0.187500       0   \n",
       "1                          0.000000       0   \n",
       "2                          0.000000       1   \n",
       "3                          0.000000       0   \n",
       "4                          0.136364       0   \n",
       "...                             ...     ...   \n",
       "39639                      0.000000       1   \n",
       "39640                      1.000000       1   \n",
       "39641                      0.136364       1   \n",
       "39642                      0.000000       0   \n",
       "39643                      0.250000       0   \n",
       "\n",
       "                                                   title  \\\n",
       "0      Amazons Streaming Video Library Now a Little E...   \n",
       "1       APs Twitter to Begin Displaying Sponsored Tweets   \n",
       "2           Apples App Store Passes 40 Billion Downloads   \n",
       "3       This Astronaut Is Rooting for Notre Dame Tonight   \n",
       "4      New UVerse Apps Simplify Sharing Photos and Vi...   \n",
       "...                                                  ...   \n",
       "39639  Samsung app aims to improve autistic childrens...   \n",
       "39640  Seth Rogen James Franco are planning to livetw...   \n",
       "39641  App developer says Merry Christmas by paying o...   \n",
       "39642  Two dead after Ukraine rocked by series of blasts   \n",
       "39643                8 YouTube channels to watch in 2015   \n",
       "\n",
       "                                                 content  label_by_mean  \\\n",
       "0      Having trouble finding something to watch on A...              1   \n",
       "1      The Associated Press is the latest news organi...              1   \n",
       "2      It looks like 2012 was a pretty good year for ...              1   \n",
       "3      When it comes to college football NASA astrona...              1   \n",
       "4      LAS VEGAS — Sharing photos and videos on your ...              1   \n",
       "...                                                  ...            ...   \n",
       "39639  While some believe smartphones and tablets may...              1   \n",
       "39640  LOS ANGELES — Call it their exit Interview\\n\\n...              1   \n",
       "39641  Nothing says Merry Christmas like never having...              1   \n",
       "39642  Ukrainians were on high alert on Saturday afte...              1   \n",
       "39643  We collectively watch more than 6 billion hour...              1   \n",
       "\n",
       "       label_by_median  content_length  \n",
       "0                    1             210  \n",
       "1                    1             250  \n",
       "2                    0             197  \n",
       "3                    1             478  \n",
       "4                    1             289  \n",
       "...                ...             ...  \n",
       "39639                0             285  \n",
       "39640                0              89  \n",
       "39641                0             100  \n",
       "39642                1             823  \n",
       "39643                1              86  \n",
       "\n",
       "[39644 rows x 65 columns]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
>>>>>>> b4a2e592245f7bee6a5e05a9b63fd51bb9cd5b5f
   "source": [
    "class NewsDataset(Dataset):\n",
    "    def __init__(self, X, Y):\n",
    "            self.X = X\n",
    "            self.y = Y\n",
    "            \n",
    "    def __len__(self):\n",
    "        return len(self.y)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return torch.from_numpy(self.X[idx][0].astype(np.int32)).to(device), self.y[idx], self.X[idx][1]"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
=======
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        [[2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, ...\n",
       "1        [[12, 145, 146, 98, 12, 99, 147, 85, 6, 148, 9...\n",
       "2        [[81, 263, 264, 265, 139, 100, 78, 237, 140, 6...\n",
       "3        [[336, 81, 337, 6, 338, 339, 340, 341, 342, 34...\n",
       "4        [[171, 172, 122, 525, 519, 25, 66, 8, 526, 26,...\n",
       "                               ...                        \n",
       "39639    [[235, 44, 3032, 538, 25, 585, 1030, 124, 2370...\n",
       "39640    [[2453, 2454, 122, 3370, 81, 29, 21224, 380, 3...\n",
       "39641    [[2369, 191, 7529, 1101, 264, 507, 2, 6, 1572,...\n",
       "39642    [[75130, 571, 8, 1479, 4390, 8, 7069, 866, 100...\n",
       "39643    [[216, 5600, 7, 134, 135, 122, 271, 430, 45, 9...\n",
       "Name: encoded, Length: 39644, dtype: object"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
>>>>>>> b4a2e592245f7bee6a5e05a9b63fd51bb9cd5b5f
   "source": [
    "train_ds = NewsDataset(X_train, torch.LongTensor(y_train))\n",
    "valid_ds = NewsDataset(X_valid, torch.LongTensor(y_valid))"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 20,
=======
   "execution_count": 19,
>>>>>>> b4a2e592245f7bee6a5e05a9b63fd51bb9cd5b5f
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_class_distribution(obj):\n",
    "    count_dict = {\n",
    "        \"0\":0,\n",
    "        \"1\":0,\n",
    "        \"2\":0,\n",
    "        \"3\":0\n",
    "    }\n",
    "    \n",
    "    for i in obj:\n",
    "        if i == 0:\n",
    "            count_dict[\"0\"] += 1\n",
    "        elif i == 1:\n",
    "            count_dict[\"1\"] += 1\n",
    "        elif i == 2:\n",
    "            count_dict[\"2\"] += 1\n",
    "        else:\n",
    "            count_dict[\"3\"] += 1\n",
    "    return count_dict"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 21,
=======
   "execution_count": 20,
>>>>>>> b4a2e592245f7bee6a5e05a9b63fd51bb9cd5b5f
   "metadata": {},
   "outputs": [],
   "source": [
    "target_list = []\n",
    "\n",
    "for _, t, s in train_ds:\n",
    "    target_list.append(t)\n",
    "\n",
    "target_list = torch.tensor(target_list)\n",
    "target_list = target_list[torch.randperm(len(target_list))]"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 22,
=======
   "execution_count": 21,
>>>>>>> b4a2e592245f7bee6a5e05a9b63fd51bb9cd5b5f
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.0002, 0.0005, 0.0005, 0.0022])\n"
     ]
    }
   ],
   "source": [
    "class_count = [i for i in get_class_distribution(y_train).values()]\n",
    "class_weights = 1./torch.tensor(class_count, dtype=torch.float)\n",
    "\n",
    "print(class_weights)"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 23,
=======
   "execution_count": 22,
>>>>>>> b4a2e592245f7bee6a5e05a9b63fd51bb9cd5b5f
   "metadata": {},
   "outputs": [],
   "source": [
    "class_weights_all = class_weights[target_list]\n",
    "\n",
    "weighted_sampler = WeightedRandomSampler(\n",
    "    weights = class_weights_all, \n",
    "    num_samples = len(class_weights_all),\n",
    "    replacement = True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
=======
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
>>>>>>> b4a2e592245f7bee6a5e05a9b63fd51bb9cd5b5f
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model):\n",
    "    parameters = filter(lambda p: p.requires_grad, model.parameters())\n",
    "    optimizer = torch.optim.Adam(parameters, lr=LEARNING_RATE)\n",
<<<<<<< HEAD
    "    #criterion = nn.CrossEntropyLoss(weight=class_weights.to(device))\n",
=======
    "    actual_loss = None\n",
>>>>>>> b4a2e592245f7bee6a5e05a9b63fd51bb9cd5b5f
    "    for i in range(EPOCHS):\n",
    "        model.train()\n",
    "        sum_loss = 0.0\n",
    "        total = 0\n",
<<<<<<< HEAD
    "        for x, y, l in train_dl:\n",
=======
    "        for x, y, l in tqdm.tqdm(train_dl, total=len(train_dl)):\n",
>>>>>>> b4a2e592245f7bee6a5e05a9b63fd51bb9cd5b5f
    "            x = x.long().to(device)\n",
    "            y = y.long().to(device)\n",
    "            y_pred = model(x, l)\n",
    "            optimizer.zero_grad()\n",
    "            loss = F.cross_entropy(y_pred, y)\n",
    "            #loss = criterion(y_pred, y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            sum_loss += loss.item() *y.shape[0]\n",
    "            total += y.shape[0]\n",
<<<<<<< HEAD
    "        val_loss, val_acc, val_rmse, y_pred_list = validation_metrics(model, val_dl)\n",
=======
    "        val_loss, val_acc, val_rmse = validation_metrics(model, val_dl)\n",
    "        if actual_loss is None or actual_loss >= val_loss:\n",
    "            actual_loss = val_loss\n",
    "#             print(\"Model Dict updated\")\n",
    "            torch.save(model.state_dict(),\"./model.pt\" )\n",
    "            \n",
>>>>>>> b4a2e592245f7bee6a5e05a9b63fd51bb9cd5b5f
    "        if i%5 == 1:\n",
    "            print(\"train loss %.3f, val loss %.3f, val accuracy %.3f, and val rmse %.3f\" % (sum_loss/total, val_loss, val_acc, val_rmse))\n",
    "    print(classification_report(y_valid, y_pred_list))\n",
    "def validation_metrics (model, valid_dl):\n",
    "    #criterion = nn.CrossEntropyLoss(weight=class_weights.to(\"cpu\"))\n",
    "    zero_pred = 0\n",
    "    one_pred = 0\n",
    "    two_pred = 0\n",
    "    three_pred =0\n",
    "    zero = 0\n",
    "    one = 0\n",
    "    two = 0\n",
    "    three = 0\n",
    "    y_pred_list = list()\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    sum_loss = 0.0\n",
    "    sum_rmse = 0.0\n",
<<<<<<< HEAD
    "    for x, y, l in valid_dl:\n",
    "        x = x.long().to(device)\n",
    "        y = y.long()\n",
    "        #print(y)\n",
=======
    "    for x, y, l in tqdm.tqdm(valid_dl, total=len(valid_dl)):\n",
    "        x = x.long().to(device)\n",
    "        y = y.long()\n",
>>>>>>> b4a2e592245f7bee6a5e05a9b63fd51bb9cd5b5f
    "        y_hat = model(x, l).cpu()\n",
    "        loss = F.cross_entropy(y_hat, y)\n",
    "        #loss = criterion(y_hat, y)\n",
    "        pred = torch.max(y_hat, 1)[1]\n",
    "        #y_pred_list.append(pred)\n",
    "        #print(\"y\", y, \"pred: \", pred)\n",
    "        for i in y:\n",
    "            tmp = int(i.numpy())\n",
    "            if tmp is 0:\n",
    "                zero += 1\n",
    "            elif tmp is 1:\n",
    "                one += 1\n",
    "            elif tmp is 2:\n",
    "                two += 1\n",
    "            else:\n",
    "                three += 1\n",
    "        for i in pred:\n",
    "            tmp = int(i.numpy())\n",
    "            y_pred_list.append(tmp)\n",
    "            if tmp is 0:\n",
    "                zero_pred += 1\n",
    "            elif tmp is 1:\n",
    "                one_pred += 1\n",
    "            elif tmp is 2:\n",
    "                two_pred += 1\n",
    "            else:\n",
    "                three_pred += 1\n",
    "        correct += (pred == y).float().sum()\n",
    "        total += y.shape[0]\n",
    "        #print(\"correct: \", correct, \" total: \", total)\n",
    "        #print(classification_report(y, pred))\n",
    "        sum_loss += loss.item()*y.shape[0]\n",
    "        sum_rmse += np.sqrt(mean_squared_error(pred, y.unsqueeze(-1)))*y.shape[0]\n",
    "    #print(\"pred: \", zero_pred, one_pred, two_pred, three_pred)\n",
    "    #print(\"actual: \", zero, one, two, three)\n",
    "    return sum_loss/total, correct/total, sum_rmse/total, y_pred_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model_regr(model):\n",
    "    parameters = filter(lambda p: p.requires_grad, model.parameters())\n",
    "    optimizer = torch.optim.Adam(parameters, lr=LEARNING_RATE)\n",
    "    criterion = nn.CrossEntropyLoss(weight=class_weights.to(device))\n",
    "    for i in range(EPOCHS):\n",
    "        model.to(\"cuda:0\")\n",
    "        model.train()\n",
    "        sum_loss = 0.0\n",
    "        total = 0\n",
    "        for x, y, l in train_dl:\n",
    "            x = x.long().to(device)\n",
    "            y = y.float().to(device)\n",
    "            y_pred = model(x, l)\n",
    "            optimizer.zero_grad()\n",
    "            #loss = F.cross_entropy(y_pred, y)\n",
    "            loss = criterion(y_pred, y.long())\n",
    "            #loss = F.mse_loss(y_pred, y.unsqueeze(-1))\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            sum_loss += loss.item() *y.shape[0]\n",
    "            total += y.shape[0]\n",
    "        val_loss = validation_metrics_regr(model.to(\"cpu\"), val_dl)\n",
    "        if i%5 == 1:\n",
    "            print(\"train mse %.3f, val rmse %.3f\" % (sum_loss/total, val_loss))\n",
    "def validation_metrics_regr (model, valid_dl):\n",
    "    #criterion = nn.CrossEntropyLoss(weight=class_weights.to(\"cpu\"))\n",
    "    model.to(\"cpu\")\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    sum_loss = 0.0\n",
    "    for x, y, l in valid_dl:\n",
    "        x = x.long()\n",
    "        y = y.float()\n",
    "        y_hat = model(x.cpu(), l)\n",
    "        loss =np.sqrt(F.mse_loss(y_hat, y.unsqueeze(-1)).item())\n",
    "        #loss = criterion(y_hat, y)\n",
    "        total += y.shape[0]\n",
    "        sum_loss += loss.item()*y.shape[0]\n",
    "    return sum_loss/total"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 33,
=======
   "execution_count": 25,
>>>>>>> b4a2e592245f7bee6a5e05a9b63fd51bb9cd5b5f
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "102824\n"
     ]
    }
   ],
   "source": [
    "vocab_size = len(words)\n",
    "print(vocab_size)\n",
    "train_dl = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True)\n",
    "val_dl = DataLoader(valid_ds, batch_size=BATCH_SIZE)\n",
    "np.save('vocab2index.npy',vocab2index)\n",
    "np.save('wordlist.npy',words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(208, torch.Size([1, 450]))"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x,y,l = train_ds[0]\n",
    "len(train_dl), x.unsqueeze(0).shape"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 48,
=======
   "execution_count": 27,
>>>>>>> b4a2e592245f7bee6a5e05a9b63fd51bb9cd5b5f
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTM_fixed_len(torch.nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim):\n",
    "        super().__init__()\n",
    "        self.embeddings = nn.Embedding(vocab_size, embedding_dim, padding_idx=0).to(device)\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, batch_first=True).to(device)\n",
    "        self.linear = nn.Linear(hidden_dim, 4).to(device)\n",
    "        self.dropout = nn.Dropout(0.2)\n",
    "        \n",
    "    def forward(self, x, l):\n",
    "        x = self.embeddings(x)\n",
    "        x = self.dropout(x)\n",
    "        lstm_out, (ht, ct) = self.lstm(x)\n",
    "        return self.linear(ht[-1])"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 19,
=======
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LSTM_fixed_len(\n",
       "  (embeddings): Embedding(102824, 256, padding_idx=0)\n",
       "  (lstm): LSTM(256, 256, batch_first=True)\n",
       "  (linear): Linear(in_features=256, out_features=2, bias=True)\n",
       "  (dropout): Dropout(p=0.2, inplace=False)\n",
       ")"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_fixed = LSTM_fixed_len(vocab_size, 256, 256)\n",
    "model_fixed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: './model.pt'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-29-2986914c285a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mmodel_fixed\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload_state_dict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"./model.pt\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mtrain_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel_fixed\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\torch\\serialization.py\u001b[0m in \u001b[0;36mload\u001b[1;34m(f, map_location, pickle_module, **pickle_load_args)\u001b[0m\n\u001b[0;32m    523\u001b[0m         \u001b[0mpickle_load_args\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'encoding'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'utf-8'\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    524\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 525\u001b[1;33m     \u001b[1;32mwith\u001b[0m \u001b[0m_open_file_like\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'rb'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mopened_file\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    526\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0m_is_zipfile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mopened_file\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    527\u001b[0m             \u001b[1;32mwith\u001b[0m \u001b[0m_open_zipfile_reader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mopened_zipfile\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\torch\\serialization.py\u001b[0m in \u001b[0;36m_open_file_like\u001b[1;34m(name_or_buffer, mode)\u001b[0m\n\u001b[0;32m    210\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0m_open_file_like\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    211\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0m_is_path\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname_or_buffer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 212\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0m_open_file\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    213\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    214\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;34m'w'\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mmode\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\torch\\serialization.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, name, mode)\u001b[0m\n\u001b[0;32m    191\u001b[0m \u001b[1;32mclass\u001b[0m \u001b[0m_open_file\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_opener\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    192\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 193\u001b[1;33m         \u001b[0msuper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_open_file\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    194\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    195\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__exit__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: './model.pt'"
     ]
    }
   ],
   "source": [
    "model_fixed.load_state_dict(torch.load(\"./model.pt\"))\n",
    "train_model(model_fixed.to(device))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
>>>>>>> b4a2e592245f7bee6a5e05a9b63fd51bb9cd5b5f
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTM_variable_len(torch.nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim):\n",
    "        super().__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.dropout = nn.Dropout(0.3)\n",
    "        self.embeddings = nn.Embedding(vocab_size, embedding_dim, padding_idx=0)\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, batch_first=True)\n",
    "        self.linear = nn.Linear(hidden_dim, 4)\n",
    "        \n",
    "    def forward(self, x, s):\n",
    "        x = self.embeddings(x)\n",
    "        x = self.dropout(x)\n",
    "        x_pack = pack_padded_sequence(x, s, batch_first=True, enforce_sorted=False)\n",
    "        out_pack, (ht, ct) = self.lstm(x_pack)\n",
    "        out = self.linear(ht[-1])\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 81,
=======
   "execution_count": null,
>>>>>>> b4a2e592245f7bee6a5e05a9b63fd51bb9cd5b5f
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTM_regr(torch.nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim):\n",
    "        super().__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.dropout = nn.Dropout(0.2)\n",
    "        self.embeddings = nn.Embedding(vocab_size, embedding_dim, padding_idx=0).to(device)\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, batch_first=True).to(device)\n",
    "        self.linear = nn.Linear(hidden_dim,1).to(device)\n",
    "        \n",
    "    def forward(self, x, s):\n",
    "        x = self.embeddings(x)\n",
    "        x = self.dropout(x)\n",
    "        out_pack, (ht, ct) = self.lstm(x)\n",
    "        out = self.linear(ht[-1])\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 36,
=======
   "execution_count": null,
>>>>>>> b4a2e592245f7bee6a5e05a9b63fd51bb9cd5b5f
   "metadata": {},
   "outputs": [],
   "source": [
    "#model_fixed = LSTM_fixed_len(vocab_size, 50, 50).to(device)\n",
    "model_variable = LSTM_variable_len(vocab_size, 50, 50)\n",
    "#model_reg = LSTM_regr(vocab_size, 50, 50).to(device)"
   ]
  },
  {
<<<<<<< HEAD
   "cell_type": "markdown",
   "metadata": {},
=======
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
>>>>>>> b4a2e592245f7bee6a5e05a9b63fd51bb9cd5b5f
   "source": [
    "## Full title + content no punc"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss 1.355, val loss 1.345, val accuracy 0.357, and val rmse 1.234\n",
      "train loss 1.259, val loss 1.162, val accuracy 0.818, and val rmse 0.653\n",
      "train loss 1.035, val loss 0.699, val accuracy 0.934, and val rmse 0.377\n",
      "train loss 1.011, val loss 0.736, val accuracy 0.853, and val rmse 0.579\n",
      "train loss 0.836, val loss 0.603, val accuracy 0.872, and val rmse 0.534\n",
      "train loss 0.797, val loss 0.610, val accuracy 0.855, and val rmse 0.621\n",
      "train loss 0.671, val loss 0.504, val accuracy 0.888, and val rmse 0.539\n",
      "train loss 0.665, val loss 0.679, val accuracy 0.814, and val rmse 0.712\n",
      "train loss 0.642, val loss 0.475, val accuracy 0.877, and val rmse 0.564\n",
      "train loss 0.534, val loss 0.529, val accuracy 0.834, and val rmse 0.658\n",
      "train loss 0.477, val loss 0.370, val accuracy 0.902, and val rmse 0.545\n",
      "train loss 0.695, val loss 0.568, val accuracy 0.802, and val rmse 0.764\n",
      "train loss 0.426, val loss 0.291, val accuracy 0.922, and val rmse 0.486\n",
      "train loss 0.446, val loss 0.612, val accuracy 0.768, and val rmse 0.848\n",
      "train loss 0.461, val loss 0.390, val accuracy 0.874, and val rmse 0.631\n",
      "train loss 0.405, val loss 0.492, val accuracy 0.821, and val rmse 0.762\n",
      "train loss 0.382, val loss 0.493, val accuracy 0.818, and val rmse 0.769\n",
      "train loss 0.384, val loss 0.552, val accuracy 0.784, and val rmse 0.818\n",
      "train loss 0.328, val loss 0.362, val accuracy 0.882, and val rmse 0.608\n",
      "train loss 0.281, val loss 0.467, val accuracy 0.827, and val rmse 0.731\n",
      "train loss 0.277, val loss 0.530, val accuracy 0.796, and val rmse 0.812\n",
      "train loss 0.314, val loss 0.526, val accuracy 0.806, and val rmse 0.799\n",
      "train loss 0.282, val loss 0.570, val accuracy 0.788, and val rmse 0.839\n",
      "train loss 0.227, val loss 0.501, val accuracy 0.829, and val rmse 0.754\n",
      "train loss 0.282, val loss 0.431, val accuracy 0.857, and val rmse 0.682\n",
      "train loss 0.230, val loss 0.470, val accuracy 0.840, and val rmse 0.721\n",
      "train loss 0.208, val loss 0.511, val accuracy 0.830, and val rmse 0.725\n",
      "train loss 0.226, val loss 0.482, val accuracy 0.843, and val rmse 0.663\n",
      "train loss 0.199, val loss 0.464, val accuracy 0.852, and val rmse 0.689\n",
      "train loss 0.203, val loss 0.495, val accuracy 0.840, and val rmse 0.697\n"
     ]
    }
   ],
=======
   "execution_count": null,
   "metadata": {},
   "outputs": [],
>>>>>>> b4a2e592245f7bee6a5e05a9b63fd51bb9cd5b5f
   "source": [
    "train_model(model_variable)"
   ]
  },
  {
<<<<<<< HEAD
   "cell_type": "markdown",
=======
   "cell_type": "code",
   "execution_count": null,
>>>>>>> b4a2e592245f7bee6a5e05a9b63fd51bb9cd5b5f
   "metadata": {},
   "source": [
    "## title + first 500 words of content, no punc"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss 1.349, val loss 1.278, val accuracy 0.603, and val rmse 1.797\n",
      "train loss 1.132, val loss 1.251, val accuracy 0.661, and val rmse 1.166\n",
      "train loss 0.921, val loss 1.191, val accuracy 0.604, and val rmse 0.944\n",
      "train loss 0.735, val loss 0.996, val accuracy 0.610, and val rmse 0.708\n",
      "train loss 0.636, val loss 0.817, val accuracy 0.818, and val rmse 0.624\n",
      "train loss 0.503, val loss 0.645, val accuracy 0.876, and val rmse 0.565\n",
      "train loss 0.485, val loss 0.673, val accuracy 0.801, and val rmse 0.656\n",
      "train loss 0.502, val loss 0.634, val accuracy 0.821, and val rmse 0.755\n",
      "train loss 0.323, val loss 0.628, val accuracy 0.779, and val rmse 0.589\n",
      "train loss 0.406, val loss 0.716, val accuracy 0.763, and val rmse 0.788\n",
      "train loss 0.360, val loss 0.646, val accuracy 0.793, and val rmse 0.688\n",
      "train loss 0.427, val loss 0.706, val accuracy 0.752, and val rmse 0.712\n",
      "train loss 0.317, val loss 0.760, val accuracy 0.714, and val rmse 0.710\n",
      "train loss 0.273, val loss 0.749, val accuracy 0.716, and val rmse 0.674\n",
      "train loss 0.312, val loss 0.761, val accuracy 0.714, and val rmse 0.772\n",
      "train loss 0.300, val loss 0.938, val accuracy 0.645, and val rmse 0.838\n",
      "train loss 0.294, val loss 0.751, val accuracy 0.712, and val rmse 0.774\n",
      "train loss 0.226, val loss 0.618, val accuracy 0.772, and val rmse 0.755\n",
      "train loss 0.254, val loss 0.668, val accuracy 0.752, and val rmse 0.725\n",
      "train loss 0.242, val loss 0.779, val accuracy 0.722, and val rmse 0.792\n",
      "train loss 0.216, val loss 0.714, val accuracy 0.746, and val rmse 0.773\n",
      "train loss 0.214, val loss 0.638, val accuracy 0.768, and val rmse 0.647\n",
      "train loss 0.204, val loss 0.849, val accuracy 0.689, and val rmse 0.779\n",
      "train loss 0.181, val loss 0.793, val accuracy 0.706, and val rmse 0.790\n",
      "train loss 0.173, val loss 0.732, val accuracy 0.741, and val rmse 0.742\n",
      "train loss 0.178, val loss 0.925, val accuracy 0.665, and val rmse 0.827\n",
      "train loss 0.204, val loss 0.725, val accuracy 0.738, and val rmse 0.728\n",
      "train loss 0.142, val loss 0.746, val accuracy 0.744, and val rmse 0.731\n",
      "train loss 0.151, val loss 0.671, val accuracy 0.776, and val rmse 0.691\n",
      "train loss 0.150, val loss 0.886, val accuracy 0.693, and val rmse 0.823\n",
      "train loss 0.164, val loss 0.513, val accuracy 0.834, and val rmse 0.644\n",
      "train loss 0.163, val loss 0.778, val accuracy 0.737, and val rmse 0.738\n",
      "train loss 0.137, val loss 0.543, val accuracy 0.829, and val rmse 0.656\n",
      "train loss 0.111, val loss 0.725, val accuracy 0.769, and val rmse 0.741\n",
      "train loss 0.150, val loss 0.719, val accuracy 0.765, and val rmse 0.695\n",
      "train loss 0.161, val loss 0.894, val accuracy 0.703, and val rmse 0.768\n",
      "train loss 0.197, val loss 0.574, val accuracy 0.837, and val rmse 0.726\n",
      "train loss 0.138, val loss 0.603, val accuracy 0.828, and val rmse 0.608\n",
      "train loss 0.118, val loss 0.596, val accuracy 0.825, and val rmse 0.625\n",
      "train loss 0.138, val loss 0.759, val accuracy 0.762, and val rmse 0.695\n"
     ]
    }
   ],
   "source": [
    "train_model(model_variable)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
=======
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.train()\n",
    "for e in range(1, EPOCHS+1):\n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0\n",
    "    for X_batch, y_batch in train_loader:\n",
    "        X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        y_pred = model(X_batch)\n",
    "        \n",
    "        loss = criterion(y_pred, y_batch.unsqueeze(1))\n",
    "        acc = binary_acc(y_pred, y_batch.unsqueeze(1))\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        epoch_loss += loss.item()\n",
    "        epoch_acc += acc.item()\n",
    "        \n",
    "\n",
    "    print(f'Epoch {e+0:03}: | Loss: {epoch_loss/len(train_loader):.5f} | Acc: {epoch_acc/len(train_loader):.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_list = []\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    for X_batch in test_loader:\n",
    "        X_batch = X_batch.to(device)\n",
    "        y_test_pred = model(X_batch)\n",
    "        y_test_pred = torch.sigmoid(y_test_pred)\n",
    "        y_pred_tag = torch.round(y_test_pred)\n",
    "        y_pred_list.append(y_pred_tag.cpu().numpy())\n",
    "\n",
    "y_pred_list = [a.squeeze().tolist() for a in y_pred_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
>>>>>>> b4a2e592245f7bee6a5e05a9b63fd51bb9cd5b5f
   "source": [
    "## title + content no punc, true train/test split"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 28,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss 1.391, val loss 1.390, val accuracy 0.282, and val rmse 1.881\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.52      0.49      0.51      1255\n",
      "         1.0       0.33      0.00      0.00       523\n",
      "         2.0       0.25      0.02      0.03       555\n",
      "         3.0       0.05      0.57      0.09       109\n",
      "\n",
      "    accuracy                           0.28      2442\n",
      "   macro avg       0.29      0.27      0.16      2442\n",
      "weighted avg       0.40      0.28      0.27      2442\n",
      "\n",
      "train loss 1.374, val loss 1.381, val accuracy 0.301, and val rmse 1.717\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.55      0.45      0.49      1255\n",
      "         1.0       0.22      0.05      0.08       523\n",
      "         2.0       0.23      0.16      0.19       555\n",
      "         3.0       0.06      0.50      0.11       109\n",
      "\n",
      "    accuracy                           0.30      2442\n",
      "   macro avg       0.26      0.29      0.22      2442\n",
      "weighted avg       0.38      0.30      0.32      2442\n",
      "\n",
      "train loss 1.357, val loss 1.374, val accuracy 0.274, and val rmse 1.636\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.55      0.30      0.39      1255\n",
      "         1.0       0.21      0.12      0.16       523\n",
      "         2.0       0.24      0.32      0.27       555\n",
      "         3.0       0.07      0.44      0.12       109\n",
      "\n",
      "    accuracy                           0.27      2442\n",
      "   macro avg       0.27      0.30      0.23      2442\n",
      "weighted avg       0.38      0.27      0.30      2442\n",
      "\n",
      "train loss 1.346, val loss 1.368, val accuracy 0.259, and val rmse 1.567\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.59      0.22      0.32      1255\n",
      "         1.0       0.21      0.23      0.22       523\n",
      "         2.0       0.24      0.34      0.28       555\n",
      "         3.0       0.07      0.38      0.12       109\n",
      "\n",
      "    accuracy                           0.26      2442\n",
      "   macro avg       0.28      0.29      0.24      2442\n",
      "weighted avg       0.41      0.26      0.28      2442\n",
      "\n",
      "train loss 1.314, val loss 1.358, val accuracy 0.273, and val rmse 1.514\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.61      0.25      0.35      1255\n",
      "         1.0       0.21      0.25      0.23       523\n",
      "         2.0       0.23      0.32      0.27       555\n",
      "         3.0       0.08      0.40      0.13       109\n",
      "\n",
      "    accuracy                           0.27      2442\n",
      "   macro avg       0.28      0.31      0.25      2442\n",
      "weighted avg       0.41      0.27      0.30      2442\n",
      "\n",
      "train loss 1.280, val loss 1.346, val accuracy 0.325, and val rmse 1.424\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.60      0.35      0.44      1255\n",
      "         1.0       0.23      0.25      0.24       523\n",
      "         2.0       0.26      0.32      0.29       555\n",
      "         3.0       0.10      0.41      0.16       109\n",
      "\n",
      "    accuracy                           0.33      2442\n",
      "   macro avg       0.30      0.33      0.28      2442\n",
      "weighted avg       0.42      0.33      0.35      2442\n",
      "\n",
      "train loss 1.251, val loss 1.331, val accuracy 0.333, and val rmse 1.438\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.62      0.40      0.49      1255\n",
      "         1.0       0.22      0.22      0.22       523\n",
      "         2.0       0.25      0.24      0.25       555\n",
      "         3.0       0.10      0.54      0.17       109\n",
      "\n",
      "    accuracy                           0.33      2442\n",
      "   macro avg       0.30      0.35      0.28      2442\n",
      "weighted avg       0.43      0.33      0.36      2442\n",
      "\n",
      "train loss 1.225, val loss 1.318, val accuracy 0.391, and val rmse 1.324\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.61      0.52      0.56      1255\n",
      "         1.0       0.22      0.17      0.19       523\n",
      "         2.0       0.28      0.29      0.29       555\n",
      "         3.0       0.12      0.44      0.19       109\n",
      "\n",
      "    accuracy                           0.39      2442\n",
      "   macro avg       0.31      0.36      0.31      2442\n",
      "weighted avg       0.43      0.39      0.40      2442\n",
      "\n",
      "train loss 1.178, val loss 1.316, val accuracy 0.409, and val rmse 1.264\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.62      0.55      0.59      1255\n",
      "         1.0       0.21      0.18      0.20       523\n",
      "         2.0       0.29      0.30      0.30       555\n",
      "         3.0       0.14      0.40      0.21       109\n",
      "\n",
      "    accuracy                           0.41      2442\n",
      "   macro avg       0.32      0.36      0.32      2442\n",
      "weighted avg       0.44      0.41      0.42      2442\n",
      "\n",
      "train loss 1.145, val loss 1.324, val accuracy 0.414, and val rmse 1.229\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.64      0.55      0.59      1255\n",
      "         1.0       0.22      0.22      0.22       523\n",
      "         2.0       0.30      0.29      0.30       555\n",
      "         3.0       0.15      0.43      0.23       109\n",
      "\n",
      "    accuracy                           0.41      2442\n",
      "   macro avg       0.33      0.37      0.33      2442\n",
      "weighted avg       0.45      0.41      0.43      2442\n",
      "\n",
      "train loss 1.108, val loss 1.319, val accuracy 0.403, and val rmse 1.254\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.66      0.50      0.57      1255\n",
      "         1.0       0.23      0.24      0.23       523\n",
      "         2.0       0.31      0.32      0.32       555\n",
      "         3.0       0.14      0.50      0.22       109\n",
      "\n",
      "    accuracy                           0.40      2442\n",
      "   macro avg       0.34      0.39      0.34      2442\n",
      "weighted avg       0.47      0.40      0.42      2442\n",
      "\n",
      "train loss 1.072, val loss 1.328, val accuracy 0.424, and val rmse 1.199\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.66      0.55      0.60      1255\n",
      "         1.0       0.24      0.26      0.25       523\n",
      "         2.0       0.32      0.29      0.30       555\n",
      "         3.0       0.16      0.49      0.24       109\n",
      "\n",
      "    accuracy                           0.42      2442\n",
      "   macro avg       0.35      0.40      0.35      2442\n",
      "weighted avg       0.47      0.42      0.44      2442\n",
      "\n",
      "train loss 1.049, val loss 1.342, val accuracy 0.446, and val rmse 1.191\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.66      0.58      0.62      1255\n",
      "         1.0       0.24      0.23      0.24       523\n",
      "         2.0       0.34      0.33      0.33       555\n",
      "         3.0       0.17      0.48      0.25       109\n",
      "\n",
      "    accuracy                           0.45      2442\n",
      "   macro avg       0.35      0.40      0.36      2442\n",
      "weighted avg       0.48      0.45      0.46      2442\n",
      "\n",
      "train loss 1.011, val loss 1.379, val accuracy 0.430, and val rmse 1.186\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.68      0.53      0.60      1255\n",
      "         1.0       0.24      0.27      0.25       523\n",
      "         2.0       0.33      0.34      0.34       555\n",
      "         3.0       0.18      0.49      0.26       109\n",
      "\n",
      "    accuracy                           0.43      2442\n",
      "   macro avg       0.36      0.41      0.36      2442\n",
      "weighted avg       0.48      0.43      0.45      2442\n",
      "\n",
      "train loss 0.950, val loss 1.392, val accuracy 0.433, and val rmse 1.189\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.69      0.53      0.60      1255\n",
      "         1.0       0.25      0.27      0.26       523\n",
      "         2.0       0.33      0.37      0.34       555\n",
      "         3.0       0.17      0.45      0.24       109\n",
      "\n",
      "    accuracy                           0.43      2442\n",
      "   macro avg       0.36      0.40      0.36      2442\n",
      "weighted avg       0.49      0.43      0.45      2442\n",
      "\n",
      "train loss 0.945, val loss 1.429, val accuracy 0.435, and val rmse 1.160\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.69      0.54      0.61      1255\n",
      "         1.0       0.25      0.32      0.28       523\n",
      "         2.0       0.33      0.30      0.31       555\n",
      "         3.0       0.17      0.48      0.25       109\n",
      "\n",
      "    accuracy                           0.43      2442\n",
      "   macro avg       0.36      0.41      0.36      2442\n",
      "weighted avg       0.49      0.43      0.45      2442\n",
      "\n",
      "train loss 0.896, val loss 1.423, val accuracy 0.446, and val rmse 1.171\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.70      0.55      0.61      1255\n",
      "         1.0       0.27      0.31      0.29       523\n",
      "         2.0       0.35      0.34      0.34       555\n",
      "         3.0       0.17      0.48      0.25       109\n",
      "\n",
      "    accuracy                           0.45      2442\n",
      "   macro avg       0.37      0.42      0.37      2442\n",
      "weighted avg       0.50      0.45      0.47      2442\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss 0.891, val loss 1.446, val accuracy 0.441, and val rmse 1.156\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.71      0.53      0.61      1255\n",
      "         1.0       0.26      0.32      0.29       523\n",
      "         2.0       0.34      0.37      0.35       555\n",
      "         3.0       0.17      0.39      0.23       109\n",
      "\n",
      "    accuracy                           0.44      2442\n",
      "   macro avg       0.37      0.40      0.37      2442\n",
      "weighted avg       0.50      0.44      0.46      2442\n",
      "\n",
      "train loss 0.887, val loss 1.486, val accuracy 0.450, and val rmse 1.105\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.72      0.53      0.61      1255\n",
      "         1.0       0.26      0.38      0.31       523\n",
      "         2.0       0.35      0.35      0.35       555\n",
      "         3.0       0.19      0.38      0.25       109\n",
      "\n",
      "    accuracy                           0.45      2442\n",
      "   macro avg       0.38      0.41      0.38      2442\n",
      "weighted avg       0.52      0.45      0.47      2442\n",
      "\n",
      "train loss 0.860, val loss 1.522, val accuracy 0.455, and val rmse 1.099\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.71      0.55      0.62      1255\n",
      "         1.0       0.26      0.36      0.30       523\n",
      "         2.0       0.35      0.35      0.35       555\n",
      "         3.0       0.19      0.37      0.25       109\n",
      "\n",
      "    accuracy                           0.45      2442\n",
      "   macro avg       0.38      0.41      0.38      2442\n",
      "weighted avg       0.51      0.45      0.47      2442\n",
      "\n",
      "train loss 0.796, val loss 1.522, val accuracy 0.466, and val rmse 1.093\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.72      0.56      0.63      1255\n",
      "         1.0       0.27      0.35      0.31       523\n",
      "         2.0       0.37      0.36      0.37       555\n",
      "         3.0       0.19      0.40      0.26       109\n",
      "\n",
      "    accuracy                           0.47      2442\n",
      "   macro avg       0.39      0.42      0.39      2442\n",
      "weighted avg       0.52      0.47      0.48      2442\n",
      "\n",
      "train loss 0.783, val loss 1.563, val accuracy 0.481, and val rmse 1.074\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.72      0.58      0.64      1255\n",
      "         1.0       0.28      0.38      0.33       523\n",
      "         2.0       0.39      0.36      0.38       555\n",
      "         3.0       0.20      0.38      0.26       109\n",
      "\n",
      "    accuracy                           0.48      2442\n",
      "   macro avg       0.40      0.43      0.40      2442\n",
      "weighted avg       0.53      0.48      0.50      2442\n",
      "\n",
      "train loss 0.780, val loss 1.575, val accuracy 0.471, and val rmse 1.077\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.72      0.56      0.63      1255\n",
      "         1.0       0.28      0.39      0.32       523\n",
      "         2.0       0.38      0.35      0.37       555\n",
      "         3.0       0.19      0.39      0.26       109\n",
      "\n",
      "    accuracy                           0.47      2442\n",
      "   macro avg       0.39      0.42      0.40      2442\n",
      "weighted avg       0.53      0.47      0.49      2442\n",
      "\n",
      "train loss 0.755, val loss 1.618, val accuracy 0.485, and val rmse 1.065\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.71      0.61      0.66      1255\n",
      "         1.0       0.27      0.33      0.30       523\n",
      "         2.0       0.39      0.37      0.38       555\n",
      "         3.0       0.19      0.36      0.25       109\n",
      "\n",
      "    accuracy                           0.48      2442\n",
      "   macro avg       0.39      0.42      0.40      2442\n",
      "weighted avg       0.52      0.48      0.50      2442\n",
      "\n",
      "train loss 0.736, val loss 1.598, val accuracy 0.479, and val rmse 1.074\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.72      0.59      0.65      1255\n",
      "         1.0       0.28      0.37      0.32       523\n",
      "         2.0       0.39      0.35      0.37       555\n",
      "         3.0       0.18      0.38      0.25       109\n",
      "\n",
      "    accuracy                           0.48      2442\n",
      "   macro avg       0.39      0.42      0.39      2442\n",
      "weighted avg       0.52      0.48      0.50      2442\n",
      "\n",
      "train loss 0.724, val loss 1.618, val accuracy 0.476, and val rmse 1.068\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.73      0.57      0.64      1255\n",
      "         1.0       0.28      0.38      0.32       523\n",
      "         2.0       0.38      0.37      0.38       555\n",
      "         3.0       0.19      0.39      0.25       109\n",
      "\n",
      "    accuracy                           0.48      2442\n",
      "   macro avg       0.40      0.43      0.40      2442\n",
      "weighted avg       0.53      0.48      0.50      2442\n",
      "\n",
      "train loss 0.709, val loss 1.682, val accuracy 0.477, and val rmse 1.057\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.73      0.57      0.64      1255\n",
      "         1.0       0.28      0.38      0.32       523\n",
      "         2.0       0.39      0.38      0.38       555\n",
      "         3.0       0.20      0.37      0.26       109\n",
      "\n",
      "    accuracy                           0.48      2442\n",
      "   macro avg       0.40      0.42      0.40      2442\n",
      "weighted avg       0.53      0.48      0.50      2442\n",
      "\n",
      "train loss 0.697, val loss 1.717, val accuracy 0.479, and val rmse 1.030\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.73      0.58      0.65      1255\n",
      "         1.0       0.27      0.40      0.32       523\n",
      "         2.0       0.39      0.36      0.38       555\n",
      "         3.0       0.20      0.34      0.25       109\n",
      "\n",
      "    accuracy                           0.48      2442\n",
      "   macro avg       0.40      0.42      0.40      2442\n",
      "weighted avg       0.53      0.48      0.50      2442\n",
      "\n",
      "train loss 0.692, val loss 1.704, val accuracy 0.474, and val rmse 1.042\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.75      0.56      0.64      1255\n",
      "         1.0       0.28      0.41      0.33       523\n",
      "         2.0       0.39      0.37      0.38       555\n",
      "         3.0       0.19      0.35      0.25       109\n",
      "\n",
      "    accuracy                           0.47      2442\n",
      "   macro avg       0.40      0.42      0.40      2442\n",
      "weighted avg       0.54      0.47      0.50      2442\n",
      "\n",
      "train loss 0.666, val loss 1.736, val accuracy 0.469, and val rmse 1.050\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.75      0.54      0.63      1255\n",
      "         1.0       0.28      0.41      0.33       523\n",
      "         2.0       0.38      0.37      0.38       555\n",
      "         3.0       0.20      0.37      0.26       109\n",
      "\n",
      "    accuracy                           0.47      2442\n",
      "   macro avg       0.40      0.42      0.40      2442\n",
      "weighted avg       0.54      0.47      0.49      2442\n",
      "\n",
      "train loss 0.649, val loss 1.771, val accuracy 0.478, and val rmse 1.033\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.74      0.57      0.64      1255\n",
      "         1.0       0.27      0.39      0.32       523\n",
      "         2.0       0.38      0.38      0.38       555\n",
      "         3.0       0.20      0.33      0.25       109\n",
      "\n",
      "    accuracy                           0.48      2442\n",
      "   macro avg       0.40      0.42      0.40      2442\n",
      "weighted avg       0.54      0.48      0.50      2442\n",
      "\n",
      "train loss 0.667, val loss 1.747, val accuracy 0.479, and val rmse 1.040\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.74      0.57      0.65      1255\n",
      "         1.0       0.28      0.39      0.33       523\n",
      "         2.0       0.39      0.38      0.38       555\n",
      "         3.0       0.18      0.32      0.23       109\n",
      "\n",
      "    accuracy                           0.48      2442\n",
      "   macro avg       0.40      0.42      0.40      2442\n",
      "weighted avg       0.54      0.48      0.50      2442\n",
      "\n",
      "train loss 0.644, val loss 1.813, val accuracy 0.474, and val rmse 1.039\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.75      0.56      0.64      1255\n",
      "         1.0       0.27      0.41      0.33       523\n",
      "         2.0       0.39      0.37      0.38       555\n",
      "         3.0       0.18      0.33      0.23       109\n",
      "\n",
      "    accuracy                           0.47      2442\n",
      "   macro avg       0.40      0.42      0.40      2442\n",
      "weighted avg       0.54      0.47      0.50      2442\n",
      "\n",
      "train loss 0.627, val loss 1.764, val accuracy 0.474, and val rmse 1.041\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.74      0.56      0.64      1255\n",
      "         1.0       0.28      0.41      0.33       523\n",
      "         2.0       0.38      0.37      0.38       555\n",
      "         3.0       0.20      0.37      0.26       109\n",
      "\n",
      "    accuracy                           0.47      2442\n",
      "   macro avg       0.40      0.42      0.40      2442\n",
      "weighted avg       0.54      0.47      0.50      2442\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss 0.605, val loss 1.801, val accuracy 0.484, and val rmse 1.030\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.75      0.57      0.65      1255\n",
      "         1.0       0.28      0.40      0.33       523\n",
      "         2.0       0.39      0.40      0.40       555\n",
      "         3.0       0.20      0.35      0.26       109\n",
      "\n",
      "    accuracy                           0.48      2442\n",
      "   macro avg       0.41      0.43      0.41      2442\n",
      "weighted avg       0.55      0.48      0.51      2442\n",
      "\n",
      "train loss 0.620, val loss 1.828, val accuracy 0.483, and val rmse 1.031\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.75      0.57      0.65      1255\n",
      "         1.0       0.28      0.39      0.33       523\n",
      "         2.0       0.39      0.40      0.39       555\n",
      "         3.0       0.20      0.34      0.25       109\n",
      "\n",
      "    accuracy                           0.48      2442\n",
      "   macro avg       0.40      0.43      0.41      2442\n",
      "weighted avg       0.54      0.48      0.50      2442\n",
      "\n",
      "train loss 0.609, val loss 1.840, val accuracy 0.477, and val rmse 1.019\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.76      0.55      0.64      1255\n",
      "         1.0       0.28      0.43      0.34       523\n",
      "         2.0       0.39      0.39      0.39       555\n",
      "         3.0       0.21      0.34      0.26       109\n",
      "\n",
      "    accuracy                           0.48      2442\n",
      "   macro avg       0.41      0.43      0.41      2442\n",
      "weighted avg       0.55      0.48      0.50      2442\n",
      "\n",
      "train loss 0.577, val loss 1.895, val accuracy 0.486, and val rmse 1.017\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.75      0.58      0.65      1255\n",
      "         1.0       0.28      0.40      0.33       523\n",
      "         2.0       0.39      0.38      0.39       555\n",
      "         3.0       0.21      0.33      0.26       109\n",
      "\n",
      "    accuracy                           0.49      2442\n",
      "   macro avg       0.41      0.42      0.41      2442\n",
      "weighted avg       0.54      0.49      0.51      2442\n",
      "\n",
      "train loss 0.593, val loss 1.901, val accuracy 0.501, and val rmse 1.019\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.73      0.62      0.67      1255\n",
      "         1.0       0.29      0.37      0.32       523\n",
      "         2.0       0.40      0.39      0.39       555\n",
      "         3.0       0.21      0.33      0.25       109\n",
      "\n",
      "    accuracy                           0.50      2442\n",
      "   macro avg       0.41      0.43      0.41      2442\n",
      "weighted avg       0.54      0.50      0.52      2442\n",
      "\n",
      "train loss 0.592, val loss 1.904, val accuracy 0.490, and val rmse 1.019\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.75      0.59      0.66      1255\n",
      "         1.0       0.29      0.41      0.34       523\n",
      "         2.0       0.39      0.37      0.38       555\n",
      "         3.0       0.20      0.34      0.25       109\n",
      "\n",
      "    accuracy                           0.49      2442\n",
      "   macro avg       0.41      0.43      0.41      2442\n",
      "weighted avg       0.55      0.49      0.51      2442\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train_model(model_variable.to(device))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Full content + title (no punc) [4 quantiles] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss 1.388, val loss 1.388, val accuracy 0.248, and val rmse 1.443\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.25      0.08      0.12       660\n",
      "         1.0       0.27      0.14      0.19       616\n",
      "         2.0       0.23      0.55      0.33       620\n",
      "         3.0       0.27      0.23      0.25       593\n",
      "\n",
      "    accuracy                           0.25      2489\n",
      "   macro avg       0.26      0.25      0.22      2489\n",
      "weighted avg       0.26      0.25      0.22      2489\n",
      "\n",
      "train loss 1.377, val loss 1.379, val accuracy 0.278, and val rmse 1.550\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.32      0.34      0.33       660\n",
      "         1.0       0.26      0.19      0.22       616\n",
      "         2.0       0.25      0.27      0.26       620\n",
      "         3.0       0.28      0.31      0.30       593\n",
      "\n",
      "    accuracy                           0.28      2489\n",
      "   macro avg       0.28      0.28      0.27      2489\n",
      "weighted avg       0.28      0.28      0.28      2489\n",
      "\n",
      "train loss 1.365, val loss 1.371, val accuracy 0.286, and val rmse 1.605\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.31      0.47      0.37       660\n",
      "         1.0       0.26      0.19      0.22       616\n",
      "         2.0       0.24      0.14      0.18       620\n",
      "         3.0       0.29      0.33      0.31       593\n",
      "\n",
      "    accuracy                           0.29      2489\n",
      "   macro avg       0.28      0.28      0.27      2489\n",
      "weighted avg       0.28      0.29      0.27      2489\n",
      "\n",
      "train loss 1.356, val loss 1.364, val accuracy 0.294, and val rmse 1.599\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.31      0.52      0.39       660\n",
      "         1.0       0.27      0.20      0.23       616\n",
      "         2.0       0.25      0.12      0.16       620\n",
      "         3.0       0.31      0.32      0.31       593\n",
      "\n",
      "    accuracy                           0.29      2489\n",
      "   macro avg       0.28      0.29      0.27      2489\n",
      "weighted avg       0.28      0.29      0.27      2489\n",
      "\n",
      "train loss 1.345, val loss 1.358, val accuracy 0.297, and val rmse 1.576\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.31      0.52      0.39       660\n",
      "         1.0       0.26      0.22      0.24       616\n",
      "         2.0       0.25      0.12      0.16       620\n",
      "         3.0       0.32      0.31      0.32       593\n",
      "\n",
      "    accuracy                           0.30      2489\n",
      "   macro avg       0.29      0.29      0.28      2489\n",
      "weighted avg       0.29      0.30      0.28      2489\n",
      "\n",
      "train loss 1.332, val loss 1.353, val accuracy 0.307, and val rmse 1.552\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.33      0.53      0.40       660\n",
      "         1.0       0.28      0.23      0.25       616\n",
      "         2.0       0.25      0.11      0.15       620\n",
      "         3.0       0.33      0.34      0.33       593\n",
      "\n",
      "    accuracy                           0.31      2489\n",
      "   macro avg       0.29      0.30      0.29      2489\n",
      "weighted avg       0.29      0.31      0.29      2489\n",
      "\n",
      "train loss 1.317, val loss 1.348, val accuracy 0.321, and val rmse 1.537\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.33      0.53      0.41       660\n",
      "         1.0       0.29      0.23      0.26       616\n",
      "         2.0       0.26      0.12      0.17       620\n",
      "         3.0       0.35      0.38      0.37       593\n",
      "\n",
      "    accuracy                           0.32      2489\n",
      "   macro avg       0.31      0.32      0.30      2489\n",
      "weighted avg       0.31      0.32      0.30      2489\n",
      "\n",
      "train loss 1.294, val loss 1.344, val accuracy 0.338, and val rmse 1.522\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.35      0.53      0.42       660\n",
      "         1.0       0.31      0.24      0.27       616\n",
      "         2.0       0.30      0.13      0.18       620\n",
      "         3.0       0.35      0.44      0.39       593\n",
      "\n",
      "    accuracy                           0.34      2489\n",
      "   macro avg       0.33      0.34      0.32      2489\n",
      "weighted avg       0.33      0.34      0.32      2489\n",
      "\n",
      "train loss 1.267, val loss 1.335, val accuracy 0.364, and val rmse 1.474\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.39      0.59      0.47       660\n",
      "         1.0       0.33      0.20      0.25       616\n",
      "         2.0       0.29      0.13      0.17       620\n",
      "         3.0       0.37      0.52      0.43       593\n",
      "\n",
      "    accuracy                           0.36      2489\n",
      "   macro avg       0.34      0.36      0.33      2489\n",
      "weighted avg       0.35      0.36      0.33      2489\n",
      "\n",
      "train loss 1.235, val loss 1.334, val accuracy 0.374, and val rmse 1.449\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.41      0.66      0.50       660\n",
      "         1.0       0.32      0.14      0.19       616\n",
      "         2.0       0.29      0.09      0.14       620\n",
      "         3.0       0.37      0.60      0.46       593\n",
      "\n",
      "    accuracy                           0.37      2489\n",
      "   macro avg       0.35      0.37      0.32      2489\n",
      "weighted avg       0.35      0.37      0.32      2489\n",
      "\n",
      "train loss 1.198, val loss 1.364, val accuracy 0.376, and val rmse 1.471\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.40      0.68      0.51       660\n",
      "         1.0       0.33      0.07      0.12       616\n",
      "         2.0       0.29      0.06      0.11       620\n",
      "         3.0       0.37      0.68      0.48       593\n",
      "\n",
      "    accuracy                           0.38      2489\n",
      "   macro avg       0.35      0.37      0.30      2489\n",
      "weighted avg       0.35      0.38      0.30      2489\n",
      "\n",
      "train loss 1.171, val loss 1.413, val accuracy 0.384, and val rmse 1.452\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.40      0.74      0.52       660\n",
      "         1.0       0.34      0.06      0.11       616\n",
      "         2.0       0.29      0.07      0.12       620\n",
      "         3.0       0.39      0.65      0.48       593\n",
      "\n",
      "    accuracy                           0.38      2489\n",
      "   macro avg       0.35      0.38      0.31      2489\n",
      "weighted avg       0.35      0.38      0.31      2489\n",
      "\n",
      "train loss 1.127, val loss 1.537, val accuracy 0.375, and val rmse 1.507\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.38      0.78      0.51       660\n",
      "         1.0       0.33      0.03      0.06       616\n",
      "         2.0       0.37      0.03      0.06       620\n",
      "         3.0       0.37      0.64      0.47       593\n",
      "\n",
      "    accuracy                           0.37      2489\n",
      "   macro avg       0.36      0.37      0.27      2489\n",
      "weighted avg       0.36      0.37      0.28      2489\n",
      "\n",
      "train loss 1.102, val loss 1.638, val accuracy 0.376, and val rmse 1.512\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.38      0.78      0.51       660\n",
      "         1.0       0.41      0.03      0.06       616\n",
      "         2.0       0.39      0.05      0.08       620\n",
      "         3.0       0.37      0.63      0.46       593\n",
      "\n",
      "    accuracy                           0.38      2489\n",
      "   macro avg       0.39      0.37      0.28      2489\n",
      "weighted avg       0.39      0.38      0.28      2489\n",
      "\n",
      "train loss 1.071, val loss 1.711, val accuracy 0.378, and val rmse 1.509\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.39      0.77      0.52       660\n",
      "         1.0       0.50      0.03      0.05       616\n",
      "         2.0       0.37      0.02      0.05       620\n",
      "         3.0       0.36      0.67      0.47       593\n",
      "\n",
      "    accuracy                           0.38      2489\n",
      "   macro avg       0.40      0.37      0.27      2489\n",
      "weighted avg       0.40      0.38      0.27      2489\n",
      "\n",
      "train loss 1.046, val loss 1.738, val accuracy 0.381, and val rmse 1.490\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.41      0.76      0.53       660\n",
      "         1.0       0.45      0.03      0.06       616\n",
      "         2.0       0.32      0.04      0.06       620\n",
      "         3.0       0.36      0.69      0.47       593\n",
      "\n",
      "    accuracy                           0.38      2489\n",
      "   macro avg       0.38      0.38      0.28      2489\n",
      "weighted avg       0.38      0.38      0.28      2489\n",
      "\n",
      "train loss 1.017, val loss 1.860, val accuracy 0.382, and val rmse 1.498\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.38      0.82      0.52       660\n",
      "         1.0       0.53      0.03      0.05       616\n",
      "         2.0       0.41      0.05      0.08       620\n",
      "         3.0       0.38      0.62      0.47       593\n",
      "\n",
      "    accuracy                           0.38      2489\n",
      "   macro avg       0.43      0.38      0.28      2489\n",
      "weighted avg       0.43      0.38      0.28      2489\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss 0.993, val loss 1.813, val accuracy 0.390, and val rmse 1.468\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.40      0.79      0.53       660\n",
      "         1.0       0.44      0.04      0.08       616\n",
      "         2.0       0.40      0.06      0.11       620\n",
      "         3.0       0.37      0.65      0.47       593\n",
      "\n",
      "    accuracy                           0.39      2489\n",
      "   macro avg       0.40      0.39      0.30      2489\n",
      "weighted avg       0.40      0.39      0.30      2489\n",
      "\n",
      "train loss 0.981, val loss 1.785, val accuracy 0.387, and val rmse 1.461\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.45      0.69      0.55       660\n",
      "         1.0       0.39      0.02      0.04       616\n",
      "         2.0       0.32      0.06      0.10       620\n",
      "         3.0       0.34      0.77      0.47       593\n",
      "\n",
      "    accuracy                           0.39      2489\n",
      "   macro avg       0.38      0.39      0.29      2489\n",
      "weighted avg       0.38      0.39      0.29      2489\n",
      "\n",
      "train loss 0.970, val loss 1.980, val accuracy 0.382, and val rmse 1.489\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.40      0.76      0.53       660\n",
      "         1.0       0.47      0.02      0.04       616\n",
      "         2.0       0.39      0.02      0.04       620\n",
      "         3.0       0.36      0.71      0.47       593\n",
      "\n",
      "    accuracy                           0.38      2489\n",
      "   macro avg       0.40      0.38      0.27      2489\n",
      "weighted avg       0.40      0.38      0.27      2489\n",
      "\n",
      "train loss 0.949, val loss 1.875, val accuracy 0.398, and val rmse 1.421\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.38      0.85      0.53       660\n",
      "         1.0       0.44      0.06      0.10       616\n",
      "         2.0       0.35      0.12      0.18       620\n",
      "         3.0       0.45      0.53      0.49       593\n",
      "\n",
      "    accuracy                           0.40      2489\n",
      "   macro avg       0.40      0.39      0.32      2489\n",
      "weighted avg       0.40      0.40      0.33      2489\n",
      "\n",
      "train loss 0.921, val loss 1.832, val accuracy 0.399, and val rmse 1.423\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.41      0.80      0.54       660\n",
      "         1.0       0.36      0.07      0.12       616\n",
      "         2.0       0.38      0.08      0.14       620\n",
      "         3.0       0.40      0.62      0.48       593\n",
      "\n",
      "    accuracy                           0.40      2489\n",
      "   macro avg       0.39      0.39      0.32      2489\n",
      "weighted avg       0.39      0.40      0.32      2489\n",
      "\n",
      "train loss 0.894, val loss 2.007, val accuracy 0.393, and val rmse 1.449\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.40      0.80      0.54       660\n",
      "         1.0       0.45      0.04      0.07       616\n",
      "         2.0       0.35      0.08      0.12       620\n",
      "         3.0       0.38      0.64      0.48       593\n",
      "\n",
      "    accuracy                           0.39      2489\n",
      "   macro avg       0.40      0.39      0.30      2489\n",
      "weighted avg       0.40      0.39      0.30      2489\n",
      "\n",
      "train loss 0.878, val loss 2.181, val accuracy 0.383, and val rmse 1.487\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.38      0.80      0.52       660\n",
      "         1.0       0.50      0.03      0.06       616\n",
      "         2.0       0.32      0.03      0.06       620\n",
      "         3.0       0.38      0.64      0.48       593\n",
      "\n",
      "    accuracy                           0.38      2489\n",
      "   macro avg       0.40      0.38      0.28      2489\n",
      "weighted avg       0.40      0.38      0.28      2489\n",
      "\n",
      "train loss 0.866, val loss 2.209, val accuracy 0.388, and val rmse 1.461\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.38      0.84      0.52       660\n",
      "         1.0       0.45      0.04      0.07       616\n",
      "         2.0       0.33      0.07      0.11       620\n",
      "         3.0       0.41      0.59      0.48       593\n",
      "\n",
      "    accuracy                           0.39      2489\n",
      "   macro avg       0.39      0.38      0.30      2489\n",
      "weighted avg       0.39      0.39      0.30      2489\n",
      "\n",
      "train loss 0.840, val loss 2.268, val accuracy 0.389, and val rmse 1.472\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.39      0.80      0.53       660\n",
      "         1.0       0.53      0.03      0.05       616\n",
      "         2.0       0.38      0.06      0.10       620\n",
      "         3.0       0.38      0.65      0.48       593\n",
      "\n",
      "    accuracy                           0.39      2489\n",
      "   macro avg       0.42      0.38      0.29      2489\n",
      "weighted avg       0.42      0.39      0.29      2489\n",
      "\n",
      "train loss 0.831, val loss 2.312, val accuracy 0.389, and val rmse 1.471\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.40      0.80      0.53       660\n",
      "         1.0       0.48      0.03      0.06       616\n",
      "         2.0       0.33      0.04      0.08       620\n",
      "         3.0       0.38      0.66      0.48       593\n",
      "\n",
      "    accuracy                           0.39      2489\n",
      "   macro avg       0.39      0.38      0.29      2489\n",
      "weighted avg       0.39      0.39      0.29      2489\n",
      "\n",
      "train loss 0.814, val loss 2.387, val accuracy 0.383, and val rmse 1.482\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.40      0.79      0.53       660\n",
      "         1.0       0.40      0.02      0.04       616\n",
      "         2.0       0.34      0.05      0.09       620\n",
      "         3.0       0.37      0.66      0.47       593\n",
      "\n",
      "    accuracy                           0.38      2489\n",
      "   macro avg       0.38      0.38      0.28      2489\n",
      "weighted avg       0.38      0.38      0.28      2489\n",
      "\n",
      "train loss 0.794, val loss 2.434, val accuracy 0.386, and val rmse 1.476\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.38      0.82      0.52       660\n",
      "         1.0       0.42      0.03      0.05       616\n",
      "         2.0       0.33      0.05      0.08       620\n",
      "         3.0       0.39      0.63      0.48       593\n",
      "\n",
      "    accuracy                           0.39      2489\n",
      "   macro avg       0.38      0.38      0.29      2489\n",
      "weighted avg       0.38      0.39      0.29      2489\n",
      "\n",
      "train loss 0.793, val loss 2.451, val accuracy 0.387, and val rmse 1.468\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.39      0.81      0.53       660\n",
      "         1.0       0.44      0.03      0.05       616\n",
      "         2.0       0.33      0.05      0.08       620\n",
      "         3.0       0.38      0.64      0.48       593\n",
      "\n",
      "    accuracy                           0.39      2489\n",
      "   macro avg       0.38      0.38      0.29      2489\n",
      "weighted avg       0.38      0.39      0.29      2489\n",
      "\n",
      "train loss 0.771, val loss 2.554, val accuracy 0.384, and val rmse 1.476\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.39      0.82      0.52       660\n",
      "         1.0       0.47      0.03      0.05       616\n",
      "         2.0       0.33      0.04      0.07       620\n",
      "         3.0       0.38      0.64      0.48       593\n",
      "\n",
      "    accuracy                           0.38      2489\n",
      "   macro avg       0.39      0.38      0.28      2489\n",
      "weighted avg       0.39      0.38      0.28      2489\n",
      "\n",
      "train loss 0.759, val loss 2.580, val accuracy 0.386, and val rmse 1.475\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.39      0.80      0.53       660\n",
      "         1.0       0.51      0.03      0.06       616\n",
      "         2.0       0.34      0.05      0.09       620\n",
      "         3.0       0.38      0.65      0.48       593\n",
      "\n",
      "    accuracy                           0.39      2489\n",
      "   macro avg       0.41      0.38      0.29      2489\n",
      "weighted avg       0.41      0.39      0.29      2489\n",
      "\n",
      "train loss 0.743, val loss 2.603, val accuracy 0.389, and val rmse 1.461\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.40      0.80      0.53       660\n",
      "         1.0       0.46      0.03      0.05       616\n",
      "         2.0       0.34      0.06      0.10       620\n",
      "         3.0       0.38      0.65      0.48       593\n",
      "\n",
      "    accuracy                           0.39      2489\n",
      "   macro avg       0.39      0.38      0.29      2489\n",
      "weighted avg       0.39      0.39      0.29      2489\n",
      "\n",
      "train loss 0.732, val loss 2.710, val accuracy 0.384, and val rmse 1.478\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.39      0.78      0.53       660\n",
      "         1.0       0.42      0.02      0.05       616\n",
      "         2.0       0.33      0.04      0.06       620\n",
      "         3.0       0.37      0.68      0.48       593\n",
      "\n",
      "    accuracy                           0.38      2489\n",
      "   macro avg       0.38      0.38      0.28      2489\n",
      "weighted avg       0.38      0.38      0.28      2489\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss 0.719, val loss 2.778, val accuracy 0.384, and val rmse 1.484\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.38      0.82      0.52       660\n",
      "         1.0       0.41      0.03      0.05       616\n",
      "         2.0       0.33      0.03      0.06       620\n",
      "         3.0       0.39      0.63      0.48       593\n",
      "\n",
      "    accuracy                           0.38      2489\n",
      "   macro avg       0.38      0.38      0.28      2489\n",
      "weighted avg       0.38      0.38      0.28      2489\n",
      "\n",
      "train loss 0.708, val loss 2.697, val accuracy 0.389, and val rmse 1.457\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.40      0.79      0.53       660\n",
      "         1.0       0.38      0.03      0.05       616\n",
      "         2.0       0.33      0.05      0.08       620\n",
      "         3.0       0.38      0.67      0.49       593\n",
      "\n",
      "    accuracy                           0.39      2489\n",
      "   macro avg       0.37      0.38      0.29      2489\n",
      "weighted avg       0.37      0.39      0.29      2489\n",
      "\n",
      "train loss 0.697, val loss 2.885, val accuracy 0.386, and val rmse 1.481\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.38      0.81      0.52       660\n",
      "         1.0       0.50      0.03      0.05       616\n",
      "         2.0       0.36      0.04      0.07       620\n",
      "         3.0       0.39      0.65      0.49       593\n",
      "\n",
      "    accuracy                           0.39      2489\n",
      "   macro avg       0.41      0.38      0.28      2489\n",
      "weighted avg       0.41      0.39      0.28      2489\n",
      "\n",
      "train loss 0.687, val loss 2.878, val accuracy 0.385, and val rmse 1.477\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.40      0.79      0.53       660\n",
      "         1.0       0.44      0.02      0.05       616\n",
      "         2.0       0.35      0.04      0.07       620\n",
      "         3.0       0.37      0.67      0.48       593\n",
      "\n",
      "    accuracy                           0.39      2489\n",
      "   macro avg       0.39      0.38      0.28      2489\n",
      "weighted avg       0.39      0.39      0.28      2489\n",
      "\n",
      "train loss 0.685, val loss 2.912, val accuracy 0.389, and val rmse 1.475\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.39      0.80      0.52       660\n",
      "         1.0       0.46      0.03      0.05       616\n",
      "         2.0       0.36      0.05      0.08       620\n",
      "         3.0       0.39      0.67      0.49       593\n",
      "\n",
      "    accuracy                           0.39      2489\n",
      "   macro avg       0.40      0.38      0.29      2489\n",
      "weighted avg       0.40      0.39      0.29      2489\n",
      "\n",
      "train loss 0.665, val loss 3.000, val accuracy 0.384, and val rmse 1.483\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.39      0.78      0.52       660\n",
      "         1.0       0.38      0.01      0.03       616\n",
      "         2.0       0.36      0.04      0.08       620\n",
      "         3.0       0.37      0.69      0.48       593\n",
      "\n",
      "    accuracy                           0.38      2489\n",
      "   macro avg       0.37      0.38      0.28      2489\n",
      "weighted avg       0.37      0.38      0.28      2489\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train_model(model_variable.to(device))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Full content + title (kept punc, deleted non-common words) [4 quantiles]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss 1.389, val loss 1.386, val accuracy 0.266, and val rmse 1.492\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.29      0.47      0.36       662\n",
      "         1.0       0.24      0.51      0.32       622\n",
      "         2.0       0.27      0.03      0.05       621\n",
      "         3.0       0.44      0.04      0.07       611\n",
      "\n",
      "    accuracy                           0.27      2516\n",
      "   macro avg       0.31      0.26      0.20      2516\n",
      "weighted avg       0.31      0.27      0.20      2516\n",
      "\n",
      "train loss 1.373, val loss 1.377, val accuracy 0.277, and val rmse 1.556\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.29      0.38      0.33       662\n",
      "         1.0       0.25      0.29      0.27       622\n",
      "         2.0       0.26      0.15      0.19       621\n",
      "         3.0       0.31      0.27      0.29       611\n",
      "\n",
      "    accuracy                           0.28      2516\n",
      "   macro avg       0.28      0.28      0.27      2516\n",
      "weighted avg       0.28      0.28      0.27      2516\n",
      "\n",
      "train loss 1.362, val loss 1.372, val accuracy 0.275, and val rmse 1.624\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.30      0.31      0.30       662\n",
      "         1.0       0.25      0.19      0.22       622\n",
      "         2.0       0.26      0.16      0.20       621\n",
      "         3.0       0.28      0.44      0.34       611\n",
      "\n",
      "    accuracy                           0.28      2516\n",
      "   macro avg       0.27      0.28      0.26      2516\n",
      "weighted avg       0.27      0.28      0.26      2516\n",
      "\n",
      "train loss 1.350, val loss 1.366, val accuracy 0.291, and val rmse 1.587\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.31      0.37      0.34       662\n",
      "         1.0       0.27      0.22      0.24       622\n",
      "         2.0       0.26      0.15      0.19       621\n",
      "         3.0       0.29      0.42      0.35       611\n",
      "\n",
      "    accuracy                           0.29      2516\n",
      "   macro avg       0.29      0.29      0.28      2516\n",
      "weighted avg       0.29      0.29      0.28      2516\n",
      "\n",
      "train loss 1.341, val loss 1.362, val accuracy 0.293, and val rmse 1.549\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.31      0.38      0.34       662\n",
      "         1.0       0.28      0.25      0.27       622\n",
      "         2.0       0.26      0.18      0.21       621\n",
      "         3.0       0.30      0.35      0.32       611\n",
      "\n",
      "    accuracy                           0.29      2516\n",
      "   macro avg       0.29      0.29      0.29      2516\n",
      "weighted avg       0.29      0.29      0.29      2516\n",
      "\n",
      "train loss 1.328, val loss 1.358, val accuracy 0.302, and val rmse 1.513\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.33      0.37      0.35       662\n",
      "         1.0       0.29      0.25      0.27       622\n",
      "         2.0       0.26      0.21      0.23       621\n",
      "         3.0       0.31      0.38      0.34       611\n",
      "\n",
      "    accuracy                           0.30      2516\n",
      "   macro avg       0.30      0.30      0.30      2516\n",
      "weighted avg       0.30      0.30      0.30      2516\n",
      "\n",
      "train loss 1.317, val loss 1.355, val accuracy 0.307, and val rmse 1.511\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.33      0.36      0.34       662\n",
      "         1.0       0.28      0.23      0.25       622\n",
      "         2.0       0.26      0.21      0.23       621\n",
      "         3.0       0.33      0.43      0.37       611\n",
      "\n",
      "    accuracy                           0.31      2516\n",
      "   macro avg       0.30      0.31      0.30      2516\n",
      "weighted avg       0.30      0.31      0.30      2516\n",
      "\n",
      "train loss 1.298, val loss 1.356, val accuracy 0.318, and val rmse 1.517\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.35      0.37      0.36       662\n",
      "         1.0       0.28      0.21      0.24       622\n",
      "         2.0       0.27      0.17      0.21       621\n",
      "         3.0       0.33      0.51      0.40       611\n",
      "\n",
      "    accuracy                           0.32      2516\n",
      "   macro avg       0.31      0.32      0.30      2516\n",
      "weighted avg       0.31      0.32      0.30      2516\n",
      "\n",
      "train loss 1.279, val loss 1.379, val accuracy 0.329, and val rmse 1.505\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.42      0.34      0.38       662\n",
      "         1.0       0.28      0.19      0.23       622\n",
      "         2.0       0.29      0.15      0.20       621\n",
      "         3.0       0.32      0.64      0.42       611\n",
      "\n",
      "    accuracy                           0.33      2516\n",
      "   macro avg       0.33      0.33      0.31      2516\n",
      "weighted avg       0.33      0.33      0.31      2516\n",
      "\n",
      "train loss 1.250, val loss 1.390, val accuracy 0.335, and val rmse 1.484\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.45      0.32      0.38       662\n",
      "         1.0       0.26      0.14      0.18       622\n",
      "         2.0       0.29      0.20      0.24       621\n",
      "         3.0       0.33      0.68      0.44       611\n",
      "\n",
      "    accuracy                           0.34      2516\n",
      "   macro avg       0.33      0.34      0.31      2516\n",
      "weighted avg       0.33      0.34      0.31      2516\n",
      "\n",
      "train loss 1.226, val loss 1.432, val accuracy 0.331, and val rmse 1.494\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.51      0.28      0.36       662\n",
      "         1.0       0.25      0.12      0.16       622\n",
      "         2.0       0.27      0.18      0.22       621\n",
      "         3.0       0.32      0.75      0.45       611\n",
      "\n",
      "    accuracy                           0.33      2516\n",
      "   macro avg       0.34      0.33      0.30      2516\n",
      "weighted avg       0.34      0.33      0.30      2516\n",
      "\n",
      "train loss 1.195, val loss 1.483, val accuracy 0.322, and val rmse 1.522\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.54      0.22      0.31       662\n",
      "         1.0       0.28      0.10      0.15       622\n",
      "         2.0       0.26      0.21      0.23       621\n",
      "         3.0       0.31      0.77      0.44       611\n",
      "\n",
      "    accuracy                           0.32      2516\n",
      "   macro avg       0.35      0.33      0.28      2516\n",
      "weighted avg       0.35      0.32      0.28      2516\n",
      "\n",
      "train loss 1.176, val loss 1.616, val accuracy 0.306, and val rmse 1.605\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.54      0.15      0.24       662\n",
      "         1.0       0.27      0.07      0.11       622\n",
      "         2.0       0.26      0.19      0.22       621\n",
      "         3.0       0.29      0.83      0.44       611\n",
      "\n",
      "    accuracy                           0.31      2516\n",
      "   macro avg       0.34      0.31      0.25      2516\n",
      "weighted avg       0.34      0.31      0.25      2516\n",
      "\n",
      "train loss 1.143, val loss 1.688, val accuracy 0.314, and val rmse 1.615\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.57      0.17      0.26       662\n",
      "         1.0       0.36      0.06      0.10       622\n",
      "         2.0       0.27      0.21      0.24       621\n",
      "         3.0       0.29      0.84      0.43       611\n",
      "\n",
      "    accuracy                           0.31      2516\n",
      "   macro avg       0.37      0.32      0.26      2516\n",
      "weighted avg       0.38      0.31      0.26      2516\n",
      "\n",
      "train loss 1.106, val loss 1.828, val accuracy 0.294, and val rmse 1.683\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.57      0.10      0.17       662\n",
      "         1.0       0.42      0.04      0.07       622\n",
      "         2.0       0.26      0.19      0.22       621\n",
      "         3.0       0.28      0.87      0.42       611\n",
      "\n",
      "    accuracy                           0.29      2516\n",
      "   macro avg       0.38      0.30      0.22      2516\n",
      "weighted avg       0.39      0.29      0.22      2516\n",
      "\n",
      "train loss 1.086, val loss 1.759, val accuracy 0.312, and val rmse 1.615\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.59      0.17      0.26       662\n",
      "         1.0       0.37      0.05      0.10       622\n",
      "         2.0       0.27      0.20      0.23       621\n",
      "         3.0       0.29      0.84      0.43       611\n",
      "\n",
      "    accuracy                           0.31      2516\n",
      "   macro avg       0.38      0.32      0.26      2516\n",
      "weighted avg       0.38      0.31      0.25      2516\n",
      "\n",
      "train loss 1.062, val loss 1.973, val accuracy 0.293, and val rmse 1.700\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.57      0.12      0.19       662\n",
      "         1.0       0.38      0.03      0.05       622\n",
      "         2.0       0.27      0.16      0.20       621\n",
      "         3.0       0.28      0.89      0.42       611\n",
      "\n",
      "    accuracy                           0.29      2516\n",
      "   macro avg       0.37      0.30      0.22      2516\n",
      "weighted avg       0.38      0.29      0.22      2516\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss 1.042, val loss 1.792, val accuracy 0.333, and val rmse 1.565\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.62      0.25      0.35       662\n",
      "         1.0       0.33      0.04      0.07       622\n",
      "         2.0       0.28      0.20      0.24       621\n",
      "         3.0       0.30      0.86      0.45       611\n",
      "\n",
      "    accuracy                           0.33      2516\n",
      "   macro avg       0.38      0.34      0.28      2516\n",
      "weighted avg       0.39      0.33      0.28      2516\n",
      "\n",
      "train loss 1.008, val loss 1.917, val accuracy 0.327, and val rmse 1.616\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.63      0.24      0.35       662\n",
      "         1.0       0.42      0.04      0.07       622\n",
      "         2.0       0.28      0.16      0.20       621\n",
      "         3.0       0.29      0.89      0.44       611\n",
      "\n",
      "    accuracy                           0.33      2516\n",
      "   macro avg       0.40      0.33      0.26      2516\n",
      "weighted avg       0.41      0.33      0.26      2516\n",
      "\n",
      "train loss 0.989, val loss 1.989, val accuracy 0.330, and val rmse 1.609\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.65      0.24      0.36       662\n",
      "         1.0       0.47      0.04      0.07       622\n",
      "         2.0       0.29      0.17      0.21       621\n",
      "         3.0       0.29      0.89      0.44       611\n",
      "\n",
      "    accuracy                           0.33      2516\n",
      "   macro avg       0.42      0.33      0.27      2516\n",
      "weighted avg       0.43      0.33      0.27      2516\n",
      "\n",
      "train loss 0.968, val loss 2.103, val accuracy 0.323, and val rmse 1.633\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.66      0.23      0.34       662\n",
      "         1.0       0.46      0.03      0.05       622\n",
      "         2.0       0.28      0.14      0.19       621\n",
      "         3.0       0.29      0.90      0.44       611\n",
      "\n",
      "    accuracy                           0.32      2516\n",
      "   macro avg       0.42      0.33      0.26      2516\n",
      "weighted avg       0.43      0.32      0.26      2516\n",
      "\n",
      "train loss 0.940, val loss 2.095, val accuracy 0.333, and val rmse 1.609\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.69      0.28      0.39       662\n",
      "         1.0       0.47      0.02      0.05       622\n",
      "         2.0       0.28      0.14      0.19       621\n",
      "         3.0       0.29      0.90      0.44       611\n",
      "\n",
      "    accuracy                           0.33      2516\n",
      "   macro avg       0.43      0.34      0.27      2516\n",
      "weighted avg       0.44      0.33      0.27      2516\n",
      "\n",
      "train loss 0.930, val loss 2.122, val accuracy 0.344, and val rmse 1.583\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.63      0.33      0.43       662\n",
      "         1.0       0.44      0.02      0.05       622\n",
      "         2.0       0.30      0.14      0.19       621\n",
      "         3.0       0.30      0.90      0.45       611\n",
      "\n",
      "    accuracy                           0.34      2516\n",
      "   macro avg       0.42      0.35      0.28      2516\n",
      "weighted avg       0.42      0.34      0.28      2516\n",
      "\n",
      "train loss 0.906, val loss 2.258, val accuracy 0.312, and val rmse 1.648\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.69      0.18      0.29       662\n",
      "         1.0       0.34      0.02      0.05       622\n",
      "         2.0       0.27      0.17      0.21       621\n",
      "         3.0       0.28      0.89      0.43       611\n",
      "\n",
      "    accuracy                           0.31      2516\n",
      "   macro avg       0.40      0.32      0.24      2516\n",
      "weighted avg       0.40      0.31      0.24      2516\n",
      "\n",
      "train loss 0.897, val loss 2.178, val accuracy 0.345, and val rmse 1.567\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.66      0.31      0.43       662\n",
      "         1.0       0.46      0.03      0.05       622\n",
      "         2.0       0.29      0.16      0.21       621\n",
      "         3.0       0.30      0.89      0.45       611\n",
      "\n",
      "    accuracy                           0.35      2516\n",
      "   macro avg       0.43      0.35      0.28      2516\n",
      "weighted avg       0.43      0.35      0.28      2516\n",
      "\n",
      "train loss 0.874, val loss 2.238, val accuracy 0.345, and val rmse 1.562\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.66      0.31      0.43       662\n",
      "         1.0       0.49      0.03      0.06       622\n",
      "         2.0       0.28      0.16      0.21       621\n",
      "         3.0       0.30      0.88      0.45       611\n",
      "\n",
      "    accuracy                           0.34      2516\n",
      "   macro avg       0.43      0.35      0.28      2516\n",
      "weighted avg       0.44      0.34      0.29      2516\n",
      "\n",
      "train loss 0.855, val loss 2.307, val accuracy 0.344, and val rmse 1.578\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.68      0.31      0.42       662\n",
      "         1.0       0.50      0.03      0.06       622\n",
      "         2.0       0.29      0.16      0.20       621\n",
      "         3.0       0.30      0.89      0.45       611\n",
      "\n",
      "    accuracy                           0.34      2516\n",
      "   macro avg       0.44      0.35      0.28      2516\n",
      "weighted avg       0.45      0.34      0.28      2516\n",
      "\n",
      "train loss 0.841, val loss 2.427, val accuracy 0.345, and val rmse 1.584\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.67      0.31      0.43       662\n",
      "         1.0       0.53      0.03      0.06       622\n",
      "         2.0       0.30      0.14      0.19       621\n",
      "         3.0       0.29      0.90      0.44       611\n",
      "\n",
      "    accuracy                           0.34      2516\n",
      "   macro avg       0.45      0.35      0.28      2516\n",
      "weighted avg       0.45      0.34      0.28      2516\n",
      "\n",
      "train loss 0.823, val loss 2.469, val accuracy 0.344, and val rmse 1.588\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.67      0.31      0.43       662\n",
      "         1.0       0.55      0.03      0.05       622\n",
      "         2.0       0.29      0.14      0.19       621\n",
      "         3.0       0.30      0.91      0.45       611\n",
      "\n",
      "    accuracy                           0.34      2516\n",
      "   macro avg       0.45      0.35      0.28      2516\n",
      "weighted avg       0.46      0.34      0.28      2516\n",
      "\n",
      "train loss 0.819, val loss 2.522, val accuracy 0.339, and val rmse 1.597\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.69      0.29      0.40       662\n",
      "         1.0       0.50      0.03      0.05       622\n",
      "         2.0       0.29      0.15      0.20       621\n",
      "         3.0       0.29      0.91      0.44       611\n",
      "\n",
      "    accuracy                           0.34      2516\n",
      "   macro avg       0.44      0.34      0.27      2516\n",
      "weighted avg       0.45      0.34      0.27      2516\n",
      "\n",
      "train loss 0.795, val loss 2.688, val accuracy 0.324, and val rmse 1.634\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.69      0.26      0.38       662\n",
      "         1.0       0.28      0.01      0.02       622\n",
      "         2.0       0.27      0.12      0.17       621\n",
      "         3.0       0.29      0.92      0.44       611\n",
      "\n",
      "    accuracy                           0.32      2516\n",
      "   macro avg       0.38      0.33      0.25      2516\n",
      "weighted avg       0.39      0.32      0.25      2516\n",
      "\n",
      "train loss 0.783, val loss 2.595, val accuracy 0.338, and val rmse 1.599\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.67      0.30      0.41       662\n",
      "         1.0       0.50      0.03      0.05       622\n",
      "         2.0       0.27      0.12      0.17       621\n",
      "         3.0       0.29      0.91      0.44       611\n",
      "\n",
      "    accuracy                           0.34      2516\n",
      "   macro avg       0.43      0.34      0.27      2516\n",
      "weighted avg       0.44      0.34      0.27      2516\n",
      "\n",
      "train loss 0.775, val loss 2.534, val accuracy 0.356, and val rmse 1.551\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.65      0.36      0.46       662\n",
      "         1.0       0.51      0.03      0.06       622\n",
      "         2.0       0.29      0.14      0.18       621\n",
      "         3.0       0.30      0.91      0.46       611\n",
      "\n",
      "    accuracy                           0.36      2516\n",
      "   macro avg       0.44      0.36      0.29      2516\n",
      "weighted avg       0.44      0.36      0.29      2516\n",
      "\n",
      "train loss 0.757, val loss 2.600, val accuracy 0.350, and val rmse 1.565\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.64      0.34      0.45       662\n",
      "         1.0       0.52      0.03      0.05       622\n",
      "         2.0       0.29      0.14      0.19       621\n",
      "         3.0       0.30      0.90      0.45       611\n",
      "\n",
      "    accuracy                           0.35      2516\n",
      "   macro avg       0.44      0.35      0.28      2516\n",
      "weighted avg       0.44      0.35      0.29      2516\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss 0.757, val loss 2.701, val accuracy 0.348, and val rmse 1.572\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.64      0.35      0.45       662\n",
      "         1.0       0.55      0.03      0.05       622\n",
      "         2.0       0.28      0.12      0.17       621\n",
      "         3.0       0.30      0.91      0.45       611\n",
      "\n",
      "    accuracy                           0.35      2516\n",
      "   macro avg       0.44      0.35      0.28      2516\n",
      "weighted avg       0.44      0.35      0.28      2516\n",
      "\n",
      "train loss 0.733, val loss 2.782, val accuracy 0.347, and val rmse 1.580\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.64      0.36      0.46       662\n",
      "         1.0       0.55      0.03      0.05       622\n",
      "         2.0       0.27      0.09      0.14       621\n",
      "         3.0       0.30      0.92      0.45       611\n",
      "\n",
      "    accuracy                           0.35      2516\n",
      "   macro avg       0.44      0.35      0.27      2516\n",
      "weighted avg       0.44      0.35      0.28      2516\n",
      "\n",
      "train loss 0.714, val loss 2.793, val accuracy 0.346, and val rmse 1.574\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.65      0.36      0.46       662\n",
      "         1.0       0.50      0.02      0.04       622\n",
      "         2.0       0.25      0.09      0.13       621\n",
      "         3.0       0.30      0.92      0.45       611\n",
      "\n",
      "    accuracy                           0.35      2516\n",
      "   macro avg       0.42      0.35      0.27      2516\n",
      "weighted avg       0.43      0.35      0.27      2516\n",
      "\n",
      "train loss 0.714, val loss 2.846, val accuracy 0.345, and val rmse 1.572\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.63      0.35      0.45       662\n",
      "         1.0       0.50      0.02      0.03       622\n",
      "         2.0       0.26      0.11      0.15       621\n",
      "         3.0       0.30      0.92      0.45       611\n",
      "\n",
      "    accuracy                           0.35      2516\n",
      "   macro avg       0.42      0.35      0.27      2516\n",
      "weighted avg       0.43      0.35      0.27      2516\n",
      "\n",
      "train loss 0.706, val loss 2.934, val accuracy 0.339, and val rmse 1.593\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.65      0.32      0.43       662\n",
      "         1.0       0.41      0.01      0.02       622\n",
      "         2.0       0.26      0.11      0.15       621\n",
      "         3.0       0.30      0.93      0.45       611\n",
      "\n",
      "    accuracy                           0.34      2516\n",
      "   macro avg       0.40      0.34      0.26      2516\n",
      "weighted avg       0.41      0.34      0.26      2516\n",
      "\n",
      "train loss 0.696, val loss 2.994, val accuracy 0.337, and val rmse 1.598\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.65      0.32      0.43       662\n",
      "         1.0       0.53      0.01      0.03       622\n",
      "         2.0       0.25      0.10      0.14       621\n",
      "         3.0       0.29      0.93      0.45       611\n",
      "\n",
      "    accuracy                           0.34      2516\n",
      "   macro avg       0.43      0.34      0.26      2516\n",
      "weighted avg       0.44      0.34      0.26      2516\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train_model(model_variable.to(device))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Full content + title (kept punc) [4 quantiles]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "CUDA out of memory. Tried to allocate 2.00 MiB (GPU 0; 8.00 GiB total capacity; 4.50 GiB already allocated; 0 bytes free; 5.67 GiB reserved in total by PyTorch)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-37-1c3194146ba1>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mtrain_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel_variable\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-24-eaa7f02a4075>\u001b[0m in \u001b[0;36mtrain_model\u001b[1;34m(model)\u001b[0m\n\u001b[0;32m     10\u001b[0m             \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlong\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m             \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlong\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 12\u001b[1;33m             \u001b[0my_pred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0ml\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     13\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m             \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mF\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcross_entropy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_pred\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mE:\\Programs\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    548\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    549\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 550\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    551\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    552\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-19-6d6c5d19cd6a>\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, x, s)\u001b[0m\n\u001b[0;32m     11\u001b[0m         \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0membeddings\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m         \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdropout\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 13\u001b[1;33m         \u001b[0mx_pack\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpack_padded_sequence\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0ms\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_first\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0menforce_sorted\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     14\u001b[0m         \u001b[0mout_pack\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mht\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mct\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlstm\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_pack\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m         \u001b[0mout\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mht\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mE:\\Programs\\lib\\site-packages\\torch\\nn\\utils\\rnn.py\u001b[0m in \u001b[0;36mpack_padded_sequence\u001b[1;34m(input, lengths, batch_first, enforce_sorted)\u001b[0m\n\u001b[0;32m    242\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    243\u001b[0m     \u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_sizes\u001b[0m \u001b[1;33m=\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 244\u001b[1;33m         \u001b[0m_VF\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_pack_padded_sequence\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlengths\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_first\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    245\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0m_packed_sequence_init\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_sizes\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msorted_indices\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    246\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: CUDA out of memory. Tried to allocate 2.00 MiB (GPU 0; 8.00 GiB total capacity; 4.50 GiB already allocated; 0 bytes free; 5.67 GiB reserved in total by PyTorch)"
     ]
    }
   ],
=======
   "execution_count": null,
   "metadata": {},
   "outputs": [],
>>>>>>> b4a2e592245f7bee6a5e05a9b63fd51bb9cd5b5f
   "source": [
    "train_model(model_variable.to(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
