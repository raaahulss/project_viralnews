{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from textblob import TextBlob\n",
    "from datetime import datetime\n",
    "\n",
    "import spacy\n",
    "import torch\n",
    "import torchtext\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader, WeightedRandomSampler\n",
    "import torch.nn.functional as F\n",
    "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler    \n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.utils import class_weight\n",
    "\n",
    "#from torchsampler import ImbalancedDatasetSampler\n",
    "\n",
    "#from imblearn.over_sampling import SMOTE\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import wordnet as wn\n",
    "\n",
    "from collections import Counter\n",
    "import re\n",
    "import string\n",
    "from random import randint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 100\n",
    "BATCH_SIZE = 700\n",
    "LEARNING_RATE = 0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in CSV\n",
    "df = pd.read_csv(\"../data/train_80.csv\", sep=\"|\")\n",
    "df_test = pd.read_csv(\"../data/test_80.csv\", sep=\"|\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remove stop words (from content and title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove stop words\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "df['content'] = df.content.str.replace(\"[^\\w\\s]\",\"\").str.lower()\n",
    "df['content'] = df['content'].apply(lambda x: ' '.join([item for item in x.split() if item not in stop_words]))\n",
    "\n",
    "df_test['content'] = df_test.content.str.replace(\"[^\\w\\s]\",\"\").str.lower()\n",
    "df_test['content'] = df_test['content'].apply(lambda x: ' '.join([item for item in x.split() if item not in stop_words]))\n",
    "\n",
    "df['title'] = df.title.str.replace(\"[^\\w\\s]\",\"\").str.lower()\n",
    "df['title'] = df['title'].apply(lambda x: ' '.join([item for item in x.split() if item not in stop_words]))\n",
    "\n",
    "df_test['title'] = df_test.title.str.replace(\"[^\\w\\s]\",\"\").str.lower()\n",
    "df_test['title'] = df_test['title'].apply(lambda x: ' '.join([item for item in x.split() if item not in stop_words]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# label 0 -> gen 5 for each\n",
    "# label 1 -> skip\n",
    "# label 2 -> gen 2\n",
    "# label 3 -> gen 15\n",
    "\n",
    "def get_syn(word):\n",
    "    replacements = []\n",
    "    for syn in wn.synsets(word):\n",
    "        syn_word = syn.name().split('.')[0]\n",
    "        if syn_word != word and re.match('\\w', word):\n",
    "            replacements.append(syn_word)\n",
    "            \n",
    "    if len(replacements) > 0:\n",
    "        return replacements[randint(0, len(replacements)-1)]\n",
    "    else:\n",
    "        return ''\n",
    "\n",
    "def random_insertion(sentence, alpha=0.5):\n",
    "    new_sentence = sentence.copy()\n",
    "    for i in range(int(len(sentence) * alpha)):\n",
    "        syn = get_syn(sentence[randint(0, len(sentence) - 1)])\n",
    "        if syn:\n",
    "            new_sentence.insert(randint(0, len(new_sentence)), syn)\n",
    "    return new_sentence\n",
    "\n",
    "def random_replacement(sentence, alpha=0.5):\n",
    "    new_sentence = sentence.copy()\n",
    "    for i in range(int(len(sentence) * alpha)):\n",
    "        word_id = randint(0, len(sentence) - 1)\n",
    "        syn = get_syn(sentence[word_id])\n",
    "        if syn:\n",
    "            new_sentence[word_id] = syn\n",
    "    return new_sentence\n",
    "\n",
    "def augment(dataframe):\n",
    "    df = dataframe   \n",
    "    df = df[[\"title\", \"content\", \"label_log_10\"]]\n",
    "    new_sentences_df = pd.DataFrame(columns=['title','content','label_log_10'])\n",
    "    for index, row in df.iterrows():\n",
    "        if row[\"label_log_10\"] is 0:\n",
    "            for i in range (5):\n",
    "                tmp = \" \"\n",
    "                tmp_title = \" \"\n",
    "                str_list = list(row[\"content\"].split(\" \"))\n",
    "                title_list = list(row[\"title\"].split(\" \"))\n",
    "                #ew_sentence = random_insertion(str_list)\n",
    "                #ew_title = random_insertion(title_list)\n",
    "                new_sentence = random_replacement(str_list)\n",
    "                new_title = random_replacement(title_list)\n",
    "                new_sentence = tmp.join(new_sentence)\n",
    "                new_title = tmp_title.join(new_title)\n",
    "                new_sentences_df = new_sentences_df.append(pd.DataFrame([[new_title, new_sentence, row[\"label_log_10\"]]], columns=['title','content','label_log_10']))\n",
    "            continue\n",
    "        if row[\"label_log_10\"] is 2:\n",
    "            for j in range (2):\n",
    "                tmp = \" \"\n",
    "                tmp_title = \" \"\n",
    "                str_list = list(row[\"content\"].split(\" \"))\n",
    "                title_list = list(row[\"title\"].split(\" \"))\n",
    "                #new_sentence = random_insertion(str_list)\n",
    "                #new_title = random_insertion(title_list)\n",
    "                new_sentence = random_replacement(str_list)\n",
    "                new_title = random_replacement(title_list)\n",
    "                new_sentence = tmp.join(new_sentence)\n",
    "                new_title = tmp_title.join(new_title)\n",
    "                new_sentences_df = new_sentences_df.append(pd.DataFrame([[new_title, new_sentence, row[\"label_log_10\"]]], columns=['title','content','label_log_10']))\n",
    "            continue\n",
    "        if row[\"label_log_10\"] is 3:\n",
    "            for k in range (15):\n",
    "                tmp = \" \"\n",
    "                tmp_title = \" \"\n",
    "                str_list = list(row[\"content\"].split(\" \"))\n",
    "                title_list = list(row[\"title\"].split(\" \"))\n",
    "                #new_sentence = random_insertion(str_list)\n",
    "                #new_title = random_insertion(title_list)\n",
    "                new_sentence = random_replacement(str_list)\n",
    "                new_title = random_replacement(title_list)\n",
    "                new_sentence = tmp.join(new_sentence)\n",
    "                new_title = tmp_title.join(new_title)\n",
    "                new_sentences_df = new_sentences_df.append(pd.DataFrame([[new_title, new_sentence, row[\"label_log_10\"]]], columns=['title','content','label_log_10']))\n",
    "            continue\n",
    "\n",
    "    df = df.append(new_sentences_df)\n",
    "    return df\n",
    "\n",
    "df = augment(df)\n",
    "df_test = augment(df_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# combine title and text, delete other columns\n",
    "df[\"full_text\"] = df['title'] + ' ' + df[\"content\"]\n",
    "#df[\"full_text\"] =  df[\"content\"]\n",
    "df = df[[\"full_text\", \"label_log_10\"]]\n",
    "\n",
    "df_test[\"full_text\"] = df_test['title'] + ' ' + df_test[\"content\"] \n",
    "df_test = df_test[[\"full_text\", \"label_log_10\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = df\n",
    "test = df_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'0': 8652, '1': 15104, '2': 17316, '3': 7344}\n",
      "{'0': 2118, '1': 3809, '2': 4275, '3': 1712}\n"
     ]
    }
   ],
   "source": [
    "def balance_classes(dataframe):\n",
    "    train = dataframe\n",
    "    count_dict = {\n",
    "        \"0\":0,\n",
    "        \"1\":0,\n",
    "        \"2\":0,\n",
    "        \"3\":0\n",
    "    }\n",
    "\n",
    "    for index, row in train.iterrows():\n",
    "        if row['label_log_10'] == 0.0:\n",
    "            count_dict[\"0\"] += 1\n",
    "        elif row['label_log_10'] == 1.0:\n",
    "            count_dict[\"1\"] += 1\n",
    "        elif row['label_log_10'] == 2.0:\n",
    "            count_dict[\"2\"] += 1\n",
    "        else:\n",
    "            count_dict[\"3\"] += 1\n",
    "\n",
    "    print(count_dict)\n",
    "\n",
    "    train_0 = train[train.label_log_10 == 0]\n",
    "    train_1 = train[train.label_log_10 == 1]\n",
    "    train_2 = train[train.label_log_10 == 2]\n",
    "    train_3 = train[train.label_log_10 == 3]\n",
    "\n",
    "    train = train_0.sample(n=count_dict[\"3\"])\n",
    "    train = train.append(train_1.sample(n=count_dict[\"3\"]))\n",
    "    train = train.append(train_2.sample(n=count_dict[\"3\"]))\n",
    "    train = train.append(train_3.sample(n=count_dict[\"3\"]))\n",
    "    train = train.sample(frac=1)\n",
    "    #print(train.shape)\n",
    "    return train\n",
    "\n",
    "train = balance_classes(train)\n",
    "test = balance_classes(test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tokenization\n",
    "tok = spacy.load('en')\n",
    "def tokenize(text):\n",
    "    text = re.sub(r\"[^\\x00-\\x7F]+\", \" \", text)\n",
    "    regex = re.compile('[' + re.escape(string.punctuation) + '0-9\\\\r\\\\t\\\\n]')\n",
    "    nopunct = regex.sub(\" \", text.lower())\n",
    "    return [token.text for token in tok.tokenizer(nopunct)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#count number of occurences of each word in train set\n",
    "counts = Counter()\n",
    "for index, row in train.iterrows():\n",
    "    counts.update(tokenize(row['full_text']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating vocab\n",
    "vocab2index = {\"\":0, \"UNK\":1}\n",
    "words = [\"\",\"UNK\"]\n",
    "for word in counts:\n",
    "    vocab2index[word] = len(words)\n",
    "    words.append(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_sentence(text, vocab2index, N=500):\n",
    "    tokenized = tokenize(text)\n",
    "    encoded = np.zeros(N,dtype=int)\n",
    "    enc1 = np.array([vocab2index.get(word,vocab2index[\"UNK\"]) for word in tokenized])\n",
    "    length = min(N, len(enc1))\n",
    "    encoded[:length] = enc1[:length]\n",
    "    return encoded, length"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GloVe Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_glove_vectors(glove_file=\"../data/glove.6B.100d.txt\"):\n",
    "    \"\"\"Load the glove word vectors\"\"\"\n",
    "    word_vectors = {}\n",
    "    with open(glove_file, encoding=\"utf8\") as f:\n",
    "        for line in f:\n",
    "            split = line.split()\n",
    "            word_vectors[split[0]] = np.array([float(x) for x in split[1:]])\n",
    "    return word_vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_emb_matrix(pretrained, word_counts, emb_size = 100):\n",
    "    \"\"\" Creates embedding matrix from word vectors\"\"\"\n",
    "    vocab_size = len(word_counts) + 2\n",
    "    vocab_to_idx = {}\n",
    "    vocab = [\"\", \"UNK\"]\n",
    "    W = np.zeros((vocab_size, emb_size), dtype=\"float32\")\n",
    "    W[0] = np.zeros(emb_size, dtype='float32') # adding a vector for padding\n",
    "    W[1] = np.random.uniform(-0.25, 0.25, emb_size) # adding a vector for unknown words \n",
    "    vocab_to_idx[\"UNK\"] = 1\n",
    "    vocab_to_idx[\"\"] = 0\n",
    "    i = 2\n",
    "    for word in word_counts:\n",
    "        if word in word_vecs:\n",
    "            W[i] = word_vecs[word]\n",
    "        else:\n",
    "            W[i] = np.random.uniform(-0.25,0.25, emb_size)\n",
    "        vocab_to_idx[word] = i\n",
    "        vocab.append(word)\n",
    "        i += 1   \n",
    "    return W, np.array(vocab), vocab_to_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_vecs = load_glove_vectors()\n",
    "pretrained_weights, vocab, vocab2index = get_emb_matrix(word_vecs, counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "train['encoded'] = train['full_text'].apply(lambda x: np.array(encode_sentence(x,vocab2index)))\n",
    "test['encoded'] = test['full_text'].apply(lambda x: np.array(encode_sentence(x,vocab2index)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, y_train = list(train['encoded']), list(train['label_log_10'])\n",
    "X_valid, y_valid = list(test['encoded']), list(test['label_log_10'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NewsDataset(Dataset):\n",
    "    def __init__(self, X, Y):\n",
    "            self.X = X\n",
    "            self.y = Y\n",
    "            \n",
    "    def __len__(self):\n",
    "        return len(self.y)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return torch.from_numpy(self.X[idx][0].astype(np.int32)).to(device), self.y[idx], self.X[idx][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = NewsDataset(X_train, y_train)\n",
    "valid_ds = NewsDataset(X_valid, y_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model):\n",
    "    parameters = filter(lambda p: p.requires_grad, model.parameters())\n",
    "    optimizer = torch.optim.Adam(parameters, lr=LEARNING_RATE)\n",
    "    #torch.save(optimizer.state_dict(), \"../data/optimizer_dropout_admap.pt\")\n",
    "    #criterion = nn.CrossEntropyLoss(weight=weights.to(device))\n",
    "    for i in range(EPOCHS):\n",
    "        model.train()\n",
    "        sum_loss = 0.0\n",
    "        total = 0\n",
    "        for x, y, l in train_dl:\n",
    "            x = x.long().to(device)\n",
    "            y = y.long().to(device)\n",
    "            y_pred = model(x, l)\n",
    "            optimizer.zero_grad()\n",
    "            #loss = criterion(y_pred, y)\n",
    "            loss = F.cross_entropy(y_pred, y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            sum_loss += loss.item() *y.shape[0]\n",
    "            total += y.shape[0]\n",
    "        val_loss, val_acc, val_rmse, y_pred_list = validation_metrics(model, val_dl)\n",
    "        torch.save(optimizer.state_dict(), \"../data/optimizer_dropout_admap.pt\")\n",
    "        torch.save(model.state_dict(), \"../data/bi-lstm_model.pt\")\n",
    "        if i%5 == 0:\n",
    "            print(\"EPOCH \", i, \": train loss %.3f, val loss %.3f, val accuracy %.3f, and val rmse %.3f\" % (sum_loss/total, val_loss, val_acc, val_rmse))\n",
    "            print(classification_report(y_valid, y_pred_list))\n",
    "def validation_metrics (model, valid_dl):\n",
    "    #criterion = nn.CrossEntropyLoss(weight=weights)\n",
    "    y_pred_list = list()\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    sum_loss = 0.0\n",
    "    sum_rmse = 0.0\n",
    "    for x, y, l in valid_dl:\n",
    "        x = x.long().to(device)\n",
    "        y = y.long()\n",
    "        y_hat = model(x, l).cpu()\n",
    "        #loss = criterion(y_hat, y)\n",
    "        loss = F.cross_entropy(y_hat, y)\n",
    "        pred = torch.max(y_hat, 1)[1]\n",
    "        for i in pred:\n",
    "            tmp = int(i.numpy())\n",
    "            y_pred_list.append(tmp)\n",
    "        correct += (pred == y).float().sum()\n",
    "        total += y.shape[0]\n",
    "        sum_loss += loss.item()*y.shape[0]\n",
    "        sum_rmse += np.sqrt(mean_squared_error(pred, y.unsqueeze(-1)))*y.shape[0]\n",
    "    return sum_loss/total, correct/total, sum_rmse/total, y_pred_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = len(words)\n",
    "train_dl = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True)\n",
    "val_dl = DataLoader(valid_ds, batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTM_glove_vecs(torch.nn.Module) :\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, glove_weights) :\n",
    "        super().__init__()\n",
    "        self.embeddings = nn.Embedding(vocab_size, embedding_dim, padding_idx=0)\n",
    "        self.embeddings.weight.data.copy_(torch.from_numpy(glove_weights))\n",
    "        self.embeddings.weight.requires_grad = False ## freeze embeddings\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, batch_first=True, bidirectional=True, num_layers=2)\n",
    "        self.linear = nn.Linear(hidden_dim, 4)\n",
    "        self.dropout = nn.Dropout(0.2)\n",
    "        \n",
    "    def forward(self, x, l):\n",
    "        x = self.embeddings(x)\n",
    "        x = self.dropout(x)\n",
    "        lstm_out, (ht, ct) = self.lstm(x)\n",
    "        return self.linear(ht[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LSTM_glove_vecs(vocab_size, 100, 100, pretrained_weights)\n",
    "#model = LSTM_variable_len(vocab_size, 50, 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BiLSTM (content + title) (num_layers=2), hidden_dim=100 GloVe embeddings emb_size=100, no stop words (train + test), N=600, (batch_size=800) w/ random synonym replacement (on content + title) alpha=0.5 (80/20 split)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH  0 : train loss 1.260, val loss 1.185, val accuracy 0.453, and val rmse 1.269\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.60      0.58      0.59      1712\n",
      "           1       0.53      0.33      0.41      1712\n",
      "           2       0.31      0.11      0.16      1712\n",
      "           3       0.39      0.79      0.52      1712\n",
      "\n",
      "    accuracy                           0.45      6848\n",
      "   macro avg       0.45      0.45      0.42      6848\n",
      "weighted avg       0.45      0.45      0.42      6848\n",
      "\n",
      "EPOCH  5 : train loss 1.037, val loss 1.132, val accuracy 0.500, and val rmse 1.068\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.70      0.48      0.57      1712\n",
      "           1       0.45      0.64      0.53      1712\n",
      "           2       0.42      0.12      0.19      1712\n",
      "           3       0.47      0.75      0.58      1712\n",
      "\n",
      "    accuracy                           0.50      6848\n",
      "   macro avg       0.51      0.50      0.47      6848\n",
      "weighted avg       0.51      0.50      0.47      6848\n",
      "\n",
      "EPOCH  10 : train loss 0.896, val loss 0.965, val accuracy 0.585, and val rmse 0.987\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.70      0.66      0.68      1712\n",
      "           1       0.59      0.83      0.69      1712\n",
      "           2       0.39      0.20      0.26      1712\n",
      "           3       0.57      0.66      0.61      1712\n",
      "\n",
      "    accuracy                           0.58      6848\n",
      "   macro avg       0.56      0.58      0.56      6848\n",
      "weighted avg       0.56      0.58      0.56      6848\n",
      "\n",
      "EPOCH  15 : train loss 0.746, val loss 0.907, val accuracy 0.608, and val rmse 0.915\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.76      0.64      0.70      1712\n",
      "           1       0.63      0.85      0.72      1712\n",
      "           2       0.41      0.28      0.33      1712\n",
      "           3       0.58      0.67      0.62      1712\n",
      "\n",
      "    accuracy                           0.61      6848\n",
      "   macro avg       0.60      0.61      0.59      6848\n",
      "weighted avg       0.60      0.61      0.59      6848\n",
      "\n",
      "EPOCH  20 : train loss 0.714, val loss 0.913, val accuracy 0.612, and val rmse 0.874\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.77      0.73      0.75      1712\n",
      "           1       0.66      0.84      0.74      1712\n",
      "           2       0.40      0.37      0.38      1712\n",
      "           3       0.60      0.50      0.55      1712\n",
      "\n",
      "    accuracy                           0.61      6848\n",
      "   macro avg       0.61      0.61      0.60      6848\n",
      "weighted avg       0.61      0.61      0.60      6848\n",
      "\n",
      "EPOCH  25 : train loss 0.599, val loss 0.906, val accuracy 0.629, and val rmse 0.847\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.84      0.70      0.76      1712\n",
      "           1       0.68      0.85      0.76      1712\n",
      "           2       0.42      0.40      0.41      1712\n",
      "           3       0.59      0.57      0.58      1712\n",
      "\n",
      "    accuracy                           0.63      6848\n",
      "   macro avg       0.63      0.63      0.63      6848\n",
      "weighted avg       0.63      0.63      0.63      6848\n",
      "\n",
      "EPOCH  30 : train loss 0.489, val loss 1.087, val accuracy 0.603, and val rmse 0.838\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.84      0.66      0.74      1712\n",
      "           1       0.66      0.87      0.75      1712\n",
      "           2       0.40      0.53      0.45      1712\n",
      "           3       0.61      0.36      0.45      1712\n",
      "\n",
      "    accuracy                           0.60      6848\n",
      "   macro avg       0.63      0.60      0.60      6848\n",
      "weighted avg       0.63      0.60      0.60      6848\n",
      "\n",
      "EPOCH  35 : train loss 0.415, val loss 1.171, val accuracy 0.612, and val rmse 0.828\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.84      0.71      0.77      1712\n",
      "           1       0.70      0.84      0.77      1712\n",
      "           2       0.41      0.58      0.48      1712\n",
      "           3       0.60      0.31      0.41      1712\n",
      "\n",
      "    accuracy                           0.61      6848\n",
      "   macro avg       0.64      0.61      0.61      6848\n",
      "weighted avg       0.64      0.61      0.61      6848\n",
      "\n",
      "EPOCH  40 : train loss 0.365, val loss 1.271, val accuracy 0.610, and val rmse 0.842\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.82      0.71      0.76      1712\n",
      "           1       0.72      0.82      0.77      1712\n",
      "           2       0.40      0.57      0.47      1712\n",
      "           3       0.59      0.34      0.43      1712\n",
      "\n",
      "    accuracy                           0.61      6848\n",
      "   macro avg       0.63      0.61      0.61      6848\n",
      "weighted avg       0.63      0.61      0.61      6848\n",
      "\n",
      "EPOCH  45 : train loss 0.324, val loss 1.430, val accuracy 0.602, and val rmse 0.832\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.85      0.68      0.75      1712\n",
      "           1       0.71      0.83      0.77      1712\n",
      "           2       0.39      0.59      0.47      1712\n",
      "           3       0.59      0.31      0.40      1712\n",
      "\n",
      "    accuracy                           0.60      6848\n",
      "   macro avg       0.63      0.60      0.60      6848\n",
      "weighted avg       0.63      0.60      0.60      6848\n",
      "\n",
      "EPOCH  50 : train loss 0.294, val loss 1.480, val accuracy 0.620, and val rmse 0.841\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.80      0.76      0.78      1712\n",
      "           1       0.73      0.81      0.77      1712\n",
      "           2       0.41      0.57      0.48      1712\n",
      "           3       0.61      0.34      0.44      1712\n",
      "\n",
      "    accuracy                           0.62      6848\n",
      "   macro avg       0.64      0.62      0.62      6848\n",
      "weighted avg       0.64      0.62      0.62      6848\n",
      "\n",
      "EPOCH  55 : train loss 0.265, val loss 1.612, val accuracy 0.606, and val rmse 0.827\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.82      0.71      0.76      1712\n",
      "           1       0.71      0.82      0.76      1712\n",
      "           2       0.41      0.63      0.49      1712\n",
      "           3       0.62      0.26      0.37      1712\n",
      "\n",
      "    accuracy                           0.61      6848\n",
      "   macro avg       0.64      0.61      0.60      6848\n",
      "weighted avg       0.64      0.61      0.60      6848\n",
      "\n",
      "EPOCH  60 : train loss 0.239, val loss 1.688, val accuracy 0.611, and val rmse 0.823\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.84      0.70      0.76      1712\n",
      "           1       0.72      0.82      0.77      1712\n",
      "           2       0.41      0.63      0.49      1712\n",
      "           3       0.61      0.30      0.40      1712\n",
      "\n",
      "    accuracy                           0.61      6848\n",
      "   macro avg       0.64      0.61      0.61      6848\n",
      "weighted avg       0.64      0.61      0.61      6848\n",
      "\n",
      "EPOCH  65 : train loss 0.218, val loss 1.789, val accuracy 0.603, and val rmse 0.813\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.85      0.68      0.76      1712\n",
      "           1       0.69      0.84      0.76      1712\n",
      "           2       0.40      0.63      0.49      1712\n",
      "           3       0.61      0.25      0.36      1712\n",
      "\n",
      "    accuracy                           0.60      6848\n",
      "   macro avg       0.64      0.60      0.59      6848\n",
      "weighted avg       0.64      0.60      0.59      6848\n",
      "\n",
      "EPOCH  70 : train loss 0.212, val loss 1.725, val accuracy 0.609, and val rmse 0.832\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.81      0.75      0.78      1712\n",
      "           1       0.70      0.85      0.77      1712\n",
      "           2       0.40      0.57      0.47      1712\n",
      "           3       0.61      0.27      0.37      1712\n",
      "\n",
      "    accuracy                           0.61      6848\n",
      "   macro avg       0.63      0.61      0.60      6848\n",
      "weighted avg       0.63      0.61      0.60      6848\n",
      "\n",
      "EPOCH  75 : train loss 0.179, val loss 1.816, val accuracy 0.599, and val rmse 0.823\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.85      0.71      0.77      1712\n",
      "           1       0.74      0.77      0.75      1712\n",
      "           2       0.40      0.66      0.50      1712\n",
      "           3       0.58      0.26      0.36      1712\n",
      "\n",
      "    accuracy                           0.60      6848\n",
      "   macro avg       0.64      0.60      0.59      6848\n",
      "weighted avg       0.64      0.60      0.59      6848\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH  80 : train loss 0.172, val loss 2.109, val accuracy 0.589, and val rmse 0.837\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.85      0.64      0.73      1712\n",
      "           1       0.71      0.81      0.75      1712\n",
      "           2       0.40      0.69      0.50      1712\n",
      "           3       0.62      0.21      0.32      1712\n",
      "\n",
      "    accuracy                           0.59      6848\n",
      "   macro avg       0.64      0.59      0.58      6848\n",
      "weighted avg       0.64      0.59      0.58      6848\n",
      "\n",
      "EPOCH  85 : train loss 0.159, val loss 2.024, val accuracy 0.611, and val rmse 0.809\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.83      0.73      0.78      1712\n",
      "           1       0.69      0.85      0.76      1712\n",
      "           2       0.41      0.62      0.50      1712\n",
      "           3       0.62      0.24      0.35      1712\n",
      "\n",
      "    accuracy                           0.61      6848\n",
      "   macro avg       0.64      0.61      0.60      6848\n",
      "weighted avg       0.64      0.61      0.60      6848\n",
      "\n",
      "EPOCH  90 : train loss 0.140, val loss 2.097, val accuracy 0.598, and val rmse 0.813\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.85      0.69      0.76      1712\n",
      "           1       0.72      0.77      0.74      1712\n",
      "           2       0.40      0.70      0.51      1712\n",
      "           3       0.60      0.23      0.33      1712\n",
      "\n",
      "    accuracy                           0.60      6848\n",
      "   macro avg       0.65      0.60      0.59      6848\n",
      "weighted avg       0.65      0.60      0.59      6848\n",
      "\n",
      "EPOCH  95 : train loss 0.129, val loss 2.172, val accuracy 0.599, and val rmse 0.805\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.85      0.69      0.76      1712\n",
      "           1       0.68      0.82      0.74      1712\n",
      "           2       0.41      0.67      0.51      1712\n",
      "           3       0.63      0.22      0.32      1712\n",
      "\n",
      "    accuracy                           0.60      6848\n",
      "   macro avg       0.64      0.60      0.58      6848\n",
      "weighted avg       0.64      0.60      0.58      6848\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train_model(model.to(device))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save off Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('vocab_80.npy', 'wb') as f:\n",
    "    np.save(f, vocab, allow_pickle=True)\n",
    "with open('vocab2index_80.npy', 'wb') as f:\n",
    "    np.save(f, vocab2index, allow_pickle=True)\n",
    "with open('pretrained_weights_80.npy', 'wb') as f:\n",
    "    np.save(f, pretrained_weights, allow_pickle=True)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
