{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quick fix final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x1b3a8814370>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from nltk.corpus import stopwords\n",
    "import spacy\n",
    "import torch\n",
    "import torchtext\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler    \n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix, classification_report, classification_report\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "\n",
    "from collections import Counter\n",
    "import re\n",
    "import string\n",
    "import tqdm\n",
    "spacy.load('en_core_web_sm')\n",
    "stopwords = stopwords.words('english')\n",
    "# torch.backends.cudnn.enabled = False \n",
    "torch.random.manual_seed(123456)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 50\n",
    "BATCH_SIZE = 64\n",
    "LEARNING_RATE = 0.00001\n",
    "NODES = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7 ./dataset_70/train_70.csv ./dataset_70/test_70.csv\n"
     ]
    }
   ],
   "source": [
    "train_split = 70\n",
    "split_train = train_split/100\n",
    "embed_size = 300\n",
    "train_file_name = \"./dataset_{}/train_{}.csv\".format(train_split,train_split) \n",
    "test_file_name = \"./dataset_{}/test_{}.csv\".format(train_split,train_split) \n",
    "vocab_file_name =  \"./dataset_{}/vocab_{}.csv\".format(train_split,train_split)\n",
    "glove_filename = \"../../data/glove.6B.{}d.txt\".format(embed_size)\n",
    "print(split_train, train_file_name, test_file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(19930,\n",
       " 8541,\n",
       " Index(['tweet_id', 'created_time', 'count', '1', '2', '3', '4', '5', '6',\n",
       "        'user_id', 'screen_name', 'url', 'follower_count', 'title', 'content',\n",
       "        'expanded_url', 'title_len', 'content_len', 'max_retweets',\n",
       "        'label_log_10', 'label_mean', 'label_median', 'label_quantile',\n",
       "        'label_grouped_median', 'grouped_median'],\n",
       "       dtype='object'))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train = pd.read_csv(train_file_name, sep=\"|\", index_col=0)\n",
    "test = pd.read_csv(test_file_name, sep=\"|\", index_col=0)\n",
    "len(train),len(test), train.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train.loc[train.max_retweets==32870]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max number of words in len 24\n"
     ]
    }
   ],
   "source": [
    "tok = spacy.load('en_core_web_sm')\n",
    "max_len = -1\n",
    "def tokenize(text):\n",
    "    text = re.sub(r\"[^\\x00-\\x7F]+\", \" \", text)\n",
    "    regex = re.compile('[' + re.escape(string.punctuation) + '0-9\\\\r\\\\t\\\\n]')\n",
    "    nopunct_num = regex.sub(\" \", text.lower()) \n",
    "#     text = \" \".join([word for word in text.split() if word not in stopwords])\n",
    "    # Removing the odd apostrophes\n",
    "    tokens = [token for token in nopunct_num.split() if len(token)>=2 ]\n",
    "    return tokens\n",
    "#     text = re.sub(r\"[^\\x00-\\x7F]+\", \" \", text)\n",
    "#     regex = re.compile('[' + re.escape(string.punctuation) + '0-9\\\\r\\\\t\\\\n]')\n",
    "#     nopunct = regex.sub(\" \", text.lower())\n",
    "#     return [token.text for token in tok.tokenizer(nopunct)]\n",
    "#count number of occurences of each word\n",
    "\n",
    "counts = Counter()\n",
    "for index, row in train.iterrows():\n",
    "    tokenized = tokenize(row['title'])\n",
    "    if max_len < len(tokenized):\n",
    "        max_len = len(tokenized)\n",
    "    counts.update(tokenized)\n",
    "print(\"Max number of words in len\",max_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#deleting infrequent words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # creating vocab\n",
    "# vocab2index = {\"\":0, \"UNK\":1}\n",
    "# words = [\"\",\"UNK\"]\n",
    "# for word in counts:\n",
    "#     vocab2index[word] = len(words)\n",
    "#     words.append(word)\n",
    "# words[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_sentence(text, vocab2index, N=max_len):\n",
    "    tokenized = tokenize(text)\n",
    "    encoded = np.zeros(N,dtype=int)\n",
    "    enc1 = np.array([vocab2index.get(word,vocab2index[\"UNK\"]) for word in tokenized])\n",
    "    length = min(N, len(enc1))\n",
    "    encoded[:length] = enc1[:length]\n",
    "    return encoded, length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_glove_vectors(glove_file=glove_filename):\n",
    "    \"\"\"Load the glove word vectors\"\"\"\n",
    "    word_vectors = {}\n",
    "    with open(glove_file, encoding=\"utf8\") as f:\n",
    "        for line in f:\n",
    "            split = line.split()\n",
    "            word_vectors[split[0]] = np.array([float(x) for x in split[1:]])\n",
    "    return word_vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_emb_matrix(pretrained, word_counts, emb_size = embed_size):\n",
    "    \"\"\" Creates embedding matrix from word vectors\"\"\"\n",
    "    vocab_size = len(word_counts) + 2\n",
    "    vocab_to_idx = {}\n",
    "    vocab = [\"<pad>\", \"UNK\"]\n",
    "    W = np.zeros((vocab_size, emb_size), dtype=\"float32\")\n",
    "    W[0] = np.zeros(emb_size, dtype='float32') # adding a vector for padding\n",
    "    W[1] = np.random.uniform(-0.25, 0.25, emb_size) # adding a vector for unknown words \n",
    "    vocab_to_idx[\"<pad>\"] = 0\n",
    "    vocab_to_idx[\"UNK\"] = 1\n",
    "    i = 2\n",
    "    for word in word_counts:\n",
    "        if word in word_vecs:\n",
    "            W[i] = word_vecs[word]\n",
    "        else:\n",
    "            W[i] = np.random.uniform(-0.25,0.25, emb_size)\n",
    "        vocab_to_idx[word] = i\n",
    "        vocab.append(word)\n",
    "        i += 1   \n",
    "    return W, np.array(vocab), vocab_to_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_vecs = load_glove_vectors()\n",
    "pretrained_weights, vocab, vocab2index = get_emb_matrix(word_vecs, counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('vocab_70.npy', 'wb') as f:\n",
    "    np.save(f, vocab, allow_pickle=True)\n",
    "with open('vocab2index_70.npy', 'wb') as f:\n",
    "    np.save(f, vocab2index, allow_pickle=True)\n",
    "with open('pretrained_weights_70.npy', 'wb') as f:\n",
    "    np.save(f, pretrained_weights, allow_pickle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pretrained_weights, vocab, vocab2index = None, None, None\n",
    "with open('vocab_70.npy', 'rb') as f:\n",
    "    vocab = np.load(f)\n",
    "with open('vocab2index_70.npy', 'rb') as f:\n",
    "    vocab2index = np.load(f)\n",
    "with open('pretrained_weights_70.npy', 'rb') as f:\n",
    "    pretrained_weights = np.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = len(vocab)\n",
    "pretrained_weights.shape, len(vocab), len(vocab2index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['encoded'] = train['title'].apply(lambda x: np.array(encode_sentence(x,vocab2index)))\n",
    "test['encoded'] = test['title'].apply(lambda x: np.array(encode_sentence(x,vocab2index)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, y_train = train['encoded'], train['label_log_10']\n",
    "X_train.head(2),y_train.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test, y_test = test['encoded'], test['label_log_10']\n",
    "X_test.head(2), y_test.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NewsDataset(Dataset):\n",
    "    def __init__(self, X, Y):\n",
    "            self.X = X\n",
    "            self.y = Y\n",
    "            \n",
    "    def __len__(self):\n",
    "        return len(self.y)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return torch.from_numpy(self.X[idx][0].astype(np.int32)), self.y[idx], self.X[idx][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = NewsDataset(X_train, y_train)\n",
    "test_ds = NewsDataset(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model):\n",
    "    parameters = filter(lambda p: p.requires_grad, model.parameters())\n",
    "    optimizer = torch.optim.Adam(parameters, lr=LEARNING_RATE)#, momentum=0.7)\n",
    "#     model_fixed.load_state_dict(torch.load(\"./model_dropout.pt\"))\n",
    "#     optimizer.load_state_dict(torch.load(\"./optimizer_dropout.pt\"))\n",
    "    actual_loss = None\n",
    "    scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', verbose=True,\n",
    "                                                     patience=3, factor=0.6)\n",
    "    softmax = nn.LogSoftmax(dim=0)\n",
    "    for i in tqdm.tqdm(range(EPOCHS), total=EPOCHS):\n",
    "        model.train()\n",
    "        sum_loss = 0.0\n",
    "        total = 0\n",
    "        for x, y, l in train_dl:\n",
    "            x = x.long().to(device)\n",
    "            y = y.long().to(device)\n",
    "            y_pred = model(x, l)\n",
    "            y_hat = softmax(y_pred)\n",
    "            optimizer.zero_grad()\n",
    "            loss = F.cross_entropy(y_pred, y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            sum_loss += loss.item() *y.shape[0]\n",
    "            total += y.shape[0]\n",
    "        val_loss, val_acc, val_rmse = validation_metrics(model, test_dl, i)\n",
    "        scheduler.step(sum_loss)\n",
    "        actual_loss = val_loss\n",
    "#         print(\"Model Dict updated\")\n",
    "#         torch.save(model.state_dict(),\"./model_dropout.pt\" )\n",
    "#         torch.save(optimizer.state_dict(),\"./optimizer_dropout_adam.pt\")\n",
    "            \n",
    "#         if i%5 == 0:\n",
    "        print(\"train loss %.3f, val loss %.3f, val accuracy %.3f, and val rmse %.3f\" % (sum_loss/total, val_loss, val_acc, val_rmse))\n",
    "\n",
    "def validation_metrics (model, valid_dl, i):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    sum_loss = 0.0\n",
    "    sum_rmse = 0.0\n",
    "    total_preds, total_actual= [], []\n",
    "    for x, y, l in valid_dl:\n",
    "        x = x.long().to(device)\n",
    "        y = y.long()\n",
    "        y_hat = model(x, l).cpu()\n",
    "        loss = F.cross_entropy(y_hat, y)\n",
    "        pred = torch.max(y_hat, 1)[1]\n",
    "#         print(pred.numpy())\n",
    "#         print(y.numpy())\n",
    "        total_preds += pred.tolist()\n",
    "        total_actual += y.tolist()\n",
    "        correct += (pred == y).float().sum()\n",
    "        total += y.shape[0]\n",
    "        sum_loss += loss.item()*y.shape[0]\n",
    "        sum_rmse += np.sqrt(mean_squared_error(pred, y.unsqueeze(-1)))*y.shape[0]\n",
    "#     if i%5 == 0:\n",
    "#         print(i)\n",
    "    print(classification_report(total_preds, total_actual))\n",
    "    return sum_loss/total, correct/total, sum_rmse/total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vocab_size = len(words)\n",
    "# print(vocab_size)\n",
    "train_dl = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True)\n",
    "test_dl = DataLoader(test_ds, batch_size=BATCH_SIZE)\n",
    "# np.save('vocab2index.npy',vocab2index)\n",
    "# np.save('wordlist.npy',words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x,y,l = train_ds[0]\n",
    "len(train_dl), x.unsqueeze(0).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTM_fixed_len(torch.nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, glove_weights):\n",
    "        super().__init__()\n",
    "#         self.model = nn.Sequential(*[\n",
    "#             nn.Embedding(vocab_size, embedding_dim, padding_idx=0),\n",
    "#             nn.Dropout(0.2),\n",
    "#             nn.LSTM(embedding_dim, hidden_dim, batch_first=True),\n",
    "#             nn.Linear(hidden_dim, 2)\n",
    "#         ])\n",
    "        self.embeddings = nn.Embedding(vocab_size, embedding_dim, padding_idx=0)\n",
    "        self.embeddings.weight.data.copy_(torch.from_numpy(glove_weights))\n",
    "        self.embeddings.weight.requires_grad = False ## freeze embeddings\n",
    "        \n",
    "#         self.embeddings = nn.Embedding(vocab_size, embedding_dim, padding_idx=0)\n",
    "        self.dropout = nn.Dropout(0.2)\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, batch_first=True, num_layers=2, bidirectional=True)\n",
    "#         self.full_connecected = [nn.Linear(hidden_dim*2, hidden_dim, bias=False),\n",
    "#                                 nn.functional.relu_(),\n",
    "#                                 nn.Linear(hidden_dim, )]\n",
    "        self.linear = nn.Linear(hidden_dim*2, 4, bias=False)\n",
    "        \n",
    "    def forward(self, x, l):\n",
    "#         print(x[0])\n",
    "#         result = self.model(x)\n",
    "#         print(result)\n",
    "#         return 0\n",
    "        x = self.embeddings(x)\n",
    "        x = self.dropout(x)\n",
    "        lstm_out,( ht, ct) = self.lstm(x)\n",
    "#         print(ht[-1].shape, torch.cat((ht[-2],ht[-1]), dim=1).shape)\n",
    "#         print(ht.view(3, 2, 128, 256).shape)\n",
    "        return self.linear(torch.cat((ht[1],ht[-1]), dim=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_fixed = LSTM_fixed_len(vocab_size, embed_size, 256, pretrained_weights)\n",
    "model_fixed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 100\n",
    "BATCH_SIZE = 128\n",
    "LEARNING_RATE = 1e-3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_model(model_fixed.to(device))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
